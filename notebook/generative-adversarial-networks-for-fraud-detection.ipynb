{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import recall_score\n",
    "import torch\n",
    "#from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Cuda is running\n",
    "#torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomize the data, just to be sure not to get any pathological ordering effects that might harm the performane of Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124011</th>\n",
       "      <td>77147.0</td>\n",
       "      <td>-1.437314</td>\n",
       "      <td>-0.672735</td>\n",
       "      <td>0.925002</td>\n",
       "      <td>-0.728117</td>\n",
       "      <td>-0.498919</td>\n",
       "      <td>-1.088232</td>\n",
       "      <td>0.098312</td>\n",
       "      <td>0.294362</td>\n",
       "      <td>-1.763211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032172</td>\n",
       "      <td>-0.254662</td>\n",
       "      <td>0.569343</td>\n",
       "      <td>0.599137</td>\n",
       "      <td>-0.909906</td>\n",
       "      <td>0.070153</td>\n",
       "      <td>0.017838</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>149.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182926</th>\n",
       "      <td>125586.0</td>\n",
       "      <td>-1.188026</td>\n",
       "      <td>0.648003</td>\n",
       "      <td>0.195708</td>\n",
       "      <td>-2.300317</td>\n",
       "      <td>-1.876858</td>\n",
       "      <td>0.483442</td>\n",
       "      <td>-0.257970</td>\n",
       "      <td>-4.314653</td>\n",
       "      <td>1.394378</td>\n",
       "      <td>...</td>\n",
       "      <td>3.773234</td>\n",
       "      <td>-1.129681</td>\n",
       "      <td>-1.190132</td>\n",
       "      <td>0.066215</td>\n",
       "      <td>0.806729</td>\n",
       "      <td>0.669240</td>\n",
       "      <td>0.580496</td>\n",
       "      <td>0.090244</td>\n",
       "      <td>392.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>2364.0</td>\n",
       "      <td>1.571567</td>\n",
       "      <td>-0.852696</td>\n",
       "      <td>-0.411170</td>\n",
       "      <td>-1.735533</td>\n",
       "      <td>-0.574247</td>\n",
       "      <td>-0.298426</td>\n",
       "      <td>-0.606201</td>\n",
       "      <td>-0.165091</td>\n",
       "      <td>-2.384797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.546954</td>\n",
       "      <td>-1.269700</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>-0.896239</td>\n",
       "      <td>0.433003</td>\n",
       "      <td>-0.425964</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>-0.003500</td>\n",
       "      <td>19.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10848</th>\n",
       "      <td>18552.0</td>\n",
       "      <td>-0.265284</td>\n",
       "      <td>0.081207</td>\n",
       "      <td>1.595559</td>\n",
       "      <td>-1.671107</td>\n",
       "      <td>-0.359368</td>\n",
       "      <td>-0.158713</td>\n",
       "      <td>-0.188054</td>\n",
       "      <td>0.271057</td>\n",
       "      <td>2.710338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102560</td>\n",
       "      <td>0.673973</td>\n",
       "      <td>0.126516</td>\n",
       "      <td>-0.065758</td>\n",
       "      <td>-1.159539</td>\n",
       "      <td>-1.066871</td>\n",
       "      <td>0.283226</td>\n",
       "      <td>0.230268</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250966</th>\n",
       "      <td>155148.0</td>\n",
       "      <td>-1.071928</td>\n",
       "      <td>0.594211</td>\n",
       "      <td>-0.352817</td>\n",
       "      <td>-0.819417</td>\n",
       "      <td>1.439513</td>\n",
       "      <td>0.755137</td>\n",
       "      <td>0.594540</td>\n",
       "      <td>0.448423</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085680</td>\n",
       "      <td>0.088365</td>\n",
       "      <td>0.361190</td>\n",
       "      <td>-0.336159</td>\n",
       "      <td>-1.006584</td>\n",
       "      <td>0.176926</td>\n",
       "      <td>-0.098098</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>14.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "124011   77147.0 -1.437314 -0.672735  0.925002 -0.728117 -0.498919 -1.088232   \n",
       "182926  125586.0 -1.188026  0.648003  0.195708 -2.300317 -1.876858  0.483442   \n",
       "2807      2364.0  1.571567 -0.852696 -0.411170 -1.735533 -0.574247 -0.298426   \n",
       "10848    18552.0 -0.265284  0.081207  1.595559 -1.671107 -0.359368 -0.158713   \n",
       "250966  155148.0 -1.071928  0.594211 -0.352817 -0.819417  1.439513  0.755137   \n",
       "\n",
       "              V7        V8        V9  ...         V21       V22       V23  \\\n",
       "124011  0.098312  0.294362 -1.763211  ...   -0.032172 -0.254662  0.569343   \n",
       "182926 -0.257970 -4.314653  1.394378  ...    3.773234 -1.129681 -1.190132   \n",
       "2807   -0.606201 -0.165091 -2.384797  ...   -0.546954 -1.269700  0.009546   \n",
       "10848  -0.188054  0.271057  2.710338  ...    0.102560  0.673973  0.126516   \n",
       "250966  0.594540  0.448423  0.063034  ...   -0.085680  0.088365  0.361190   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "124011  0.599137 -0.909906  0.070153  0.017838  0.001861  149.20      0  \n",
       "182926  0.066215  0.806729  0.669240  0.580496  0.090244  392.00      0  \n",
       "2807   -0.896239  0.433003 -0.425964  0.000977 -0.003500   19.75      0  \n",
       "10848  -0.065758 -1.159539 -1.066871  0.283226  0.230268   11.85      0  \n",
       "250966 -0.336159 -1.006584  0.176926 -0.098098  0.001136   14.75      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.reindex(np.random.permutation(data.index))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.174225e-15</td>\n",
       "      <td>3.429687e-16</td>\n",
       "      <td>-1.386421e-15</td>\n",
       "      <td>2.073779e-15</td>\n",
       "      <td>9.939598e-16</td>\n",
       "      <td>1.493625e-15</td>\n",
       "      <td>-5.931037e-16</td>\n",
       "      <td>1.318317e-16</td>\n",
       "      <td>-2.414318e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.416845e-16</td>\n",
       "      <td>-3.515296e-16</td>\n",
       "      <td>2.727492e-16</td>\n",
       "      <td>4.482012e-15</td>\n",
       "      <td>5.203181e-16</td>\n",
       "      <td>1.689590e-15</td>\n",
       "      <td>-3.712632e-16</td>\n",
       "      <td>-1.159267e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.174225e-15  3.429687e-16 -1.386421e-15  2.073779e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.939598e-16  1.493625e-15 -5.931037e-16  1.318317e-16 -2.414318e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...        1.416845e-16 -3.515296e-16  2.727492e-16  4.482012e-15   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.203181e-16  1.689590e-15 -3.712632e-16 -1.159267e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Frequency')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = pd.value_counts(data['Class'], sort = True).sort_index()\n",
    "classes.plot(kind = 'bar')\n",
    "plt.title(\"Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Class')['Class'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Data is hihgly imbalance. 284315 Normal transaction vs 492 Fraud transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run with Normalising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns != 'Class']\n",
    "y = data.loc[:, data.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape\n",
      "(199364, 29)\n",
      "xtest shape\n",
      "(85443, 29)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "print('xtrain shape')\n",
    "print(X_train.shape)\n",
    "print('xtest shape')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the training data and test data\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit classifier to a model\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85291,     5],\n",
       "       [   39,   108]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     85296\n",
      "          1       0.96      0.73      0.83       147\n",
      "\n",
      "avg / total       1.00      1.00      1.00     85443\n",
      "\n",
      "Accuracy : 0.999485\n",
      "Area under the curve : 0.867318\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print('Accuracy : %f' % (metrics.accuracy_score(y_test, y_pred)))\n",
    "print('Area under the curve : %f' % (metrics.roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run with Over Sampling data using SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bbd82a65bc57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"creditcard.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10     ...           V21       V22       V23  \\\n",
       "0  0.098698  0.363787  0.090794     ...     -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425 -0.166974     ...     -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654  0.207643     ...      0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024 -0.054952     ...     -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739  0.753074     ...     -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  normAmount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0    0.244964  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   -0.342475  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0    1.160686  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0    0.140534  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0   -0.073403  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "data = data.drop(['Time','Amount'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (284807, 29)\n",
      "Shape of y: (284807, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(data.ix[:, data.columns != 'Class'])\n",
    "y = np.array(data.ix[:, data.columns == 'Class'])\n",
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape\n",
      "(199364, 29)\n",
      "xtest shape\n",
      "(85443, 29)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print('xtrain shape')\n",
    "print(X_train.shape)\n",
    "print('xtest shape')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over Sampling data using SMOTE\n",
    "smote = SMOTE(random_state=2)\n",
    "X_train_resample, y_train_resample = smote.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train_resample, y_train_resample.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85283,    13],\n",
       "       [   31,   116]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a Confision Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     85296\n",
      "          1       0.90      0.79      0.84       147\n",
      "\n",
      "avg / total       1.00      1.00      1.00     85443\n",
      "\n",
      "Accuracy : 0.999485\n",
      "Area under the curve : 0.894482\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print('Accuracy : %f' % (metrics.accuracy_score(y_test, y_pred)))\n",
    "print('Area under the curve : %f' % (metrics.roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run with GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "OUTPUT_PATH = '/output/'\n",
    "MODE = 'wgan-gp'\n",
    "RESTORE_MODE = False # If this flag is True, it will continue to train from the saved model.\n",
    "\n",
    "# Custom DataLoader\n",
    "class FraudDataset(Dataset):\n",
    "    \n",
    "    # Initialize the data\n",
    "    def __init__(self):\n",
    "        data = pd.read_csv(\"creditcard.csv\")\n",
    "        data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "        data = data.drop(['Time','Amount'],axis=1)\n",
    "        \n",
    "        # Rearrange columns to the right order\n",
    "        cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "        'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount', 'Class']\n",
    "        data = data[cols]\n",
    "        \n",
    "        fraud_data = data.loc[data['Class']==1]\n",
    "        fraud_data = fraud_data.drop('Class', 1)\n",
    "        self.len = fraud_data.shape[0]\n",
    "        \n",
    "        self.fraud_data = torch.FloatTensor(np.array(fraud_data))\n",
    "        \n",
    "        #self.X = np.array(data.loc[:, data.columns != 'Class'])\n",
    "        #self.y = np.array(data.loc[:, data.columns == 'Class'])\n",
    "        \n",
    "        #self.X = torch.FloatTensor(self.X)\n",
    "        #self.y = torch.FloatTensor(self.y)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.fraud_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESTORE_MODE:\n",
    "    generator = torch.load(OUTPUT_PATH + \"generator.pt\" )\n",
    "    discriminator = torch.load(OUTPUT_PATH + \"discriminator.pt\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FraudDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator's paragrams\n",
    "g_input_size = 29     # Random noise dimension\n",
    "g_hidden_size = 50   # Generator complexity\n",
    "g_output_size = 1   \n",
    "g_learning_rate = 0.0002\n",
    "\n",
    "#Discriminator's paragrams\n",
    "d_input_size = 29   # Minibatch size\n",
    "d_hidden_size = 50  # Discriminator complexity\n",
    "d_output_size = 1   # Single dimension for 'real' vs. 'fake'\n",
    "d_learning_rate = 0.0002\n",
    "\n",
    "minibatch_size = d_input_size\n",
    "\n",
    "num_epochs = 2000\n",
    "print_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ELU (Exponential Linear Unit) function tends to converge cost to zero faster and produce more accurate results\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.sigmoid(self.map2(x))\n",
    "        return self.map3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.map2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.map3(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "discriminator = Discriminator(input_size=d_input_size, hidden_size=d_hidden_size, output_size=d_output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these 2 lines to run on GPU\n",
    "#generator.cuda()\n",
    "#discriminator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(disc):\n",
    "    h=0.1\n",
    "    x_min, x_max = data[:, 0].min() - .5, data[:, 0].max() + .5\n",
    "    y_min, y_max = data[:, 1].min() - .5, data[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    x_test = np.c_[xx.ravel(), yy.ravel()]\n",
    "    y_hat_test = disc.forward_with_sigmoid(Variable(torch.from_numpy(x_test).float()))\n",
    "\n",
    "    plt.pcolormesh(xx, yy, y_hat_test.data.numpy().reshape(xx.shape), cmap=plt.cm.Paired)\n",
    "    plt.colorbar()\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y*20, alpha=0.1, cmap=plt.cm.flag, s=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Binary Cross Entropy loss\n",
    "BCE_loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizers\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=d_learning_rate/2, betas=(beta_1, beta_2))\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=g_learning_rate, betas=(beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Discriminator Loss: 0.834, Generator Loss: 0.849\n",
      "Epoch 11 - Discriminator Loss: 0.380, Generator Loss: 3.360\n",
      "Epoch 21 - Discriminator Loss: 0.245, Generator Loss: 3.556\n",
      "Epoch 31 - Discriminator Loss: 0.233, Generator Loss: 3.647\n",
      "Epoch 41 - Discriminator Loss: 0.223, Generator Loss: 3.945\n",
      "Epoch 51 - Discriminator Loss: 0.196, Generator Loss: 4.104\n",
      "Epoch 61 - Discriminator Loss: 0.185, Generator Loss: 4.153\n",
      "Epoch 71 - Discriminator Loss: 0.159, Generator Loss: 4.382\n",
      "Epoch 81 - Discriminator Loss: 0.122, Generator Loss: 4.473\n",
      "Epoch 91 - Discriminator Loss: 0.109, Generator Loss: 4.988\n",
      "Epoch 101 - Discriminator Loss: 0.079, Generator Loss: 5.336\n",
      "Epoch 111 - Discriminator Loss: 0.071, Generator Loss: 5.516\n",
      "Epoch 121 - Discriminator Loss: 0.054, Generator Loss: 6.145\n",
      "Epoch 131 - Discriminator Loss: 0.046, Generator Loss: 6.852\n",
      "Epoch 141 - Discriminator Loss: 0.041, Generator Loss: 6.962\n",
      "Epoch 151 - Discriminator Loss: 0.032, Generator Loss: 7.701\n",
      "Epoch 161 - Discriminator Loss: 0.043, Generator Loss: 8.357\n",
      "Epoch 171 - Discriminator Loss: 0.014, Generator Loss: 8.549\n",
      "Epoch 181 - Discriminator Loss: 0.014, Generator Loss: 9.231\n",
      "Epoch 191 - Discriminator Loss: 0.010, Generator Loss: 9.850\n",
      "Epoch 201 - Discriminator Loss: 0.006, Generator Loss: 10.267\n",
      "Epoch 211 - Discriminator Loss: 0.023, Generator Loss: 10.756\n",
      "Epoch 221 - Discriminator Loss: 0.003, Generator Loss: 10.951\n",
      "Epoch 231 - Discriminator Loss: 0.008, Generator Loss: 11.935\n",
      "Epoch 241 - Discriminator Loss: 0.002, Generator Loss: 11.996\n",
      "Epoch 251 - Discriminator Loss: 0.006, Generator Loss: 13.144\n",
      "Epoch 261 - Discriminator Loss: 0.009, Generator Loss: 13.163\n",
      "Epoch 271 - Discriminator Loss: 0.002, Generator Loss: 13.475\n",
      "Epoch 281 - Discriminator Loss: 0.014, Generator Loss: 14.524\n",
      "Epoch 291 - Discriminator Loss: 0.007, Generator Loss: 15.672\n",
      "Epoch 301 - Discriminator Loss: 0.007, Generator Loss: 15.152\n",
      "Epoch 311 - Discriminator Loss: 0.004, Generator Loss: 15.711\n",
      "Epoch 321 - Discriminator Loss: 0.003, Generator Loss: 16.092\n",
      "Epoch 331 - Discriminator Loss: 0.010, Generator Loss: 17.018\n",
      "Epoch 341 - Discriminator Loss: 0.025, Generator Loss: 17.090\n",
      "Epoch 351 - Discriminator Loss: 0.002, Generator Loss: 16.865\n",
      "Epoch 361 - Discriminator Loss: 0.005, Generator Loss: 17.474\n",
      "Epoch 371 - Discriminator Loss: 0.004, Generator Loss: 17.145\n",
      "Epoch 381 - Discriminator Loss: 0.002, Generator Loss: 17.500\n",
      "Epoch 391 - Discriminator Loss: 0.006, Generator Loss: 18.408\n",
      "Epoch 401 - Discriminator Loss: 0.001, Generator Loss: 18.117\n",
      "Epoch 411 - Discriminator Loss: 0.031, Generator Loss: 19.775\n",
      "Epoch 421 - Discriminator Loss: 0.000, Generator Loss: 18.443\n",
      "Epoch 431 - Discriminator Loss: 0.000, Generator Loss: 16.746\n",
      "Epoch 441 - Discriminator Loss: 0.000, Generator Loss: 17.426\n",
      "Epoch 451 - Discriminator Loss: 0.031, Generator Loss: 20.401\n",
      "Epoch 461 - Discriminator Loss: 0.015, Generator Loss: 21.837\n",
      "Epoch 471 - Discriminator Loss: 0.000, Generator Loss: 18.791\n",
      "Epoch 481 - Discriminator Loss: 0.000, Generator Loss: 17.325\n",
      "Epoch 491 - Discriminator Loss: 0.000, Generator Loss: 15.998\n",
      "Epoch 501 - Discriminator Loss: 0.000, Generator Loss: 17.918\n",
      "Epoch 511 - Discriminator Loss: 0.124, Generator Loss: 22.581\n",
      "Epoch 521 - Discriminator Loss: 0.066, Generator Loss: 24.416\n",
      "Epoch 531 - Discriminator Loss: 0.000, Generator Loss: 20.533\n",
      "Epoch 541 - Discriminator Loss: 0.000, Generator Loss: 18.605\n",
      "Epoch 551 - Discriminator Loss: 0.000, Generator Loss: 16.861\n",
      "Epoch 561 - Discriminator Loss: 0.000, Generator Loss: 15.639\n",
      "Epoch 571 - Discriminator Loss: 0.000, Generator Loss: 16.696\n",
      "Epoch 581 - Discriminator Loss: 0.000, Generator Loss: 19.882\n",
      "Epoch 591 - Discriminator Loss: 0.183, Generator Loss: 24.578\n",
      "Epoch 601 - Discriminator Loss: 0.129, Generator Loss: 25.844\n",
      "Epoch 611 - Discriminator Loss: 0.003, Generator Loss: 25.047\n",
      "Epoch 621 - Discriminator Loss: 0.000, Generator Loss: 21.722\n",
      "Epoch 631 - Discriminator Loss: 0.000, Generator Loss: 20.060\n",
      "Epoch 641 - Discriminator Loss: 0.000, Generator Loss: 18.353\n",
      "Epoch 651 - Discriminator Loss: 0.000, Generator Loss: 17.056\n",
      "Epoch 661 - Discriminator Loss: 0.000, Generator Loss: 18.386\n",
      "Epoch 671 - Discriminator Loss: 0.000, Generator Loss: 20.744\n",
      "Epoch 681 - Discriminator Loss: 0.004, Generator Loss: 23.994\n",
      "Epoch 691 - Discriminator Loss: 0.072, Generator Loss: 25.728\n",
      "Epoch 701 - Discriminator Loss: 0.111, Generator Loss: 26.110\n",
      "Epoch 711 - Discriminator Loss: 0.152, Generator Loss: 26.573\n",
      "Epoch 721 - Discriminator Loss: 0.001, Generator Loss: 25.809\n",
      "Epoch 731 - Discriminator Loss: 0.000, Generator Loss: 24.838\n",
      "Epoch 741 - Discriminator Loss: 0.000, Generator Loss: 22.889\n",
      "Epoch 751 - Discriminator Loss: 0.000, Generator Loss: 21.158\n",
      "Epoch 761 - Discriminator Loss: 0.000, Generator Loss: 19.524\n",
      "Epoch 771 - Discriminator Loss: 0.000, Generator Loss: 19.452\n",
      "Epoch 781 - Discriminator Loss: 0.000, Generator Loss: 21.400\n",
      "Epoch 791 - Discriminator Loss: 0.000, Generator Loss: 22.952\n",
      "Epoch 801 - Discriminator Loss: 0.036, Generator Loss: 25.775\n",
      "Epoch 811 - Discriminator Loss: 0.096, Generator Loss: 26.339\n",
      "Epoch 821 - Discriminator Loss: 0.063, Generator Loss: 26.273\n",
      "Epoch 831 - Discriminator Loss: 0.023, Generator Loss: 26.181\n",
      "Epoch 841 - Discriminator Loss: 0.003, Generator Loss: 26.251\n",
      "Epoch 851 - Discriminator Loss: 0.037, Generator Loss: 25.492\n",
      "Epoch 861 - Discriminator Loss: 0.006, Generator Loss: 25.357\n",
      "Epoch 871 - Discriminator Loss: 0.001, Generator Loss: 24.631\n",
      "Epoch 881 - Discriminator Loss: 0.007, Generator Loss: 25.170\n",
      "Epoch 891 - Discriminator Loss: 0.002, Generator Loss: 25.065\n",
      "Epoch 901 - Discriminator Loss: 0.027, Generator Loss: 25.187\n",
      "Epoch 911 - Discriminator Loss: 0.067, Generator Loss: 25.129\n",
      "Epoch 921 - Discriminator Loss: 0.002, Generator Loss: 23.985\n",
      "Epoch 931 - Discriminator Loss: 0.000, Generator Loss: 21.959\n",
      "Epoch 941 - Discriminator Loss: 0.000, Generator Loss: 20.274\n",
      "Epoch 951 - Discriminator Loss: 0.000, Generator Loss: 18.590\n",
      "Epoch 961 - Discriminator Loss: 0.000, Generator Loss: 17.470\n",
      "Epoch 971 - Discriminator Loss: 0.000, Generator Loss: 18.831\n",
      "Epoch 981 - Discriminator Loss: 0.000, Generator Loss: 21.064\n",
      "Epoch 991 - Discriminator Loss: 0.000, Generator Loss: 22.809\n",
      "Epoch 1001 - Discriminator Loss: 0.000, Generator Loss: 24.090\n",
      "Epoch 1011 - Discriminator Loss: 0.000, Generator Loss: 22.540\n",
      "Epoch 1021 - Discriminator Loss: 0.000, Generator Loss: 23.363\n",
      "Epoch 1031 - Discriminator Loss: 0.000, Generator Loss: 24.183\n",
      "Epoch 1041 - Discriminator Loss: 0.000, Generator Loss: 25.143\n",
      "Epoch 1051 - Discriminator Loss: 0.243, Generator Loss: 26.713\n",
      "Epoch 1061 - Discriminator Loss: 0.068, Generator Loss: 26.390\n",
      "Epoch 1071 - Discriminator Loss: 0.007, Generator Loss: 25.522\n",
      "Epoch 1081 - Discriminator Loss: 0.000, Generator Loss: 26.247\n",
      "Epoch 1091 - Discriminator Loss: 0.014, Generator Loss: 26.597\n",
      "Epoch 1101 - Discriminator Loss: 0.003, Generator Loss: 26.361\n",
      "Epoch 1111 - Discriminator Loss: 0.011, Generator Loss: 26.467\n",
      "Epoch 1121 - Discriminator Loss: 0.058, Generator Loss: 26.001\n",
      "Epoch 1131 - Discriminator Loss: 0.084, Generator Loss: 26.053\n",
      "Epoch 1141 - Discriminator Loss: 0.032, Generator Loss: 26.352\n",
      "Epoch 1151 - Discriminator Loss: 0.003, Generator Loss: 25.509\n",
      "Epoch 1161 - Discriminator Loss: 0.004, Generator Loss: 25.742\n",
      "Epoch 1171 - Discriminator Loss: 0.012, Generator Loss: 25.837\n",
      "Epoch 1181 - Discriminator Loss: 0.033, Generator Loss: 25.834\n",
      "Epoch 1191 - Discriminator Loss: 0.034, Generator Loss: 25.825\n",
      "Epoch 1201 - Discriminator Loss: 0.017, Generator Loss: 26.078\n",
      "Epoch 1211 - Discriminator Loss: 0.018, Generator Loss: 26.145\n",
      "Epoch 1221 - Discriminator Loss: 0.000, Generator Loss: 25.725\n",
      "Epoch 1231 - Discriminator Loss: 0.001, Generator Loss: 25.734\n",
      "Epoch 1241 - Discriminator Loss: 0.005, Generator Loss: 25.460\n",
      "Epoch 1251 - Discriminator Loss: 0.000, Generator Loss: 25.563\n",
      "Epoch 1261 - Discriminator Loss: 0.004, Generator Loss: 25.679\n",
      "Epoch 1271 - Discriminator Loss: 0.007, Generator Loss: 25.996\n",
      "Epoch 1281 - Discriminator Loss: 0.002, Generator Loss: 25.724\n",
      "Epoch 1291 - Discriminator Loss: 0.007, Generator Loss: 26.151\n",
      "Epoch 1301 - Discriminator Loss: 0.002, Generator Loss: 26.211\n",
      "Epoch 1311 - Discriminator Loss: 0.001, Generator Loss: 25.567\n",
      "Epoch 1321 - Discriminator Loss: 0.005, Generator Loss: 25.927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1331 - Discriminator Loss: 0.114, Generator Loss: 26.137\n",
      "Epoch 1341 - Discriminator Loss: 0.052, Generator Loss: 26.126\n",
      "Epoch 1351 - Discriminator Loss: 0.001, Generator Loss: 25.977\n",
      "Epoch 1361 - Discriminator Loss: 0.000, Generator Loss: 25.571\n",
      "Epoch 1371 - Discriminator Loss: 0.000, Generator Loss: 25.758\n",
      "Epoch 1381 - Discriminator Loss: 0.000, Generator Loss: 25.971\n",
      "Epoch 1391 - Discriminator Loss: 0.007, Generator Loss: 26.009\n",
      "Epoch 1401 - Discriminator Loss: 0.008, Generator Loss: 26.192\n",
      "Epoch 1411 - Discriminator Loss: 0.020, Generator Loss: 26.190\n",
      "Epoch 1421 - Discriminator Loss: 0.003, Generator Loss: 26.408\n",
      "Epoch 1431 - Discriminator Loss: 0.000, Generator Loss: 26.160\n",
      "Epoch 1441 - Discriminator Loss: 0.101, Generator Loss: 26.289\n",
      "Epoch 1451 - Discriminator Loss: 0.031, Generator Loss: 26.646\n",
      "Epoch 1461 - Discriminator Loss: 0.002, Generator Loss: 26.378\n",
      "Epoch 1471 - Discriminator Loss: 0.034, Generator Loss: 26.690\n",
      "Epoch 1481 - Discriminator Loss: 0.001, Generator Loss: 25.804\n",
      "Epoch 1491 - Discriminator Loss: 0.024, Generator Loss: 26.405\n",
      "Epoch 1501 - Discriminator Loss: 0.004, Generator Loss: 26.455\n",
      "Epoch 1511 - Discriminator Loss: 0.000, Generator Loss: 25.591\n",
      "Epoch 1521 - Discriminator Loss: 0.000, Generator Loss: 26.127\n",
      "Epoch 1531 - Discriminator Loss: 0.029, Generator Loss: 26.450\n",
      "Epoch 1541 - Discriminator Loss: 0.004, Generator Loss: 26.095\n",
      "Epoch 1551 - Discriminator Loss: 0.016, Generator Loss: 26.192\n",
      "Epoch 1561 - Discriminator Loss: 0.037, Generator Loss: 26.738\n",
      "Epoch 1571 - Discriminator Loss: 0.007, Generator Loss: 26.480\n",
      "Epoch 1581 - Discriminator Loss: 0.000, Generator Loss: 26.088\n",
      "Epoch 1591 - Discriminator Loss: 0.002, Generator Loss: 26.389\n",
      "Epoch 1601 - Discriminator Loss: 0.005, Generator Loss: 26.142\n",
      "Epoch 1611 - Discriminator Loss: 0.003, Generator Loss: 26.412\n",
      "Epoch 1621 - Discriminator Loss: 0.000, Generator Loss: 25.774\n",
      "Epoch 1631 - Discriminator Loss: 0.013, Generator Loss: 26.261\n",
      "Epoch 1641 - Discriminator Loss: 0.041, Generator Loss: 26.571\n",
      "Epoch 1651 - Discriminator Loss: 0.035, Generator Loss: 26.296\n",
      "Epoch 1661 - Discriminator Loss: 0.003, Generator Loss: 26.249\n",
      "Epoch 1671 - Discriminator Loss: 0.000, Generator Loss: 26.309\n",
      "Epoch 1681 - Discriminator Loss: 0.002, Generator Loss: 26.258\n",
      "Epoch 1691 - Discriminator Loss: 0.000, Generator Loss: 26.429\n",
      "Epoch 1701 - Discriminator Loss: 0.041, Generator Loss: 26.810\n",
      "Epoch 1711 - Discriminator Loss: 0.000, Generator Loss: 26.493\n",
      "Epoch 1721 - Discriminator Loss: 0.000, Generator Loss: 26.418\n",
      "Epoch 1731 - Discriminator Loss: 0.021, Generator Loss: 26.423\n",
      "Epoch 1741 - Discriminator Loss: 0.018, Generator Loss: 26.875\n",
      "Epoch 1751 - Discriminator Loss: 0.014, Generator Loss: 26.499\n",
      "Epoch 1761 - Discriminator Loss: 0.008, Generator Loss: 26.777\n",
      "Epoch 1771 - Discriminator Loss: 0.012, Generator Loss: 26.381\n",
      "Epoch 1781 - Discriminator Loss: 0.022, Generator Loss: 26.643\n",
      "Epoch 1791 - Discriminator Loss: 0.014, Generator Loss: 26.598\n",
      "Epoch 1801 - Discriminator Loss: 0.009, Generator Loss: 26.633\n",
      "Epoch 1811 - Discriminator Loss: 0.002, Generator Loss: 26.431\n",
      "Epoch 1821 - Discriminator Loss: 0.000, Generator Loss: 26.117\n",
      "Epoch 1831 - Discriminator Loss: 0.000, Generator Loss: 26.548\n",
      "Epoch 1841 - Discriminator Loss: 0.002, Generator Loss: 26.974\n",
      "Epoch 1851 - Discriminator Loss: 0.026, Generator Loss: 26.539\n",
      "Epoch 1861 - Discriminator Loss: 0.015, Generator Loss: 26.633\n",
      "Epoch 1871 - Discriminator Loss: 0.008, Generator Loss: 26.542\n",
      "Epoch 1881 - Discriminator Loss: 0.008, Generator Loss: 26.813\n",
      "Epoch 1891 - Discriminator Loss: 0.002, Generator Loss: 26.764\n",
      "Epoch 1901 - Discriminator Loss: 0.001, Generator Loss: 26.368\n",
      "Epoch 1911 - Discriminator Loss: 0.001, Generator Loss: 26.381\n",
      "Epoch 1921 - Discriminator Loss: 0.015, Generator Loss: 26.588\n",
      "Epoch 1931 - Discriminator Loss: 0.002, Generator Loss: 26.447\n",
      "Epoch 1941 - Discriminator Loss: 0.011, Generator Loss: 26.471\n",
      "Epoch 1951 - Discriminator Loss: 0.001, Generator Loss: 26.928\n",
      "Epoch 1961 - Discriminator Loss: 0.009, Generator Loss: 26.773\n",
      "Epoch 1971 - Discriminator Loss: 0.001, Generator Loss: 26.300\n",
      "Epoch 1981 - Discriminator Loss: 0.005, Generator Loss: 26.653\n",
      "Epoch 1991 - Discriminator Loss: 0.001, Generator Loss: 26.848\n"
     ]
    }
   ],
   "source": [
    "# Training DCGANs\n",
    "for epoch in range(num_epochs):\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    synthentic_data = []\n",
    "    for i, fraud_data in enumerate(train_loader):\n",
    "        # Updating the weights of the Discriminator\n",
    "        discriminator.zero_grad() # Initialize gradients of the Discriminator to 0\n",
    "        \n",
    "        mini_batch = fraud_data.size()[0]\n",
    "        \n",
    "        # Wrap data in PyTorch Variable\n",
    "        d_real_data = Variable(fraud_data[0])\n",
    "        y_real = Variable(torch.ones(1))\n",
    "        y_fake = Variable(torch.zeros(1))\n",
    "\n",
    "        # Training the Discriminator with real data\n",
    "        d_real_result = discriminator(d_real_data) # Forward propagate this real data into the neural network\n",
    "        d_real_loss = BCE_loss(d_real_result, y_real) # Compute the loss between the prediction and actual\n",
    "        d_real_loss.backward()\n",
    "    \n",
    "        # Inject fake data to the generator\n",
    "        d_gen_input = Variable(torch.randn(minibatch_size, g_input_size))\n",
    "        d_fake_data = generator(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "        \n",
    "        # Train the Discriminator with a fake data generated by the Generator\n",
    "        d_fake_result = discriminator(d_fake_data.t())\n",
    "        d_fake_loss = BCE_loss(d_fake_result, y_fake)  # zeros = fake\n",
    "        d_fake_loss.backward()\n",
    "        \n",
    "        # Combine discriminator loss from real data and fake data\n",
    "        d_train_loss = d_real_loss + d_fake_loss\n",
    "        \n",
    "        #d_train_loss.backward()\n",
    "        d_optimizer.step()     # Apply SGD to update the weight\n",
    "        d_losses.append(d_train_loss.data[0])\n",
    "        \n",
    "        # Update the weight of the Generator \n",
    "        generator.zero_grad()\n",
    "        gen_input = Variable(torch.randn(minibatch_size, g_input_size))  \n",
    "        g_fake_data = generator(gen_input)\n",
    "        \n",
    "        dg_fake_result = discriminator(g_fake_data.t())\n",
    "        g_loss = BCE_loss(dg_fake_result, y_real)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        g_losses.append(g_loss.data[0])\n",
    "        \n",
    "        synthentic_data.append(d_fake_data.t())\n",
    "        \n",
    "    if epoch % print_interval == 0:       \n",
    "        print('Epoch {} - Discriminator Loss: {:.3f}, Generator Loss: {:.3f}'.format((epoch + 1), \n",
    "                          torch.mean(torch.FloatTensor(d_losses)), torch.mean(torch.FloatTensor(g_losses))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a1fa83da0>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(d_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2020ba20>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3814e+00, -5.6949e-02, -5.7463e-02,  ..., -1.6260e-01,\n",
       "         -5.4332e-02, -4.6051e-02],\n",
       "        [-4.5472e-02, -1.0283e-01, -5.7138e-02,  ..., -5.6385e-02,\n",
       "          5.2308e-02, -4.5068e-02],\n",
       "        [ 1.2165e-01,  2.9972e+00, -9.1125e-03,  ...,  3.7267e+00,\n",
       "         -5.5316e-02,  9.2408e-01],\n",
       "        ...,\n",
       "        [-1.7963e-02,  9.3903e-03,  1.9030e-02,  ...,  2.2842e-02,\n",
       "         -2.0149e-02,  3.0771e-06],\n",
       "        [-3.8866e-02, -1.7103e-02, -1.9109e-02,  ..., -1.4033e-02,\n",
       "         -1.7922e-02, -3.6210e-01],\n",
       "        [-1.6612e-02, -1.7381e-02, -1.9701e-02,  ...,  2.5594e+00,\n",
       "          1.0047e+00, -3.1743e-02]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can train and test model on the generated data\n",
    "synthentic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-248c0e83a90a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert array of tensor to tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msynthentic_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynthentic_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "# Convert array of tensor to tensors\n",
    "temp = torch.Tensor(99)\n",
    "synthentic_data = torch.cat(synthentic_data, out=temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.381356</td>\n",
       "      <td>-0.056949</td>\n",
       "      <td>-0.057463</td>\n",
       "      <td>-0.040298</td>\n",
       "      <td>-0.057084</td>\n",
       "      <td>-0.056528</td>\n",
       "      <td>-0.066441</td>\n",
       "      <td>-0.057056</td>\n",
       "      <td>-0.048743</td>\n",
       "      <td>-1.982630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057270</td>\n",
       "      <td>-0.054910</td>\n",
       "      <td>-0.051792</td>\n",
       "      <td>-0.055885</td>\n",
       "      <td>-0.056607</td>\n",
       "      <td>-0.040957</td>\n",
       "      <td>2.976045</td>\n",
       "      <td>-0.162601</td>\n",
       "      <td>-0.054332</td>\n",
       "      <td>-0.046051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.102833</td>\n",
       "      <td>-0.057138</td>\n",
       "      <td>0.295807</td>\n",
       "      <td>-0.097408</td>\n",
       "      <td>-0.144796</td>\n",
       "      <td>-0.055682</td>\n",
       "      <td>-0.014743</td>\n",
       "      <td>-0.300167</td>\n",
       "      <td>-0.063522</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057548</td>\n",
       "      <td>-0.045611</td>\n",
       "      <td>-0.071561</td>\n",
       "      <td>1.058161</td>\n",
       "      <td>-0.641967</td>\n",
       "      <td>3.363811</td>\n",
       "      <td>0.424565</td>\n",
       "      <td>-0.056385</td>\n",
       "      <td>0.052308</td>\n",
       "      <td>-0.045068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.121647</td>\n",
       "      <td>2.997174</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>-0.061742</td>\n",
       "      <td>-0.633560</td>\n",
       "      <td>-0.057275</td>\n",
       "      <td>-0.046771</td>\n",
       "      <td>-0.053500</td>\n",
       "      <td>-0.129914</td>\n",
       "      <td>-0.932856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.361437</td>\n",
       "      <td>-0.057405</td>\n",
       "      <td>-0.059111</td>\n",
       "      <td>-1.944048</td>\n",
       "      <td>-1.632115</td>\n",
       "      <td>-0.031994</td>\n",
       "      <td>-1.712119</td>\n",
       "      <td>3.726690</td>\n",
       "      <td>-0.055316</td>\n",
       "      <td>0.924076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011625</td>\n",
       "      <td>-0.057230</td>\n",
       "      <td>-0.053383</td>\n",
       "      <td>-0.233911</td>\n",
       "      <td>0.090462</td>\n",
       "      <td>-0.058491</td>\n",
       "      <td>-0.062384</td>\n",
       "      <td>-0.085092</td>\n",
       "      <td>-0.292447</td>\n",
       "      <td>-2.205114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058340</td>\n",
       "      <td>-0.105928</td>\n",
       "      <td>-0.542505</td>\n",
       "      <td>0.128057</td>\n",
       "      <td>-0.056475</td>\n",
       "      <td>-0.047488</td>\n",
       "      <td>-0.058267</td>\n",
       "      <td>-0.034562</td>\n",
       "      <td>-0.035444</td>\n",
       "      <td>-0.058541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.090371</td>\n",
       "      <td>-0.049098</td>\n",
       "      <td>-0.058707</td>\n",
       "      <td>0.014012</td>\n",
       "      <td>-0.062817</td>\n",
       "      <td>-0.178670</td>\n",
       "      <td>-2.141018</td>\n",
       "      <td>-0.058279</td>\n",
       "      <td>-0.079392</td>\n",
       "      <td>-1.248853</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138195</td>\n",
       "      <td>0.042109</td>\n",
       "      <td>-0.056917</td>\n",
       "      <td>-0.058688</td>\n",
       "      <td>-1.175004</td>\n",
       "      <td>-0.056022</td>\n",
       "      <td>-0.048289</td>\n",
       "      <td>-1.096804</td>\n",
       "      <td>-1.868680</td>\n",
       "      <td>-0.055720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  1.381356 -0.056949 -0.057463 -0.040298 -0.057084 -0.056528 -0.066441   \n",
       "1 -0.045472 -0.102833 -0.057138  0.295807 -0.097408 -0.144796 -0.055682   \n",
       "2  0.121647  2.997174 -0.009113 -0.061742 -0.633560 -0.057275 -0.046771   \n",
       "3  0.011625 -0.057230 -0.053383 -0.233911  0.090462 -0.058491 -0.062384   \n",
       "4 -0.090371 -0.049098 -0.058707  0.014012 -0.062817 -0.178670 -2.141018   \n",
       "\n",
       "         7         8         9     ...           19        20        21  \\\n",
       "0 -0.057056 -0.048743 -1.982630    ...    -0.057270 -0.054910 -0.051792   \n",
       "1 -0.014743 -0.300167 -0.063522    ...    -0.057548 -0.045611 -0.071561   \n",
       "2 -0.053500 -0.129914 -0.932856    ...    -0.361437 -0.057405 -0.059111   \n",
       "3 -0.085092 -0.292447 -2.205114    ...    -0.058340 -0.105928 -0.542505   \n",
       "4 -0.058279 -0.079392 -1.248853    ...    -0.138195  0.042109 -0.056917   \n",
       "\n",
       "         22        23        24        25        26        27        28  \n",
       "0 -0.055885 -0.056607 -0.040957  2.976045 -0.162601 -0.054332 -0.046051  \n",
       "1  1.058161 -0.641967  3.363811  0.424565 -0.056385  0.052308 -0.045068  \n",
       "2 -1.944048 -1.632115 -0.031994 -1.712119  3.726690 -0.055316  0.924076  \n",
       "3  0.128057 -0.056475 -0.047488 -0.058267 -0.034562 -0.035444 -0.058541  \n",
       "4 -0.058688 -1.175004 -0.056022 -0.048289 -1.096804 -1.868680 -0.055720  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tensor to PD data frame\n",
    "synthentic_data_df = pd.DataFrame(data=synthentic_data.data.numpy());\n",
    "synthentic_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns of the synthentic dataset\n",
    "cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "        'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount']\n",
    "synthentic_data_df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.381356</td>\n",
       "      <td>-0.056949</td>\n",
       "      <td>-0.057463</td>\n",
       "      <td>-0.040298</td>\n",
       "      <td>-0.057084</td>\n",
       "      <td>-0.056528</td>\n",
       "      <td>-0.066441</td>\n",
       "      <td>-0.057056</td>\n",
       "      <td>-0.048743</td>\n",
       "      <td>-1.982630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057270</td>\n",
       "      <td>-0.054910</td>\n",
       "      <td>-0.051792</td>\n",
       "      <td>-0.055885</td>\n",
       "      <td>-0.056607</td>\n",
       "      <td>-0.040957</td>\n",
       "      <td>2.976045</td>\n",
       "      <td>-0.162601</td>\n",
       "      <td>-0.054332</td>\n",
       "      <td>-0.046051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.102833</td>\n",
       "      <td>-0.057138</td>\n",
       "      <td>0.295807</td>\n",
       "      <td>-0.097408</td>\n",
       "      <td>-0.144796</td>\n",
       "      <td>-0.055682</td>\n",
       "      <td>-0.014743</td>\n",
       "      <td>-0.300167</td>\n",
       "      <td>-0.063522</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057548</td>\n",
       "      <td>-0.045611</td>\n",
       "      <td>-0.071561</td>\n",
       "      <td>1.058161</td>\n",
       "      <td>-0.641967</td>\n",
       "      <td>3.363811</td>\n",
       "      <td>0.424565</td>\n",
       "      <td>-0.056385</td>\n",
       "      <td>0.052308</td>\n",
       "      <td>-0.045068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.121647</td>\n",
       "      <td>2.997174</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>-0.061742</td>\n",
       "      <td>-0.633560</td>\n",
       "      <td>-0.057275</td>\n",
       "      <td>-0.046771</td>\n",
       "      <td>-0.053500</td>\n",
       "      <td>-0.129914</td>\n",
       "      <td>-0.932856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.361437</td>\n",
       "      <td>-0.057405</td>\n",
       "      <td>-0.059111</td>\n",
       "      <td>-1.944048</td>\n",
       "      <td>-1.632115</td>\n",
       "      <td>-0.031994</td>\n",
       "      <td>-1.712119</td>\n",
       "      <td>3.726690</td>\n",
       "      <td>-0.055316</td>\n",
       "      <td>0.924076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011625</td>\n",
       "      <td>-0.057230</td>\n",
       "      <td>-0.053383</td>\n",
       "      <td>-0.233911</td>\n",
       "      <td>0.090462</td>\n",
       "      <td>-0.058491</td>\n",
       "      <td>-0.062384</td>\n",
       "      <td>-0.085092</td>\n",
       "      <td>-0.292447</td>\n",
       "      <td>-2.205114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058340</td>\n",
       "      <td>-0.105928</td>\n",
       "      <td>-0.542505</td>\n",
       "      <td>0.128057</td>\n",
       "      <td>-0.056475</td>\n",
       "      <td>-0.047488</td>\n",
       "      <td>-0.058267</td>\n",
       "      <td>-0.034562</td>\n",
       "      <td>-0.035444</td>\n",
       "      <td>-0.058541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.090371</td>\n",
       "      <td>-0.049098</td>\n",
       "      <td>-0.058707</td>\n",
       "      <td>0.014012</td>\n",
       "      <td>-0.062817</td>\n",
       "      <td>-0.178670</td>\n",
       "      <td>-2.141018</td>\n",
       "      <td>-0.058279</td>\n",
       "      <td>-0.079392</td>\n",
       "      <td>-1.248853</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138195</td>\n",
       "      <td>0.042109</td>\n",
       "      <td>-0.056917</td>\n",
       "      <td>-0.058688</td>\n",
       "      <td>-1.175004</td>\n",
       "      <td>-0.056022</td>\n",
       "      <td>-0.048289</td>\n",
       "      <td>-1.096804</td>\n",
       "      <td>-1.868680</td>\n",
       "      <td>-0.055720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  1.381356 -0.056949 -0.057463 -0.040298 -0.057084 -0.056528 -0.066441   \n",
       "1 -0.045472 -0.102833 -0.057138  0.295807 -0.097408 -0.144796 -0.055682   \n",
       "2  0.121647  2.997174 -0.009113 -0.061742 -0.633560 -0.057275 -0.046771   \n",
       "3  0.011625 -0.057230 -0.053383 -0.233911  0.090462 -0.058491 -0.062384   \n",
       "4 -0.090371 -0.049098 -0.058707  0.014012 -0.062817 -0.178670 -2.141018   \n",
       "\n",
       "         V8        V9       V10     ...           V20       V21       V22  \\\n",
       "0 -0.057056 -0.048743 -1.982630     ...     -0.057270 -0.054910 -0.051792   \n",
       "1 -0.014743 -0.300167 -0.063522     ...     -0.057548 -0.045611 -0.071561   \n",
       "2 -0.053500 -0.129914 -0.932856     ...     -0.361437 -0.057405 -0.059111   \n",
       "3 -0.085092 -0.292447 -2.205114     ...     -0.058340 -0.105928 -0.542505   \n",
       "4 -0.058279 -0.079392 -1.248853     ...     -0.138195  0.042109 -0.056917   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  normAmount  \n",
       "0 -0.055885 -0.056607 -0.040957  2.976045 -0.162601 -0.054332   -0.046051  \n",
       "1  1.058161 -0.641967  3.363811  0.424565 -0.056385  0.052308   -0.045068  \n",
       "2 -1.944048 -1.632115 -0.031994 -1.712119  3.726690 -0.055316    0.924076  \n",
       "3  0.128057 -0.056475 -0.047488 -0.058267 -0.034562 -0.035444   -0.058541  \n",
       "4 -0.058688 -1.175004 -0.056022 -0.048289 -1.096804 -1.868680   -0.055720  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tensor to PD data frame\n",
    "#synthentic_data_df = pd.DataFrame(data=synthentic_data.data.numpy());\n",
    "synthentic_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 1 to Class column since they're all synthetic generated fraud data\n",
    "synthentic_data_df['Class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "data = data.drop(['Time','Amount'],axis=1)\n",
    "\n",
    "# Rearrange columns to the right order\n",
    "cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "        'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount', 'Class']\n",
    "data = data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (284807, 29)\n",
      "Shape of y: (284807, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(data.loc[:, data.columns != 'Class'])\n",
    "y = np.array(data.loc[:, data.columns == 'Class'])\n",
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape\n",
      "(199364, 29)\n",
      "xtest shape\n",
      "(85443, 29)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print('xtrain shape')\n",
    "print(X_train.shape)\n",
    "print('xtest shape')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the synthentic data\n",
    "X_synthentic = np.array(synthentic_data_df.loc[:, synthentic_data_df.columns != 'Class'])\n",
    "y_synthentic = np.array(synthentic_data_df.loc[:, synthentic_data_df.columns == 'Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_synthentic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add newly generated fraud data to the training data set\n",
    "new_X = np.concatenate((X_train, X_synthentic), axis=0)\n",
    "new_y = np.concatenate((y_train, y_synthentic), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(new_X, new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85284,    12],\n",
       "       [   36,   111]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/output/generator.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-ac5a9421c4e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"generator.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"discriminator.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/output/generator.pt'"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(generator, OUTPUT_PATH + \"generator.pt\")\n",
    "torch.save(discriminator, OUTPUT_PATH + \"discriminator.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
