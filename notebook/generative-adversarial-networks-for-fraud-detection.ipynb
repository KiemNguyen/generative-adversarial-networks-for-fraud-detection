{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import recall_score\n",
    "import torch\n",
    "#from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Cuda is running\n",
    "#torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomize the data, just to be sure not to get any pathological ordering effects that might harm the performane of Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124011</th>\n",
       "      <td>77147.0</td>\n",
       "      <td>-1.437314</td>\n",
       "      <td>-0.672735</td>\n",
       "      <td>0.925002</td>\n",
       "      <td>-0.728117</td>\n",
       "      <td>-0.498919</td>\n",
       "      <td>-1.088232</td>\n",
       "      <td>0.098312</td>\n",
       "      <td>0.294362</td>\n",
       "      <td>-1.763211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032172</td>\n",
       "      <td>-0.254662</td>\n",
       "      <td>0.569343</td>\n",
       "      <td>0.599137</td>\n",
       "      <td>-0.909906</td>\n",
       "      <td>0.070153</td>\n",
       "      <td>0.017838</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>149.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182926</th>\n",
       "      <td>125586.0</td>\n",
       "      <td>-1.188026</td>\n",
       "      <td>0.648003</td>\n",
       "      <td>0.195708</td>\n",
       "      <td>-2.300317</td>\n",
       "      <td>-1.876858</td>\n",
       "      <td>0.483442</td>\n",
       "      <td>-0.257970</td>\n",
       "      <td>-4.314653</td>\n",
       "      <td>1.394378</td>\n",
       "      <td>...</td>\n",
       "      <td>3.773234</td>\n",
       "      <td>-1.129681</td>\n",
       "      <td>-1.190132</td>\n",
       "      <td>0.066215</td>\n",
       "      <td>0.806729</td>\n",
       "      <td>0.669240</td>\n",
       "      <td>0.580496</td>\n",
       "      <td>0.090244</td>\n",
       "      <td>392.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>2364.0</td>\n",
       "      <td>1.571567</td>\n",
       "      <td>-0.852696</td>\n",
       "      <td>-0.411170</td>\n",
       "      <td>-1.735533</td>\n",
       "      <td>-0.574247</td>\n",
       "      <td>-0.298426</td>\n",
       "      <td>-0.606201</td>\n",
       "      <td>-0.165091</td>\n",
       "      <td>-2.384797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.546954</td>\n",
       "      <td>-1.269700</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>-0.896239</td>\n",
       "      <td>0.433003</td>\n",
       "      <td>-0.425964</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>-0.003500</td>\n",
       "      <td>19.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10848</th>\n",
       "      <td>18552.0</td>\n",
       "      <td>-0.265284</td>\n",
       "      <td>0.081207</td>\n",
       "      <td>1.595559</td>\n",
       "      <td>-1.671107</td>\n",
       "      <td>-0.359368</td>\n",
       "      <td>-0.158713</td>\n",
       "      <td>-0.188054</td>\n",
       "      <td>0.271057</td>\n",
       "      <td>2.710338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102560</td>\n",
       "      <td>0.673973</td>\n",
       "      <td>0.126516</td>\n",
       "      <td>-0.065758</td>\n",
       "      <td>-1.159539</td>\n",
       "      <td>-1.066871</td>\n",
       "      <td>0.283226</td>\n",
       "      <td>0.230268</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250966</th>\n",
       "      <td>155148.0</td>\n",
       "      <td>-1.071928</td>\n",
       "      <td>0.594211</td>\n",
       "      <td>-0.352817</td>\n",
       "      <td>-0.819417</td>\n",
       "      <td>1.439513</td>\n",
       "      <td>0.755137</td>\n",
       "      <td>0.594540</td>\n",
       "      <td>0.448423</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085680</td>\n",
       "      <td>0.088365</td>\n",
       "      <td>0.361190</td>\n",
       "      <td>-0.336159</td>\n",
       "      <td>-1.006584</td>\n",
       "      <td>0.176926</td>\n",
       "      <td>-0.098098</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>14.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "124011   77147.0 -1.437314 -0.672735  0.925002 -0.728117 -0.498919 -1.088232   \n",
       "182926  125586.0 -1.188026  0.648003  0.195708 -2.300317 -1.876858  0.483442   \n",
       "2807      2364.0  1.571567 -0.852696 -0.411170 -1.735533 -0.574247 -0.298426   \n",
       "10848    18552.0 -0.265284  0.081207  1.595559 -1.671107 -0.359368 -0.158713   \n",
       "250966  155148.0 -1.071928  0.594211 -0.352817 -0.819417  1.439513  0.755137   \n",
       "\n",
       "              V7        V8        V9  ...         V21       V22       V23  \\\n",
       "124011  0.098312  0.294362 -1.763211  ...   -0.032172 -0.254662  0.569343   \n",
       "182926 -0.257970 -4.314653  1.394378  ...    3.773234 -1.129681 -1.190132   \n",
       "2807   -0.606201 -0.165091 -2.384797  ...   -0.546954 -1.269700  0.009546   \n",
       "10848  -0.188054  0.271057  2.710338  ...    0.102560  0.673973  0.126516   \n",
       "250966  0.594540  0.448423  0.063034  ...   -0.085680  0.088365  0.361190   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "124011  0.599137 -0.909906  0.070153  0.017838  0.001861  149.20      0  \n",
       "182926  0.066215  0.806729  0.669240  0.580496  0.090244  392.00      0  \n",
       "2807   -0.896239  0.433003 -0.425964  0.000977 -0.003500   19.75      0  \n",
       "10848  -0.065758 -1.159539 -1.066871  0.283226  0.230268   11.85      0  \n",
       "250966 -0.336159 -1.006584  0.176926 -0.098098  0.001136   14.75      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.reindex(np.random.permutation(data.index))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.174225e-15</td>\n",
       "      <td>3.429687e-16</td>\n",
       "      <td>-1.386421e-15</td>\n",
       "      <td>2.073779e-15</td>\n",
       "      <td>9.939598e-16</td>\n",
       "      <td>1.493625e-15</td>\n",
       "      <td>-5.931037e-16</td>\n",
       "      <td>1.318317e-16</td>\n",
       "      <td>-2.414318e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.416845e-16</td>\n",
       "      <td>-3.515296e-16</td>\n",
       "      <td>2.727492e-16</td>\n",
       "      <td>4.482012e-15</td>\n",
       "      <td>5.203181e-16</td>\n",
       "      <td>1.689590e-15</td>\n",
       "      <td>-3.712632e-16</td>\n",
       "      <td>-1.159267e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.174225e-15  3.429687e-16 -1.386421e-15  2.073779e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.939598e-16  1.493625e-15 -5.931037e-16  1.318317e-16 -2.414318e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...        1.416845e-16 -3.515296e-16  2.727492e-16  4.482012e-15   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.203181e-16  1.689590e-15 -3.712632e-16 -1.159267e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Frequency')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = pd.value_counts(data['Class'], sort = True).sort_index()\n",
    "classes.plot(kind = 'bar')\n",
    "plt.title(\"Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Class')['Class'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Data is hihgly imbalance. 284315 Normal transaction vs 492 Fraud transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run with Normalising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns != 'Class']\n",
    "y = data.loc[:, data.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape\n",
      "(199364, 29)\n",
      "xtest shape\n",
      "(85443, 29)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "print('xtrain shape')\n",
    "print(X_train.shape)\n",
    "print('xtest shape')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the training data and test data\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit classifier to a model\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85291,     5],\n",
       "       [   39,   108]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     85296\n",
      "          1       0.96      0.73      0.83       147\n",
      "\n",
      "avg / total       1.00      1.00      1.00     85443\n",
      "\n",
      "Accuracy : 0.999485\n",
      "Area under the curve : 0.867318\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print('Accuracy : %f' % (metrics.accuracy_score(y_test, y_pred)))\n",
    "print('Area under the curve : %f' % (metrics.roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run with Over Sampling data using SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bbd82a65bc57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"creditcard.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10     ...           V21       V22       V23  \\\n",
       "0  0.098698  0.363787  0.090794     ...     -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425 -0.166974     ...     -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654  0.207643     ...      0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024 -0.054952     ...     -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739  0.753074     ...     -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  normAmount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0    0.244964  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   -0.342475  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0    1.160686  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0    0.140534  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0   -0.073403  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "data = data.drop(['Time','Amount'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (284807, 29)\n",
      "Shape of y: (284807, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(data.ix[:, data.columns != 'Class'])\n",
    "y = np.array(data.ix[:, data.columns == 'Class'])\n",
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape\n",
      "(199364, 29)\n",
      "xtest shape\n",
      "(85443, 29)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print('xtrain shape')\n",
    "print(X_train.shape)\n",
    "print('xtest shape')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over Sampling data using SMOTE\n",
    "smote = SMOTE(random_state=2)\n",
    "X_train_resample, y_train_resample = smote.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train_resample, y_train_resample.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85283,    13],\n",
       "       [   31,   116]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a Confision Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     85296\n",
      "          1       0.90      0.79      0.84       147\n",
      "\n",
      "avg / total       1.00      1.00      1.00     85443\n",
      "\n",
      "Accuracy : 0.999485\n",
      "Area under the curve : 0.894482\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print('Accuracy : %f' % (metrics.accuracy_score(y_test, y_pred)))\n",
    "print('Area under the curve : %f' % (metrics.roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run with GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "OUTPUT_PATH = '/output/'\n",
    "MODE = 'wgan-gp'\n",
    "RESTORE_MODE = False # If this flag is True, it will continue to train from the saved model.\n",
    "\n",
    "# Custom DataLoader\n",
    "class FraudDataset(Dataset):\n",
    "    \n",
    "    # Initialize the data\n",
    "    def __init__(self):\n",
    "        data = pd.read_csv(\"creditcard.csv\")\n",
    "        data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "        data = data.drop(['Time','Amount'],axis=1)\n",
    "        \n",
    "        # Rearrange columns to the right order\n",
    "        cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "        'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount', 'Class']\n",
    "        data = data[cols]\n",
    "        \n",
    "        fraud_data = data.loc[data['Class']==1]\n",
    "        fraud_data = fraud_data.drop('Class', 1)\n",
    "        self.len = fraud_data.shape[0]\n",
    "        \n",
    "        self.fraud_data = torch.FloatTensor(np.array(fraud_data))\n",
    "        \n",
    "        #self.X = np.array(data.loc[:, data.columns != 'Class'])\n",
    "        #self.y = np.array(data.loc[:, data.columns == 'Class'])\n",
    "        \n",
    "        #self.X = torch.FloatTensor(self.X)\n",
    "        #self.y = torch.FloatTensor(self.y)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.fraud_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESTORE_MODE:\n",
    "    generator = torch.load(OUTPUT_PATH + \"generator.pt\" )\n",
    "    discriminator = torch.load(OUTPUT_PATH + \"discriminator.pt\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FraudDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator's paragrams\n",
    "g_input_size = 29     # Random noise dimension\n",
    "g_hidden_size = 50   # Generator complexity\n",
    "g_output_size = 1   \n",
    "g_learning_rate = 0.0002\n",
    "\n",
    "#Discriminator's paragrams\n",
    "d_input_size = 29   # Minibatch size\n",
    "d_hidden_size = 50  # Discriminator complexity\n",
    "d_output_size = 1   # Single dimension for 'real' vs. 'fake'\n",
    "d_learning_rate = 0.0002\n",
    "\n",
    "minibatch_size = d_input_size\n",
    "\n",
    "num_epochs = 10000\n",
    "print_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ELU (Exponential Linear Unit) function tends to converge cost to zero faster and produce more accurate results\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.sigmoid(self.map2(x))\n",
    "        return self.map3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.map2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.map3(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "discriminator = Discriminator(input_size=d_input_size, hidden_size=d_hidden_size, output_size=d_output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these 2 lines to run on GPU\n",
    "#generator.cuda()\n",
    "#discriminator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(disc):\n",
    "    h=0.1\n",
    "    x_min, x_max = data[:, 0].min() - .5, data[:, 0].max() + .5\n",
    "    y_min, y_max = data[:, 1].min() - .5, data[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    x_test = np.c_[xx.ravel(), yy.ravel()]\n",
    "    y_hat_test = disc.forward_with_sigmoid(Variable(torch.from_numpy(x_test).float()))\n",
    "\n",
    "    plt.pcolormesh(xx, yy, y_hat_test.data.numpy().reshape(xx.shape), cmap=plt.cm.Paired)\n",
    "    plt.colorbar()\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y*20, alpha=0.1, cmap=plt.cm.flag, s=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Binary Cross Entropy loss\n",
    "BCE_loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizers\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=d_learning_rate/2, betas=(beta_1, beta_2))\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=g_learning_rate, betas=(beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Discriminator Loss: 0.914, Generator Loss: 0.811\n",
      "Epoch 11 - Discriminator Loss: 0.296, Generator Loss: 3.832\n",
      "Epoch 21 - Discriminator Loss: 0.277, Generator Loss: 3.618\n",
      "Epoch 31 - Discriminator Loss: 0.243, Generator Loss: 3.821\n",
      "Epoch 41 - Discriminator Loss: 0.215, Generator Loss: 3.946\n",
      "Epoch 51 - Discriminator Loss: 0.202, Generator Loss: 4.157\n",
      "Epoch 61 - Discriminator Loss: 0.166, Generator Loss: 4.134\n",
      "Epoch 71 - Discriminator Loss: 0.142, Generator Loss: 4.327\n",
      "Epoch 81 - Discriminator Loss: 0.128, Generator Loss: 4.560\n",
      "Epoch 91 - Discriminator Loss: 0.122, Generator Loss: 4.766\n",
      "Epoch 101 - Discriminator Loss: 0.093, Generator Loss: 5.351\n",
      "Epoch 111 - Discriminator Loss: 0.079, Generator Loss: 5.788\n",
      "Epoch 121 - Discriminator Loss: 0.058, Generator Loss: 5.919\n",
      "Epoch 131 - Discriminator Loss: 0.037, Generator Loss: 6.535\n",
      "Epoch 141 - Discriminator Loss: 0.034, Generator Loss: 7.217\n",
      "Epoch 151 - Discriminator Loss: 0.028, Generator Loss: 7.471\n",
      "Epoch 161 - Discriminator Loss: 0.021, Generator Loss: 8.188\n",
      "Epoch 171 - Discriminator Loss: 0.014, Generator Loss: 9.033\n",
      "Epoch 181 - Discriminator Loss: 0.017, Generator Loss: 9.462\n",
      "Epoch 191 - Discriminator Loss: 0.015, Generator Loss: 10.081\n",
      "Epoch 201 - Discriminator Loss: 0.011, Generator Loss: 10.755\n",
      "Epoch 211 - Discriminator Loss: 0.013, Generator Loss: 11.013\n",
      "Epoch 221 - Discriminator Loss: 0.009, Generator Loss: 11.384\n",
      "Epoch 231 - Discriminator Loss: 0.006, Generator Loss: 12.074\n",
      "Epoch 241 - Discriminator Loss: 0.005, Generator Loss: 12.377\n",
      "Epoch 251 - Discriminator Loss: 0.004, Generator Loss: 12.649\n",
      "Epoch 261 - Discriminator Loss: 0.003, Generator Loss: 13.077\n",
      "Epoch 271 - Discriminator Loss: 0.003, Generator Loss: 13.290\n",
      "Epoch 281 - Discriminator Loss: 0.006, Generator Loss: 14.418\n",
      "Epoch 291 - Discriminator Loss: 0.004, Generator Loss: 15.214\n",
      "Epoch 301 - Discriminator Loss: 0.003, Generator Loss: 15.149\n",
      "Epoch 311 - Discriminator Loss: 0.004, Generator Loss: 15.163\n",
      "Epoch 321 - Discriminator Loss: 0.003, Generator Loss: 16.285\n",
      "Epoch 331 - Discriminator Loss: 0.002, Generator Loss: 16.448\n",
      "Epoch 341 - Discriminator Loss: 0.003, Generator Loss: 16.335\n",
      "Epoch 351 - Discriminator Loss: 0.005, Generator Loss: 15.853\n",
      "Epoch 361 - Discriminator Loss: 0.001, Generator Loss: 17.257\n",
      "Epoch 371 - Discriminator Loss: 0.001, Generator Loss: 17.160\n",
      "Epoch 381 - Discriminator Loss: 0.013, Generator Loss: 19.413\n",
      "Epoch 391 - Discriminator Loss: 0.001, Generator Loss: 18.849\n",
      "Epoch 401 - Discriminator Loss: 0.000, Generator Loss: 17.426\n",
      "Epoch 411 - Discriminator Loss: 0.000, Generator Loss: 17.572\n",
      "Epoch 421 - Discriminator Loss: 0.000, Generator Loss: 18.066\n",
      "Epoch 431 - Discriminator Loss: 0.015, Generator Loss: 19.919\n",
      "Epoch 441 - Discriminator Loss: 0.030, Generator Loss: 20.529\n",
      "Epoch 451 - Discriminator Loss: 0.003, Generator Loss: 20.111\n",
      "Epoch 461 - Discriminator Loss: 0.000, Generator Loss: 18.132\n",
      "Epoch 471 - Discriminator Loss: 0.000, Generator Loss: 17.236\n",
      "Epoch 481 - Discriminator Loss: 0.001, Generator Loss: 18.916\n",
      "Epoch 491 - Discriminator Loss: 0.024, Generator Loss: 20.663\n",
      "Epoch 501 - Discriminator Loss: 0.037, Generator Loss: 23.099\n",
      "Epoch 511 - Discriminator Loss: 0.001, Generator Loss: 21.485\n",
      "Epoch 521 - Discriminator Loss: 0.000, Generator Loss: 19.366\n",
      "Epoch 531 - Discriminator Loss: 0.000, Generator Loss: 17.799\n",
      "Epoch 541 - Discriminator Loss: 0.000, Generator Loss: 16.970\n",
      "Epoch 551 - Discriminator Loss: 0.000, Generator Loss: 19.650\n",
      "Epoch 561 - Discriminator Loss: 0.057, Generator Loss: 23.312\n",
      "Epoch 571 - Discriminator Loss: 0.059, Generator Loss: 24.476\n",
      "Epoch 581 - Discriminator Loss: 0.000, Generator Loss: 22.327\n",
      "Epoch 591 - Discriminator Loss: 0.000, Generator Loss: 20.437\n",
      "Epoch 601 - Discriminator Loss: 0.000, Generator Loss: 18.726\n",
      "Epoch 611 - Discriminator Loss: 0.000, Generator Loss: 17.037\n",
      "Epoch 621 - Discriminator Loss: 0.000, Generator Loss: 17.048\n",
      "Epoch 631 - Discriminator Loss: 0.003, Generator Loss: 22.377\n",
      "Epoch 641 - Discriminator Loss: 0.101, Generator Loss: 25.752\n",
      "Epoch 651 - Discriminator Loss: 0.012, Generator Loss: 24.954\n",
      "Epoch 661 - Discriminator Loss: 0.002, Generator Loss: 25.333\n",
      "Epoch 671 - Discriminator Loss: 0.000, Generator Loss: 23.660\n",
      "Epoch 681 - Discriminator Loss: 0.000, Generator Loss: 22.152\n",
      "Epoch 691 - Discriminator Loss: 0.000, Generator Loss: 20.430\n",
      "Epoch 701 - Discriminator Loss: 0.000, Generator Loss: 18.914\n",
      "Epoch 711 - Discriminator Loss: 0.000, Generator Loss: 20.090\n",
      "Epoch 721 - Discriminator Loss: 0.127, Generator Loss: 25.833\n",
      "Epoch 731 - Discriminator Loss: 0.100, Generator Loss: 25.794\n",
      "Epoch 741 - Discriminator Loss: 0.008, Generator Loss: 26.398\n",
      "Epoch 751 - Discriminator Loss: 0.027, Generator Loss: 25.996\n",
      "Epoch 761 - Discriminator Loss: 0.016, Generator Loss: 25.214\n",
      "Epoch 771 - Discriminator Loss: 0.001, Generator Loss: 24.524\n",
      "Epoch 781 - Discriminator Loss: 0.041, Generator Loss: 24.560\n",
      "Epoch 791 - Discriminator Loss: 0.010, Generator Loss: 23.666\n",
      "Epoch 801 - Discriminator Loss: 0.000, Generator Loss: 21.549\n",
      "Epoch 811 - Discriminator Loss: 0.000, Generator Loss: 19.753\n",
      "Epoch 821 - Discriminator Loss: 0.000, Generator Loss: 18.236\n",
      "Epoch 831 - Discriminator Loss: 0.000, Generator Loss: 16.856\n",
      "Epoch 841 - Discriminator Loss: 0.000, Generator Loss: 17.651\n",
      "Epoch 851 - Discriminator Loss: 0.000, Generator Loss: 19.889\n",
      "Epoch 861 - Discriminator Loss: 0.000, Generator Loss: 21.342\n",
      "Epoch 871 - Discriminator Loss: 0.000, Generator Loss: 22.536\n",
      "Epoch 881 - Discriminator Loss: 0.000, Generator Loss: 23.011\n",
      "Epoch 891 - Discriminator Loss: 0.085, Generator Loss: 25.721\n",
      "Epoch 901 - Discriminator Loss: 0.062, Generator Loss: 25.913\n",
      "Epoch 911 - Discriminator Loss: 0.102, Generator Loss: 26.443\n",
      "Epoch 921 - Discriminator Loss: 0.033, Generator Loss: 26.614\n",
      "Epoch 931 - Discriminator Loss: 0.079, Generator Loss: 25.478\n",
      "Epoch 941 - Discriminator Loss: 0.181, Generator Loss: 25.213\n",
      "Epoch 951 - Discriminator Loss: 0.002, Generator Loss: 23.344\n",
      "Epoch 961 - Discriminator Loss: 0.000, Generator Loss: 21.097\n",
      "Epoch 971 - Discriminator Loss: 0.000, Generator Loss: 19.478\n",
      "Epoch 981 - Discriminator Loss: 0.000, Generator Loss: 17.722\n",
      "Epoch 991 - Discriminator Loss: 0.000, Generator Loss: 17.061\n",
      "Epoch 1001 - Discriminator Loss: 0.000, Generator Loss: 18.849\n",
      "Epoch 1011 - Discriminator Loss: 0.000, Generator Loss: 20.893\n",
      "Epoch 1021 - Discriminator Loss: 0.000, Generator Loss: 22.617\n",
      "Epoch 1031 - Discriminator Loss: 0.194, Generator Loss: 24.653\n",
      "Epoch 1041 - Discriminator Loss: 0.019, Generator Loss: 26.157\n",
      "Epoch 1051 - Discriminator Loss: 0.001, Generator Loss: 25.502\n",
      "Epoch 1061 - Discriminator Loss: 0.000, Generator Loss: 24.403\n",
      "Epoch 1071 - Discriminator Loss: 0.000, Generator Loss: 23.737\n",
      "Epoch 1081 - Discriminator Loss: 0.019, Generator Loss: 25.597\n",
      "Epoch 1091 - Discriminator Loss: 0.043, Generator Loss: 25.732\n",
      "Epoch 1101 - Discriminator Loss: 0.010, Generator Loss: 26.286\n",
      "Epoch 1111 - Discriminator Loss: 0.017, Generator Loss: 26.197\n",
      "Epoch 1121 - Discriminator Loss: 0.004, Generator Loss: 25.884\n",
      "Epoch 1131 - Discriminator Loss: 0.037, Generator Loss: 25.735\n",
      "Epoch 1141 - Discriminator Loss: 0.014, Generator Loss: 25.905\n",
      "Epoch 1151 - Discriminator Loss: 0.005, Generator Loss: 25.827\n",
      "Epoch 1161 - Discriminator Loss: 0.010, Generator Loss: 25.815\n",
      "Epoch 1171 - Discriminator Loss: 0.007, Generator Loss: 25.950\n",
      "Epoch 1181 - Discriminator Loss: 0.025, Generator Loss: 26.107\n",
      "Epoch 1191 - Discriminator Loss: 0.057, Generator Loss: 25.996\n",
      "Epoch 1201 - Discriminator Loss: 0.060, Generator Loss: 25.517\n",
      "Epoch 1211 - Discriminator Loss: 0.007, Generator Loss: 25.427\n",
      "Epoch 1221 - Discriminator Loss: 0.002, Generator Loss: 25.585\n",
      "Epoch 1231 - Discriminator Loss: 0.009, Generator Loss: 25.365\n",
      "Epoch 1241 - Discriminator Loss: 0.001, Generator Loss: 25.811\n",
      "Epoch 1251 - Discriminator Loss: 0.000, Generator Loss: 25.591\n",
      "Epoch 1261 - Discriminator Loss: 0.000, Generator Loss: 25.391\n",
      "Epoch 1271 - Discriminator Loss: 0.003, Generator Loss: 25.674\n",
      "Epoch 1281 - Discriminator Loss: 0.003, Generator Loss: 25.765\n",
      "Epoch 1291 - Discriminator Loss: 0.001, Generator Loss: 25.883\n",
      "Epoch 1301 - Discriminator Loss: 0.023, Generator Loss: 25.885\n",
      "Epoch 1311 - Discriminator Loss: 0.001, Generator Loss: 25.813\n",
      "Epoch 1321 - Discriminator Loss: 0.011, Generator Loss: 25.868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1331 - Discriminator Loss: 0.009, Generator Loss: 25.889\n",
      "Epoch 1341 - Discriminator Loss: 0.004, Generator Loss: 25.771\n",
      "Epoch 1351 - Discriminator Loss: 0.006, Generator Loss: 25.933\n",
      "Epoch 1361 - Discriminator Loss: 0.000, Generator Loss: 26.079\n",
      "Epoch 1371 - Discriminator Loss: 0.013, Generator Loss: 25.941\n",
      "Epoch 1381 - Discriminator Loss: 0.002, Generator Loss: 26.041\n",
      "Epoch 1391 - Discriminator Loss: 0.006, Generator Loss: 25.829\n",
      "Epoch 1401 - Discriminator Loss: 0.002, Generator Loss: 26.066\n",
      "Epoch 1411 - Discriminator Loss: 0.039, Generator Loss: 25.939\n",
      "Epoch 1421 - Discriminator Loss: 0.003, Generator Loss: 25.908\n",
      "Epoch 1431 - Discriminator Loss: 0.002, Generator Loss: 25.725\n",
      "Epoch 1441 - Discriminator Loss: 0.000, Generator Loss: 25.962\n",
      "Epoch 1451 - Discriminator Loss: 0.000, Generator Loss: 25.828\n",
      "Epoch 1461 - Discriminator Loss: 0.001, Generator Loss: 25.838\n",
      "Epoch 1471 - Discriminator Loss: 0.015, Generator Loss: 25.583\n",
      "Epoch 1481 - Discriminator Loss: 0.000, Generator Loss: 25.946\n",
      "Epoch 1491 - Discriminator Loss: 0.002, Generator Loss: 25.637\n",
      "Epoch 1501 - Discriminator Loss: 0.000, Generator Loss: 25.942\n",
      "Epoch 1511 - Discriminator Loss: 0.002, Generator Loss: 25.574\n",
      "Epoch 1521 - Discriminator Loss: 0.006, Generator Loss: 25.744\n",
      "Epoch 1531 - Discriminator Loss: 0.008, Generator Loss: 26.056\n",
      "Epoch 1541 - Discriminator Loss: 0.033, Generator Loss: 26.028\n",
      "Epoch 1551 - Discriminator Loss: 0.023, Generator Loss: 25.925\n",
      "Epoch 1561 - Discriminator Loss: 0.000, Generator Loss: 25.915\n",
      "Epoch 1571 - Discriminator Loss: 0.025, Generator Loss: 26.271\n",
      "Epoch 1581 - Discriminator Loss: 0.056, Generator Loss: 25.961\n",
      "Epoch 1591 - Discriminator Loss: 0.016, Generator Loss: 26.013\n",
      "Epoch 1601 - Discriminator Loss: 0.015, Generator Loss: 26.180\n",
      "Epoch 1611 - Discriminator Loss: 0.001, Generator Loss: 25.907\n",
      "Epoch 1621 - Discriminator Loss: 0.001, Generator Loss: 26.151\n",
      "Epoch 1631 - Discriminator Loss: 0.013, Generator Loss: 26.137\n",
      "Epoch 1641 - Discriminator Loss: 0.001, Generator Loss: 25.780\n",
      "Epoch 1651 - Discriminator Loss: 0.000, Generator Loss: 25.818\n",
      "Epoch 1661 - Discriminator Loss: 0.000, Generator Loss: 26.278\n",
      "Epoch 1671 - Discriminator Loss: 0.009, Generator Loss: 26.564\n",
      "Epoch 1681 - Discriminator Loss: 0.013, Generator Loss: 26.525\n",
      "Epoch 1691 - Discriminator Loss: 0.001, Generator Loss: 26.352\n",
      "Epoch 1701 - Discriminator Loss: 0.000, Generator Loss: 26.048\n",
      "Epoch 1711 - Discriminator Loss: 0.001, Generator Loss: 26.198\n",
      "Epoch 1721 - Discriminator Loss: 0.003, Generator Loss: 26.272\n",
      "Epoch 1731 - Discriminator Loss: 0.018, Generator Loss: 26.008\n",
      "Epoch 1741 - Discriminator Loss: 0.003, Generator Loss: 26.275\n",
      "Epoch 1751 - Discriminator Loss: 0.007, Generator Loss: 26.445\n",
      "Epoch 1761 - Discriminator Loss: 0.000, Generator Loss: 26.059\n",
      "Epoch 1771 - Discriminator Loss: 0.001, Generator Loss: 26.212\n",
      "Epoch 1781 - Discriminator Loss: 0.001, Generator Loss: 26.169\n",
      "Epoch 1791 - Discriminator Loss: 0.001, Generator Loss: 26.421\n",
      "Epoch 1801 - Discriminator Loss: 0.001, Generator Loss: 26.190\n",
      "Epoch 1811 - Discriminator Loss: 0.011, Generator Loss: 26.249\n",
      "Epoch 1821 - Discriminator Loss: 0.004, Generator Loss: 26.030\n",
      "Epoch 1831 - Discriminator Loss: 0.001, Generator Loss: 26.571\n",
      "Epoch 1841 - Discriminator Loss: 0.000, Generator Loss: 26.232\n",
      "Epoch 1851 - Discriminator Loss: 0.001, Generator Loss: 26.467\n",
      "Epoch 1861 - Discriminator Loss: 0.001, Generator Loss: 26.297\n",
      "Epoch 1871 - Discriminator Loss: 0.001, Generator Loss: 26.242\n",
      "Epoch 1881 - Discriminator Loss: 0.013, Generator Loss: 26.615\n",
      "Epoch 1891 - Discriminator Loss: 0.001, Generator Loss: 26.207\n",
      "Epoch 1901 - Discriminator Loss: 0.005, Generator Loss: 26.608\n",
      "Epoch 1911 - Discriminator Loss: 0.007, Generator Loss: 26.101\n",
      "Epoch 1921 - Discriminator Loss: 0.013, Generator Loss: 26.657\n",
      "Epoch 1931 - Discriminator Loss: 0.001, Generator Loss: 26.081\n",
      "Epoch 1941 - Discriminator Loss: 0.001, Generator Loss: 26.351\n",
      "Epoch 1951 - Discriminator Loss: 0.029, Generator Loss: 26.156\n",
      "Epoch 1961 - Discriminator Loss: 0.006, Generator Loss: 26.384\n",
      "Epoch 1971 - Discriminator Loss: 0.003, Generator Loss: 26.282\n",
      "Epoch 1981 - Discriminator Loss: 0.002, Generator Loss: 26.213\n",
      "Epoch 1991 - Discriminator Loss: 0.000, Generator Loss: 26.124\n",
      "Epoch 2001 - Discriminator Loss: 0.001, Generator Loss: 26.372\n",
      "Epoch 2011 - Discriminator Loss: 0.002, Generator Loss: 26.510\n",
      "Epoch 2021 - Discriminator Loss: 0.025, Generator Loss: 25.888\n",
      "Epoch 2031 - Discriminator Loss: 0.000, Generator Loss: 25.850\n",
      "Epoch 2041 - Discriminator Loss: 0.000, Generator Loss: 25.803\n",
      "Epoch 2051 - Discriminator Loss: 0.003, Generator Loss: 26.474\n",
      "Epoch 2061 - Discriminator Loss: 0.000, Generator Loss: 26.087\n",
      "Epoch 2071 - Discriminator Loss: 0.025, Generator Loss: 25.999\n",
      "Epoch 2081 - Discriminator Loss: 0.015, Generator Loss: 26.095\n",
      "Epoch 2091 - Discriminator Loss: 0.000, Generator Loss: 25.996\n",
      "Epoch 2101 - Discriminator Loss: 0.001, Generator Loss: 26.210\n",
      "Epoch 2111 - Discriminator Loss: 0.026, Generator Loss: 26.550\n",
      "Epoch 2121 - Discriminator Loss: 0.001, Generator Loss: 26.206\n",
      "Epoch 2131 - Discriminator Loss: 0.003, Generator Loss: 26.463\n",
      "Epoch 2141 - Discriminator Loss: 0.030, Generator Loss: 26.537\n",
      "Epoch 2151 - Discriminator Loss: 0.000, Generator Loss: 26.215\n",
      "Epoch 2161 - Discriminator Loss: 0.001, Generator Loss: 26.341\n",
      "Epoch 2171 - Discriminator Loss: 0.002, Generator Loss: 26.640\n",
      "Epoch 2181 - Discriminator Loss: 0.007, Generator Loss: 26.427\n",
      "Epoch 2191 - Discriminator Loss: 0.024, Generator Loss: 26.648\n",
      "Epoch 2201 - Discriminator Loss: 0.013, Generator Loss: 26.367\n",
      "Epoch 2211 - Discriminator Loss: 0.002, Generator Loss: 26.319\n",
      "Epoch 2221 - Discriminator Loss: 0.004, Generator Loss: 26.712\n",
      "Epoch 2231 - Discriminator Loss: 0.020, Generator Loss: 26.403\n",
      "Epoch 2241 - Discriminator Loss: 0.000, Generator Loss: 26.200\n",
      "Epoch 2251 - Discriminator Loss: 0.001, Generator Loss: 26.669\n",
      "Epoch 2261 - Discriminator Loss: 0.019, Generator Loss: 26.276\n",
      "Epoch 2271 - Discriminator Loss: 0.000, Generator Loss: 26.380\n",
      "Epoch 2281 - Discriminator Loss: 0.000, Generator Loss: 26.325\n",
      "Epoch 2291 - Discriminator Loss: 0.000, Generator Loss: 26.039\n",
      "Epoch 2301 - Discriminator Loss: 0.010, Generator Loss: 26.284\n",
      "Epoch 2311 - Discriminator Loss: 0.016, Generator Loss: 26.568\n",
      "Epoch 2321 - Discriminator Loss: 0.001, Generator Loss: 26.532\n",
      "Epoch 2331 - Discriminator Loss: 0.014, Generator Loss: 26.416\n",
      "Epoch 2341 - Discriminator Loss: 0.001, Generator Loss: 26.002\n",
      "Epoch 2351 - Discriminator Loss: 0.000, Generator Loss: 25.928\n",
      "Epoch 2361 - Discriminator Loss: 0.000, Generator Loss: 25.702\n",
      "Epoch 2371 - Discriminator Loss: 0.008, Generator Loss: 26.289\n",
      "Epoch 2381 - Discriminator Loss: 0.001, Generator Loss: 26.123\n",
      "Epoch 2391 - Discriminator Loss: 0.000, Generator Loss: 26.135\n",
      "Epoch 2401 - Discriminator Loss: 0.001, Generator Loss: 26.169\n",
      "Epoch 2411 - Discriminator Loss: 0.000, Generator Loss: 26.373\n",
      "Epoch 2421 - Discriminator Loss: 0.008, Generator Loss: 26.390\n",
      "Epoch 2431 - Discriminator Loss: 0.020, Generator Loss: 26.251\n",
      "Epoch 2441 - Discriminator Loss: 0.002, Generator Loss: 26.324\n",
      "Epoch 2451 - Discriminator Loss: 0.001, Generator Loss: 26.485\n",
      "Epoch 2461 - Discriminator Loss: 0.001, Generator Loss: 26.596\n",
      "Epoch 2471 - Discriminator Loss: 0.002, Generator Loss: 26.204\n",
      "Epoch 2481 - Discriminator Loss: 0.000, Generator Loss: 26.189\n",
      "Epoch 2491 - Discriminator Loss: 0.010, Generator Loss: 26.377\n",
      "Epoch 2501 - Discriminator Loss: 0.002, Generator Loss: 26.174\n",
      "Epoch 2511 - Discriminator Loss: 0.000, Generator Loss: 26.119\n",
      "Epoch 2521 - Discriminator Loss: 0.002, Generator Loss: 26.343\n",
      "Epoch 2531 - Discriminator Loss: 0.004, Generator Loss: 26.279\n",
      "Epoch 2541 - Discriminator Loss: 0.010, Generator Loss: 26.566\n",
      "Epoch 2551 - Discriminator Loss: 0.006, Generator Loss: 26.195\n",
      "Epoch 2561 - Discriminator Loss: 0.014, Generator Loss: 26.638\n",
      "Epoch 2571 - Discriminator Loss: 0.002, Generator Loss: 26.512\n",
      "Epoch 2581 - Discriminator Loss: 0.000, Generator Loss: 26.102\n",
      "Epoch 2591 - Discriminator Loss: 0.003, Generator Loss: 26.805\n",
      "Epoch 2601 - Discriminator Loss: 0.001, Generator Loss: 26.408\n",
      "Epoch 2611 - Discriminator Loss: 0.001, Generator Loss: 26.279\n",
      "Epoch 2621 - Discriminator Loss: 0.003, Generator Loss: 26.074\n",
      "Epoch 2631 - Discriminator Loss: 0.000, Generator Loss: 26.408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2641 - Discriminator Loss: 0.000, Generator Loss: 26.314\n",
      "Epoch 2651 - Discriminator Loss: 0.003, Generator Loss: 26.168\n",
      "Epoch 2661 - Discriminator Loss: 0.000, Generator Loss: 26.311\n",
      "Epoch 2671 - Discriminator Loss: 0.001, Generator Loss: 26.638\n",
      "Epoch 2681 - Discriminator Loss: 0.007, Generator Loss: 26.141\n",
      "Epoch 2691 - Discriminator Loss: 0.000, Generator Loss: 26.302\n",
      "Epoch 2701 - Discriminator Loss: 0.006, Generator Loss: 26.596\n",
      "Epoch 2711 - Discriminator Loss: 0.001, Generator Loss: 26.428\n",
      "Epoch 2721 - Discriminator Loss: 0.009, Generator Loss: 26.741\n",
      "Epoch 2731 - Discriminator Loss: 0.000, Generator Loss: 26.655\n",
      "Epoch 2741 - Discriminator Loss: 0.004, Generator Loss: 26.137\n",
      "Epoch 2751 - Discriminator Loss: 0.004, Generator Loss: 26.546\n",
      "Epoch 2761 - Discriminator Loss: 0.001, Generator Loss: 26.366\n",
      "Epoch 2771 - Discriminator Loss: 0.000, Generator Loss: 26.591\n",
      "Epoch 2781 - Discriminator Loss: 0.000, Generator Loss: 26.006\n",
      "Epoch 2791 - Discriminator Loss: 0.001, Generator Loss: 26.345\n",
      "Epoch 2801 - Discriminator Loss: 0.000, Generator Loss: 26.413\n",
      "Epoch 2811 - Discriminator Loss: 0.000, Generator Loss: 26.075\n",
      "Epoch 2821 - Discriminator Loss: 0.002, Generator Loss: 26.472\n",
      "Epoch 2831 - Discriminator Loss: 0.007, Generator Loss: 26.342\n",
      "Epoch 2841 - Discriminator Loss: 0.002, Generator Loss: 26.368\n",
      "Epoch 2851 - Discriminator Loss: 0.004, Generator Loss: 26.543\n",
      "Epoch 2861 - Discriminator Loss: 0.000, Generator Loss: 26.545\n",
      "Epoch 2871 - Discriminator Loss: 0.001, Generator Loss: 26.474\n",
      "Epoch 2881 - Discriminator Loss: 0.006, Generator Loss: 26.492\n",
      "Epoch 2891 - Discriminator Loss: 0.003, Generator Loss: 26.457\n",
      "Epoch 2901 - Discriminator Loss: 0.001, Generator Loss: 26.265\n",
      "Epoch 2911 - Discriminator Loss: 0.005, Generator Loss: 26.300\n",
      "Epoch 2921 - Discriminator Loss: 0.008, Generator Loss: 26.605\n",
      "Epoch 2931 - Discriminator Loss: 0.000, Generator Loss: 26.583\n",
      "Epoch 2941 - Discriminator Loss: 0.001, Generator Loss: 26.766\n",
      "Epoch 2951 - Discriminator Loss: 0.001, Generator Loss: 26.390\n",
      "Epoch 2961 - Discriminator Loss: 0.009, Generator Loss: 26.553\n",
      "Epoch 2971 - Discriminator Loss: 0.018, Generator Loss: 26.065\n",
      "Epoch 2981 - Discriminator Loss: 0.000, Generator Loss: 26.562\n",
      "Epoch 2991 - Discriminator Loss: 0.000, Generator Loss: 26.539\n",
      "Epoch 3001 - Discriminator Loss: 0.001, Generator Loss: 26.497\n",
      "Epoch 3011 - Discriminator Loss: 0.000, Generator Loss: 26.413\n",
      "Epoch 3021 - Discriminator Loss: 0.005, Generator Loss: 26.374\n",
      "Epoch 3031 - Discriminator Loss: 0.000, Generator Loss: 26.618\n",
      "Epoch 3041 - Discriminator Loss: 0.013, Generator Loss: 26.629\n",
      "Epoch 3051 - Discriminator Loss: 0.001, Generator Loss: 26.366\n",
      "Epoch 3061 - Discriminator Loss: 0.000, Generator Loss: 26.705\n",
      "Epoch 3071 - Discriminator Loss: 0.004, Generator Loss: 26.468\n",
      "Epoch 3081 - Discriminator Loss: 0.006, Generator Loss: 26.188\n",
      "Epoch 3091 - Discriminator Loss: 0.000, Generator Loss: 26.460\n",
      "Epoch 3101 - Discriminator Loss: 0.001, Generator Loss: 26.411\n",
      "Epoch 3111 - Discriminator Loss: 0.001, Generator Loss: 26.478\n",
      "Epoch 3121 - Discriminator Loss: 0.007, Generator Loss: 26.646\n",
      "Epoch 3131 - Discriminator Loss: 0.000, Generator Loss: 26.390\n",
      "Epoch 3141 - Discriminator Loss: 0.001, Generator Loss: 26.451\n",
      "Epoch 3151 - Discriminator Loss: 0.002, Generator Loss: 26.546\n",
      "Epoch 3161 - Discriminator Loss: 0.001, Generator Loss: 26.555\n",
      "Epoch 3171 - Discriminator Loss: 0.002, Generator Loss: 26.791\n",
      "Epoch 3181 - Discriminator Loss: 0.000, Generator Loss: 26.245\n",
      "Epoch 3191 - Discriminator Loss: 0.001, Generator Loss: 26.198\n",
      "Epoch 3201 - Discriminator Loss: 0.007, Generator Loss: 26.401\n",
      "Epoch 3211 - Discriminator Loss: 0.001, Generator Loss: 26.615\n",
      "Epoch 3221 - Discriminator Loss: 0.017, Generator Loss: 26.528\n",
      "Epoch 3231 - Discriminator Loss: 0.000, Generator Loss: 26.433\n",
      "Epoch 3241 - Discriminator Loss: 0.026, Generator Loss: 26.377\n",
      "Epoch 3251 - Discriminator Loss: 0.000, Generator Loss: 26.521\n",
      "Epoch 3261 - Discriminator Loss: 0.001, Generator Loss: 26.363\n",
      "Epoch 3271 - Discriminator Loss: 0.002, Generator Loss: 26.863\n",
      "Epoch 3281 - Discriminator Loss: 0.000, Generator Loss: 26.296\n",
      "Epoch 3291 - Discriminator Loss: 0.000, Generator Loss: 26.453\n",
      "Epoch 3301 - Discriminator Loss: 0.000, Generator Loss: 26.255\n",
      "Epoch 3311 - Discriminator Loss: 0.000, Generator Loss: 26.715\n",
      "Epoch 3321 - Discriminator Loss: 0.000, Generator Loss: 26.477\n",
      "Epoch 3331 - Discriminator Loss: 0.001, Generator Loss: 26.690\n",
      "Epoch 3341 - Discriminator Loss: 0.011, Generator Loss: 26.410\n",
      "Epoch 3351 - Discriminator Loss: 0.000, Generator Loss: 26.602\n",
      "Epoch 3361 - Discriminator Loss: 0.001, Generator Loss: 26.537\n",
      "Epoch 3371 - Discriminator Loss: 0.001, Generator Loss: 26.541\n",
      "Epoch 3381 - Discriminator Loss: 0.001, Generator Loss: 26.812\n",
      "Epoch 3391 - Discriminator Loss: 0.001, Generator Loss: 26.754\n",
      "Epoch 3401 - Discriminator Loss: 0.008, Generator Loss: 26.781\n",
      "Epoch 3411 - Discriminator Loss: 0.001, Generator Loss: 26.752\n",
      "Epoch 3421 - Discriminator Loss: 0.007, Generator Loss: 26.668\n",
      "Epoch 3431 - Discriminator Loss: 0.000, Generator Loss: 26.829\n",
      "Epoch 3441 - Discriminator Loss: 0.005, Generator Loss: 26.621\n",
      "Epoch 3451 - Discriminator Loss: 0.005, Generator Loss: 26.594\n",
      "Epoch 3461 - Discriminator Loss: 0.000, Generator Loss: 26.573\n",
      "Epoch 3471 - Discriminator Loss: 0.005, Generator Loss: 26.861\n",
      "Epoch 3481 - Discriminator Loss: 0.003, Generator Loss: 26.525\n",
      "Epoch 3491 - Discriminator Loss: 0.000, Generator Loss: 26.588\n",
      "Epoch 3501 - Discriminator Loss: 0.000, Generator Loss: 26.394\n",
      "Epoch 3511 - Discriminator Loss: 0.000, Generator Loss: 26.526\n",
      "Epoch 3521 - Discriminator Loss: 0.000, Generator Loss: 26.517\n",
      "Epoch 3531 - Discriminator Loss: 0.000, Generator Loss: 26.520\n",
      "Epoch 3541 - Discriminator Loss: 0.006, Generator Loss: 26.702\n",
      "Epoch 3551 - Discriminator Loss: 0.000, Generator Loss: 26.632\n",
      "Epoch 3561 - Discriminator Loss: 0.000, Generator Loss: 26.652\n",
      "Epoch 3571 - Discriminator Loss: 0.003, Generator Loss: 26.704\n",
      "Epoch 3581 - Discriminator Loss: 0.011, Generator Loss: 26.872\n",
      "Epoch 3591 - Discriminator Loss: 0.018, Generator Loss: 26.597\n",
      "Epoch 3601 - Discriminator Loss: 0.000, Generator Loss: 26.778\n",
      "Epoch 3611 - Discriminator Loss: 0.000, Generator Loss: 26.725\n",
      "Epoch 3621 - Discriminator Loss: 0.022, Generator Loss: 26.653\n",
      "Epoch 3631 - Discriminator Loss: 0.000, Generator Loss: 26.513\n",
      "Epoch 3641 - Discriminator Loss: 0.000, Generator Loss: 26.511\n",
      "Epoch 3651 - Discriminator Loss: 0.000, Generator Loss: 26.448\n",
      "Epoch 3661 - Discriminator Loss: 0.001, Generator Loss: 26.660\n",
      "Epoch 3671 - Discriminator Loss: 0.003, Generator Loss: 26.934\n",
      "Epoch 3681 - Discriminator Loss: 0.000, Generator Loss: 26.459\n",
      "Epoch 3691 - Discriminator Loss: 0.000, Generator Loss: 26.317\n",
      "Epoch 3701 - Discriminator Loss: 0.001, Generator Loss: 26.824\n",
      "Epoch 3711 - Discriminator Loss: 0.003, Generator Loss: 26.882\n",
      "Epoch 3721 - Discriminator Loss: 0.001, Generator Loss: 26.917\n",
      "Epoch 3731 - Discriminator Loss: 0.001, Generator Loss: 26.675\n",
      "Epoch 3741 - Discriminator Loss: 0.000, Generator Loss: 26.563\n",
      "Epoch 3751 - Discriminator Loss: 0.024, Generator Loss: 26.788\n",
      "Epoch 3761 - Discriminator Loss: 0.000, Generator Loss: 26.725\n",
      "Epoch 3771 - Discriminator Loss: 0.001, Generator Loss: 26.444\n",
      "Epoch 3781 - Discriminator Loss: 0.000, Generator Loss: 25.977\n",
      "Epoch 3791 - Discriminator Loss: 0.000, Generator Loss: 26.639\n",
      "Epoch 3801 - Discriminator Loss: 0.008, Generator Loss: 26.478\n",
      "Epoch 3811 - Discriminator Loss: 0.023, Generator Loss: 26.667\n",
      "Epoch 3821 - Discriminator Loss: 0.002, Generator Loss: 26.826\n",
      "Epoch 3831 - Discriminator Loss: 0.001, Generator Loss: 26.656\n",
      "Epoch 3841 - Discriminator Loss: 0.000, Generator Loss: 26.654\n",
      "Epoch 3851 - Discriminator Loss: 0.003, Generator Loss: 26.534\n",
      "Epoch 3861 - Discriminator Loss: 0.000, Generator Loss: 26.688\n",
      "Epoch 3871 - Discriminator Loss: 0.000, Generator Loss: 26.227\n",
      "Epoch 3881 - Discriminator Loss: 0.000, Generator Loss: 26.458\n",
      "Epoch 3891 - Discriminator Loss: 0.003, Generator Loss: 26.630\n",
      "Epoch 3901 - Discriminator Loss: 0.012, Generator Loss: 26.808\n",
      "Epoch 3911 - Discriminator Loss: 0.008, Generator Loss: 26.503\n",
      "Epoch 3921 - Discriminator Loss: 0.001, Generator Loss: 26.280\n",
      "Epoch 3931 - Discriminator Loss: 0.002, Generator Loss: 26.487\n",
      "Epoch 3941 - Discriminator Loss: 0.017, Generator Loss: 26.552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3951 - Discriminator Loss: 0.000, Generator Loss: 26.752\n",
      "Epoch 3961 - Discriminator Loss: 0.000, Generator Loss: 26.317\n",
      "Epoch 3971 - Discriminator Loss: 0.004, Generator Loss: 26.918\n",
      "Epoch 3981 - Discriminator Loss: 0.006, Generator Loss: 26.608\n",
      "Epoch 3991 - Discriminator Loss: 0.009, Generator Loss: 26.625\n",
      "Epoch 4001 - Discriminator Loss: 0.000, Generator Loss: 26.369\n",
      "Epoch 4011 - Discriminator Loss: 0.000, Generator Loss: 26.526\n",
      "Epoch 4021 - Discriminator Loss: 0.000, Generator Loss: 26.720\n",
      "Epoch 4031 - Discriminator Loss: 0.002, Generator Loss: 26.250\n",
      "Epoch 4041 - Discriminator Loss: 0.000, Generator Loss: 26.454\n",
      "Epoch 4051 - Discriminator Loss: 0.000, Generator Loss: 26.543\n",
      "Epoch 4061 - Discriminator Loss: 0.000, Generator Loss: 26.626\n",
      "Epoch 4071 - Discriminator Loss: 0.001, Generator Loss: 26.418\n",
      "Epoch 4081 - Discriminator Loss: 0.010, Generator Loss: 26.818\n",
      "Epoch 4091 - Discriminator Loss: 0.006, Generator Loss: 26.592\n",
      "Epoch 4101 - Discriminator Loss: 0.021, Generator Loss: 26.879\n",
      "Epoch 4111 - Discriminator Loss: 0.019, Generator Loss: 26.921\n",
      "Epoch 4121 - Discriminator Loss: 0.002, Generator Loss: 26.552\n",
      "Epoch 4131 - Discriminator Loss: 0.000, Generator Loss: 26.837\n",
      "Epoch 4141 - Discriminator Loss: 0.013, Generator Loss: 26.420\n",
      "Epoch 4151 - Discriminator Loss: 0.001, Generator Loss: 26.548\n",
      "Epoch 4161 - Discriminator Loss: 0.000, Generator Loss: 26.655\n",
      "Epoch 4171 - Discriminator Loss: 0.003, Generator Loss: 26.972\n",
      "Epoch 4181 - Discriminator Loss: 0.003, Generator Loss: 26.364\n",
      "Epoch 4191 - Discriminator Loss: 0.002, Generator Loss: 26.742\n",
      "Epoch 4201 - Discriminator Loss: 0.000, Generator Loss: 26.466\n",
      "Epoch 4211 - Discriminator Loss: 0.002, Generator Loss: 26.636\n",
      "Epoch 4221 - Discriminator Loss: 0.001, Generator Loss: 26.850\n",
      "Epoch 4231 - Discriminator Loss: 0.000, Generator Loss: 26.739\n",
      "Epoch 4241 - Discriminator Loss: 0.000, Generator Loss: 26.603\n",
      "Epoch 4251 - Discriminator Loss: 0.000, Generator Loss: 26.206\n",
      "Epoch 4261 - Discriminator Loss: 0.002, Generator Loss: 26.698\n",
      "Epoch 4271 - Discriminator Loss: 0.004, Generator Loss: 26.225\n",
      "Epoch 4281 - Discriminator Loss: 0.000, Generator Loss: 26.999\n",
      "Epoch 4291 - Discriminator Loss: 0.000, Generator Loss: 26.589\n",
      "Epoch 4301 - Discriminator Loss: 0.017, Generator Loss: 26.580\n",
      "Epoch 4311 - Discriminator Loss: 0.000, Generator Loss: 26.548\n",
      "Epoch 4321 - Discriminator Loss: 0.012, Generator Loss: 26.703\n",
      "Epoch 4331 - Discriminator Loss: 0.004, Generator Loss: 26.747\n",
      "Epoch 4341 - Discriminator Loss: 0.001, Generator Loss: 26.842\n",
      "Epoch 4351 - Discriminator Loss: 0.000, Generator Loss: 26.681\n",
      "Epoch 4361 - Discriminator Loss: 0.001, Generator Loss: 26.622\n",
      "Epoch 4371 - Discriminator Loss: 0.001, Generator Loss: 26.698\n",
      "Epoch 4381 - Discriminator Loss: 0.000, Generator Loss: 26.946\n",
      "Epoch 4391 - Discriminator Loss: 0.000, Generator Loss: 26.511\n",
      "Epoch 4401 - Discriminator Loss: 0.008, Generator Loss: 26.747\n",
      "Epoch 4411 - Discriminator Loss: 0.002, Generator Loss: 26.765\n",
      "Epoch 4421 - Discriminator Loss: 0.000, Generator Loss: 26.398\n",
      "Epoch 4431 - Discriminator Loss: 0.027, Generator Loss: 26.786\n",
      "Epoch 4441 - Discriminator Loss: 0.000, Generator Loss: 26.710\n",
      "Epoch 4451 - Discriminator Loss: 0.019, Generator Loss: 26.353\n",
      "Epoch 4461 - Discriminator Loss: 0.001, Generator Loss: 26.958\n",
      "Epoch 4471 - Discriminator Loss: 0.014, Generator Loss: 26.921\n",
      "Epoch 4481 - Discriminator Loss: 0.002, Generator Loss: 26.807\n",
      "Epoch 4491 - Discriminator Loss: 0.000, Generator Loss: 27.013\n",
      "Epoch 4501 - Discriminator Loss: 0.001, Generator Loss: 26.855\n",
      "Epoch 4511 - Discriminator Loss: 0.001, Generator Loss: 26.868\n",
      "Epoch 4521 - Discriminator Loss: 0.000, Generator Loss: 26.777\n",
      "Epoch 4531 - Discriminator Loss: 0.000, Generator Loss: 26.712\n",
      "Epoch 4541 - Discriminator Loss: 0.004, Generator Loss: 27.102\n",
      "Epoch 4551 - Discriminator Loss: 0.000, Generator Loss: 26.750\n",
      "Epoch 4561 - Discriminator Loss: 0.001, Generator Loss: 26.636\n",
      "Epoch 4571 - Discriminator Loss: 0.000, Generator Loss: 26.784\n",
      "Epoch 4581 - Discriminator Loss: 0.000, Generator Loss: 27.068\n",
      "Epoch 4591 - Discriminator Loss: 0.029, Generator Loss: 27.119\n",
      "Epoch 4601 - Discriminator Loss: 0.000, Generator Loss: 26.788\n",
      "Epoch 4611 - Discriminator Loss: 0.000, Generator Loss: 26.784\n",
      "Epoch 4621 - Discriminator Loss: 0.001, Generator Loss: 26.824\n",
      "Epoch 4631 - Discriminator Loss: 0.000, Generator Loss: 26.674\n",
      "Epoch 4641 - Discriminator Loss: 0.009, Generator Loss: 26.969\n",
      "Epoch 4651 - Discriminator Loss: 0.001, Generator Loss: 26.722\n",
      "Epoch 4661 - Discriminator Loss: 0.002, Generator Loss: 27.053\n",
      "Epoch 4671 - Discriminator Loss: 0.000, Generator Loss: 26.913\n",
      "Epoch 4681 - Discriminator Loss: 0.000, Generator Loss: 27.050\n",
      "Epoch 4691 - Discriminator Loss: 0.001, Generator Loss: 26.857\n",
      "Epoch 4701 - Discriminator Loss: 0.000, Generator Loss: 26.680\n",
      "Epoch 4711 - Discriminator Loss: 0.001, Generator Loss: 26.816\n",
      "Epoch 4721 - Discriminator Loss: 0.000, Generator Loss: 26.705\n",
      "Epoch 4731 - Discriminator Loss: 0.000, Generator Loss: 26.703\n",
      "Epoch 4741 - Discriminator Loss: 0.000, Generator Loss: 26.461\n",
      "Epoch 4751 - Discriminator Loss: 0.015, Generator Loss: 26.738\n",
      "Epoch 4761 - Discriminator Loss: 0.007, Generator Loss: 26.862\n",
      "Epoch 4771 - Discriminator Loss: 0.008, Generator Loss: 26.844\n",
      "Epoch 4781 - Discriminator Loss: 0.017, Generator Loss: 27.058\n",
      "Epoch 4791 - Discriminator Loss: 0.025, Generator Loss: 26.678\n",
      "Epoch 4801 - Discriminator Loss: 0.024, Generator Loss: 26.846\n",
      "Epoch 4811 - Discriminator Loss: 0.001, Generator Loss: 26.931\n",
      "Epoch 4821 - Discriminator Loss: 0.001, Generator Loss: 26.869\n",
      "Epoch 4831 - Discriminator Loss: 0.002, Generator Loss: 26.770\n",
      "Epoch 4841 - Discriminator Loss: 0.003, Generator Loss: 26.771\n",
      "Epoch 4851 - Discriminator Loss: 0.001, Generator Loss: 26.653\n",
      "Epoch 4861 - Discriminator Loss: 0.000, Generator Loss: 26.840\n",
      "Epoch 4871 - Discriminator Loss: 0.024, Generator Loss: 26.713\n",
      "Epoch 4881 - Discriminator Loss: 0.001, Generator Loss: 26.848\n",
      "Epoch 4891 - Discriminator Loss: 0.000, Generator Loss: 26.789\n",
      "Epoch 4901 - Discriminator Loss: 0.002, Generator Loss: 26.852\n",
      "Epoch 4911 - Discriminator Loss: 0.000, Generator Loss: 26.762\n",
      "Epoch 4921 - Discriminator Loss: 0.000, Generator Loss: 26.669\n",
      "Epoch 4931 - Discriminator Loss: 0.000, Generator Loss: 26.732\n",
      "Epoch 4941 - Discriminator Loss: 0.001, Generator Loss: 26.885\n",
      "Epoch 4951 - Discriminator Loss: 0.000, Generator Loss: 26.842\n",
      "Epoch 4961 - Discriminator Loss: 0.001, Generator Loss: 26.811\n",
      "Epoch 4971 - Discriminator Loss: 0.002, Generator Loss: 26.516\n",
      "Epoch 4981 - Discriminator Loss: 0.001, Generator Loss: 26.689\n",
      "Epoch 4991 - Discriminator Loss: 0.000, Generator Loss: 26.692\n",
      "Epoch 5001 - Discriminator Loss: 0.000, Generator Loss: 26.797\n",
      "Epoch 5011 - Discriminator Loss: 0.001, Generator Loss: 26.632\n",
      "Epoch 5021 - Discriminator Loss: 0.000, Generator Loss: 26.492\n",
      "Epoch 5031 - Discriminator Loss: 0.000, Generator Loss: 26.640\n",
      "Epoch 5041 - Discriminator Loss: 0.000, Generator Loss: 26.969\n",
      "Epoch 5051 - Discriminator Loss: 0.000, Generator Loss: 26.548\n",
      "Epoch 5061 - Discriminator Loss: 0.001, Generator Loss: 26.635\n",
      "Epoch 5071 - Discriminator Loss: 0.000, Generator Loss: 26.703\n",
      "Epoch 5081 - Discriminator Loss: 0.000, Generator Loss: 26.859\n",
      "Epoch 5091 - Discriminator Loss: 0.001, Generator Loss: 26.773\n",
      "Epoch 5101 - Discriminator Loss: 0.001, Generator Loss: 26.796\n",
      "Epoch 5111 - Discriminator Loss: 0.007, Generator Loss: 26.809\n",
      "Epoch 5121 - Discriminator Loss: 0.000, Generator Loss: 26.869\n",
      "Epoch 5131 - Discriminator Loss: 0.000, Generator Loss: 26.898\n",
      "Epoch 5141 - Discriminator Loss: 0.001, Generator Loss: 26.610\n",
      "Epoch 5151 - Discriminator Loss: 0.000, Generator Loss: 26.878\n",
      "Epoch 5161 - Discriminator Loss: 0.000, Generator Loss: 26.866\n",
      "Epoch 5171 - Discriminator Loss: 0.001, Generator Loss: 26.801\n",
      "Epoch 5181 - Discriminator Loss: 0.001, Generator Loss: 26.935\n",
      "Epoch 5191 - Discriminator Loss: 0.004, Generator Loss: 27.014\n",
      "Epoch 5201 - Discriminator Loss: 0.001, Generator Loss: 26.853\n",
      "Epoch 5211 - Discriminator Loss: 0.001, Generator Loss: 27.138\n",
      "Epoch 5221 - Discriminator Loss: 0.001, Generator Loss: 27.088\n",
      "Epoch 5231 - Discriminator Loss: 0.001, Generator Loss: 27.119\n",
      "Epoch 5241 - Discriminator Loss: 0.001, Generator Loss: 26.969\n",
      "Epoch 5251 - Discriminator Loss: 0.001, Generator Loss: 26.857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5261 - Discriminator Loss: 0.001, Generator Loss: 27.193\n",
      "Epoch 5271 - Discriminator Loss: 0.000, Generator Loss: 26.949\n",
      "Epoch 5281 - Discriminator Loss: 0.007, Generator Loss: 26.908\n",
      "Epoch 5291 - Discriminator Loss: 0.000, Generator Loss: 26.596\n",
      "Epoch 5301 - Discriminator Loss: 0.000, Generator Loss: 26.906\n",
      "Epoch 5311 - Discriminator Loss: 0.000, Generator Loss: 26.716\n",
      "Epoch 5321 - Discriminator Loss: 0.005, Generator Loss: 26.948\n",
      "Epoch 5331 - Discriminator Loss: 0.000, Generator Loss: 26.828\n",
      "Epoch 5341 - Discriminator Loss: 0.007, Generator Loss: 26.714\n",
      "Epoch 5351 - Discriminator Loss: 0.007, Generator Loss: 26.754\n",
      "Epoch 5361 - Discriminator Loss: 0.002, Generator Loss: 26.840\n",
      "Epoch 5371 - Discriminator Loss: 0.000, Generator Loss: 26.630\n",
      "Epoch 5381 - Discriminator Loss: 0.000, Generator Loss: 26.979\n",
      "Epoch 5391 - Discriminator Loss: 0.002, Generator Loss: 26.780\n",
      "Epoch 5401 - Discriminator Loss: 0.001, Generator Loss: 26.958\n",
      "Epoch 5411 - Discriminator Loss: 0.001, Generator Loss: 26.426\n",
      "Epoch 5421 - Discriminator Loss: 0.000, Generator Loss: 26.446\n",
      "Epoch 5431 - Discriminator Loss: 0.002, Generator Loss: 26.811\n",
      "Epoch 5441 - Discriminator Loss: 0.002, Generator Loss: 26.587\n",
      "Epoch 5451 - Discriminator Loss: 0.000, Generator Loss: 26.887\n",
      "Epoch 5461 - Discriminator Loss: 0.002, Generator Loss: 27.043\n",
      "Epoch 5471 - Discriminator Loss: 0.000, Generator Loss: 26.713\n",
      "Epoch 5481 - Discriminator Loss: 0.000, Generator Loss: 26.924\n",
      "Epoch 5491 - Discriminator Loss: 0.000, Generator Loss: 27.006\n",
      "Epoch 5501 - Discriminator Loss: 0.001, Generator Loss: 26.773\n",
      "Epoch 5511 - Discriminator Loss: 0.025, Generator Loss: 26.738\n",
      "Epoch 5521 - Discriminator Loss: 0.001, Generator Loss: 26.740\n",
      "Epoch 5531 - Discriminator Loss: 0.003, Generator Loss: 26.897\n",
      "Epoch 5541 - Discriminator Loss: 0.018, Generator Loss: 26.632\n",
      "Epoch 5551 - Discriminator Loss: 0.002, Generator Loss: 26.847\n",
      "Epoch 5561 - Discriminator Loss: 0.008, Generator Loss: 26.747\n",
      "Epoch 5571 - Discriminator Loss: 0.001, Generator Loss: 27.011\n",
      "Epoch 5581 - Discriminator Loss: 0.001, Generator Loss: 26.907\n",
      "Epoch 5591 - Discriminator Loss: 0.000, Generator Loss: 26.864\n",
      "Epoch 5601 - Discriminator Loss: 0.000, Generator Loss: 26.962\n",
      "Epoch 5611 - Discriminator Loss: 0.000, Generator Loss: 26.773\n",
      "Epoch 5621 - Discriminator Loss: 0.001, Generator Loss: 26.880\n",
      "Epoch 5631 - Discriminator Loss: 0.001, Generator Loss: 26.955\n",
      "Epoch 5641 - Discriminator Loss: 0.000, Generator Loss: 26.801\n",
      "Epoch 5651 - Discriminator Loss: 0.000, Generator Loss: 26.529\n",
      "Epoch 5661 - Discriminator Loss: 0.001, Generator Loss: 27.034\n",
      "Epoch 5671 - Discriminator Loss: 0.000, Generator Loss: 26.728\n",
      "Epoch 5681 - Discriminator Loss: 0.003, Generator Loss: 26.720\n",
      "Epoch 5691 - Discriminator Loss: 0.000, Generator Loss: 26.616\n",
      "Epoch 5701 - Discriminator Loss: 0.001, Generator Loss: 26.577\n",
      "Epoch 5711 - Discriminator Loss: 0.002, Generator Loss: 27.082\n",
      "Epoch 5721 - Discriminator Loss: 0.000, Generator Loss: 26.695\n",
      "Epoch 5731 - Discriminator Loss: 0.000, Generator Loss: 26.597\n",
      "Epoch 5741 - Discriminator Loss: 0.008, Generator Loss: 26.751\n",
      "Epoch 5751 - Discriminator Loss: 0.001, Generator Loss: 26.676\n",
      "Epoch 5761 - Discriminator Loss: 0.001, Generator Loss: 26.904\n",
      "Epoch 5771 - Discriminator Loss: 0.008, Generator Loss: 26.912\n",
      "Epoch 5781 - Discriminator Loss: 0.000, Generator Loss: 26.903\n",
      "Epoch 5791 - Discriminator Loss: 0.001, Generator Loss: 27.098\n",
      "Epoch 5801 - Discriminator Loss: 0.002, Generator Loss: 27.040\n",
      "Epoch 5811 - Discriminator Loss: 0.000, Generator Loss: 26.764\n",
      "Epoch 5821 - Discriminator Loss: 0.008, Generator Loss: 26.740\n",
      "Epoch 5831 - Discriminator Loss: 0.000, Generator Loss: 26.700\n",
      "Epoch 5841 - Discriminator Loss: 0.001, Generator Loss: 27.067\n",
      "Epoch 5851 - Discriminator Loss: 0.000, Generator Loss: 26.630\n",
      "Epoch 5861 - Discriminator Loss: 0.005, Generator Loss: 26.992\n",
      "Epoch 5871 - Discriminator Loss: 0.001, Generator Loss: 26.922\n",
      "Epoch 5881 - Discriminator Loss: 0.000, Generator Loss: 26.740\n",
      "Epoch 5891 - Discriminator Loss: 0.000, Generator Loss: 26.823\n",
      "Epoch 5901 - Discriminator Loss: 0.000, Generator Loss: 26.969\n",
      "Epoch 5911 - Discriminator Loss: 0.008, Generator Loss: 26.847\n",
      "Epoch 5921 - Discriminator Loss: 0.002, Generator Loss: 27.090\n",
      "Epoch 5931 - Discriminator Loss: 0.000, Generator Loss: 27.119\n",
      "Epoch 5941 - Discriminator Loss: 0.002, Generator Loss: 26.956\n",
      "Epoch 5951 - Discriminator Loss: 0.001, Generator Loss: 26.741\n",
      "Epoch 5961 - Discriminator Loss: 0.000, Generator Loss: 26.807\n",
      "Epoch 5971 - Discriminator Loss: 0.001, Generator Loss: 26.695\n",
      "Epoch 5981 - Discriminator Loss: 0.000, Generator Loss: 26.958\n",
      "Epoch 5991 - Discriminator Loss: 0.005, Generator Loss: 26.872\n",
      "Epoch 6001 - Discriminator Loss: 0.000, Generator Loss: 26.890\n",
      "Epoch 6011 - Discriminator Loss: 0.001, Generator Loss: 27.096\n",
      "Epoch 6021 - Discriminator Loss: 0.000, Generator Loss: 26.823\n",
      "Epoch 6031 - Discriminator Loss: 0.000, Generator Loss: 26.487\n",
      "Epoch 6041 - Discriminator Loss: 0.000, Generator Loss: 26.707\n",
      "Epoch 6051 - Discriminator Loss: 0.017, Generator Loss: 26.946\n",
      "Epoch 6061 - Discriminator Loss: 0.001, Generator Loss: 27.048\n",
      "Epoch 6071 - Discriminator Loss: 0.000, Generator Loss: 26.670\n",
      "Epoch 6081 - Discriminator Loss: 0.007, Generator Loss: 26.623\n",
      "Epoch 6091 - Discriminator Loss: 0.000, Generator Loss: 26.898\n",
      "Epoch 6101 - Discriminator Loss: 0.001, Generator Loss: 26.784\n",
      "Epoch 6111 - Discriminator Loss: 0.001, Generator Loss: 26.702\n",
      "Epoch 6121 - Discriminator Loss: 0.001, Generator Loss: 26.984\n",
      "Epoch 6131 - Discriminator Loss: 0.001, Generator Loss: 26.841\n",
      "Epoch 6141 - Discriminator Loss: 0.001, Generator Loss: 26.856\n",
      "Epoch 6151 - Discriminator Loss: 0.004, Generator Loss: 26.728\n",
      "Epoch 6161 - Discriminator Loss: 0.006, Generator Loss: 27.028\n",
      "Epoch 6171 - Discriminator Loss: 0.000, Generator Loss: 26.768\n",
      "Epoch 6181 - Discriminator Loss: 0.002, Generator Loss: 26.993\n",
      "Epoch 6191 - Discriminator Loss: 0.000, Generator Loss: 26.788\n",
      "Epoch 6201 - Discriminator Loss: 0.001, Generator Loss: 26.749\n",
      "Epoch 6211 - Discriminator Loss: 0.010, Generator Loss: 26.822\n",
      "Epoch 6221 - Discriminator Loss: 0.002, Generator Loss: 26.720\n",
      "Epoch 6231 - Discriminator Loss: 0.004, Generator Loss: 26.921\n",
      "Epoch 6241 - Discriminator Loss: 0.000, Generator Loss: 27.139\n",
      "Epoch 6251 - Discriminator Loss: 0.001, Generator Loss: 27.161\n",
      "Epoch 6261 - Discriminator Loss: 0.008, Generator Loss: 26.839\n",
      "Epoch 6271 - Discriminator Loss: 0.005, Generator Loss: 27.187\n",
      "Epoch 6281 - Discriminator Loss: 0.002, Generator Loss: 26.776\n",
      "Epoch 6291 - Discriminator Loss: 0.000, Generator Loss: 26.740\n",
      "Epoch 6301 - Discriminator Loss: 0.001, Generator Loss: 26.929\n",
      "Epoch 6311 - Discriminator Loss: 0.006, Generator Loss: 26.693\n",
      "Epoch 6321 - Discriminator Loss: 0.000, Generator Loss: 26.855\n",
      "Epoch 6331 - Discriminator Loss: 0.001, Generator Loss: 26.936\n",
      "Epoch 6341 - Discriminator Loss: 0.000, Generator Loss: 26.852\n",
      "Epoch 6351 - Discriminator Loss: 0.000, Generator Loss: 26.914\n",
      "Epoch 6361 - Discriminator Loss: 0.002, Generator Loss: 27.046\n",
      "Epoch 6371 - Discriminator Loss: 0.000, Generator Loss: 26.941\n",
      "Epoch 6381 - Discriminator Loss: 0.002, Generator Loss: 26.946\n",
      "Epoch 6391 - Discriminator Loss: 0.000, Generator Loss: 27.038\n",
      "Epoch 6401 - Discriminator Loss: 0.000, Generator Loss: 27.023\n",
      "Epoch 6411 - Discriminator Loss: 0.012, Generator Loss: 26.845\n",
      "Epoch 6421 - Discriminator Loss: 0.000, Generator Loss: 27.184\n",
      "Epoch 6431 - Discriminator Loss: 0.000, Generator Loss: 26.875\n",
      "Epoch 6441 - Discriminator Loss: 0.006, Generator Loss: 26.844\n",
      "Epoch 6451 - Discriminator Loss: 0.000, Generator Loss: 27.043\n",
      "Epoch 6461 - Discriminator Loss: 0.001, Generator Loss: 27.018\n",
      "Epoch 6471 - Discriminator Loss: 0.022, Generator Loss: 26.960\n",
      "Epoch 6481 - Discriminator Loss: 0.001, Generator Loss: 26.988\n",
      "Epoch 6491 - Discriminator Loss: 0.001, Generator Loss: 27.025\n",
      "Epoch 6501 - Discriminator Loss: 0.004, Generator Loss: 27.041\n",
      "Epoch 6511 - Discriminator Loss: 0.018, Generator Loss: 27.134\n",
      "Epoch 6521 - Discriminator Loss: 0.001, Generator Loss: 27.144\n",
      "Epoch 6531 - Discriminator Loss: 0.000, Generator Loss: 27.050\n",
      "Epoch 6541 - Discriminator Loss: 0.000, Generator Loss: 26.739\n",
      "Epoch 6551 - Discriminator Loss: 0.000, Generator Loss: 27.094\n",
      "Epoch 6561 - Discriminator Loss: 0.010, Generator Loss: 27.047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6571 - Discriminator Loss: 0.000, Generator Loss: 27.006\n",
      "Epoch 6581 - Discriminator Loss: 0.001, Generator Loss: 27.133\n",
      "Epoch 6591 - Discriminator Loss: 0.015, Generator Loss: 27.084\n",
      "Epoch 6601 - Discriminator Loss: 0.000, Generator Loss: 26.971\n",
      "Epoch 6611 - Discriminator Loss: 0.004, Generator Loss: 27.074\n",
      "Epoch 6621 - Discriminator Loss: 0.000, Generator Loss: 26.885\n",
      "Epoch 6631 - Discriminator Loss: 0.000, Generator Loss: 27.122\n",
      "Epoch 6641 - Discriminator Loss: 0.000, Generator Loss: 26.937\n",
      "Epoch 6651 - Discriminator Loss: 0.000, Generator Loss: 26.758\n",
      "Epoch 6661 - Discriminator Loss: 0.000, Generator Loss: 26.921\n",
      "Epoch 6671 - Discriminator Loss: 0.000, Generator Loss: 26.732\n",
      "Epoch 6681 - Discriminator Loss: 0.004, Generator Loss: 26.902\n",
      "Epoch 6691 - Discriminator Loss: 0.028, Generator Loss: 26.784\n",
      "Epoch 6701 - Discriminator Loss: 0.000, Generator Loss: 26.718\n",
      "Epoch 6711 - Discriminator Loss: 0.000, Generator Loss: 26.858\n",
      "Epoch 6721 - Discriminator Loss: 0.006, Generator Loss: 26.949\n",
      "Epoch 6731 - Discriminator Loss: 0.000, Generator Loss: 26.971\n",
      "Epoch 6741 - Discriminator Loss: 0.000, Generator Loss: 26.879\n",
      "Epoch 6751 - Discriminator Loss: 0.000, Generator Loss: 26.878\n",
      "Epoch 6761 - Discriminator Loss: 0.001, Generator Loss: 27.016\n",
      "Epoch 6771 - Discriminator Loss: 0.000, Generator Loss: 26.994\n",
      "Epoch 6781 - Discriminator Loss: 0.001, Generator Loss: 26.975\n",
      "Epoch 6791 - Discriminator Loss: 0.001, Generator Loss: 26.787\n",
      "Epoch 6801 - Discriminator Loss: 0.001, Generator Loss: 26.879\n",
      "Epoch 6811 - Discriminator Loss: 0.000, Generator Loss: 26.947\n",
      "Epoch 6821 - Discriminator Loss: 0.001, Generator Loss: 26.859\n",
      "Epoch 6831 - Discriminator Loss: 0.000, Generator Loss: 27.047\n",
      "Epoch 6841 - Discriminator Loss: 0.006, Generator Loss: 26.722\n",
      "Epoch 6851 - Discriminator Loss: 0.000, Generator Loss: 26.910\n",
      "Epoch 6861 - Discriminator Loss: 0.003, Generator Loss: 26.969\n",
      "Epoch 6871 - Discriminator Loss: 0.001, Generator Loss: 27.115\n",
      "Epoch 6881 - Discriminator Loss: 0.000, Generator Loss: 27.012\n",
      "Epoch 6891 - Discriminator Loss: 0.022, Generator Loss: 26.879\n",
      "Epoch 6901 - Discriminator Loss: 0.000, Generator Loss: 26.761\n",
      "Epoch 6911 - Discriminator Loss: 0.000, Generator Loss: 26.932\n",
      "Epoch 6921 - Discriminator Loss: 0.001, Generator Loss: 26.956\n",
      "Epoch 6931 - Discriminator Loss: 0.000, Generator Loss: 27.013\n",
      "Epoch 6941 - Discriminator Loss: 0.000, Generator Loss: 26.817\n",
      "Epoch 6951 - Discriminator Loss: 0.001, Generator Loss: 27.050\n",
      "Epoch 6961 - Discriminator Loss: 0.013, Generator Loss: 27.025\n",
      "Epoch 6971 - Discriminator Loss: 0.000, Generator Loss: 27.157\n",
      "Epoch 6981 - Discriminator Loss: 0.000, Generator Loss: 26.913\n",
      "Epoch 6991 - Discriminator Loss: 0.001, Generator Loss: 26.968\n",
      "Epoch 7001 - Discriminator Loss: 0.000, Generator Loss: 26.836\n",
      "Epoch 7011 - Discriminator Loss: 0.000, Generator Loss: 26.754\n",
      "Epoch 7021 - Discriminator Loss: 0.000, Generator Loss: 26.756\n",
      "Epoch 7031 - Discriminator Loss: 0.000, Generator Loss: 26.974\n",
      "Epoch 7041 - Discriminator Loss: 0.000, Generator Loss: 26.755\n",
      "Epoch 7051 - Discriminator Loss: 0.000, Generator Loss: 26.645\n",
      "Epoch 7061 - Discriminator Loss: 0.003, Generator Loss: 26.847\n",
      "Epoch 7071 - Discriminator Loss: 0.000, Generator Loss: 26.887\n",
      "Epoch 7081 - Discriminator Loss: 0.020, Generator Loss: 27.018\n",
      "Epoch 7091 - Discriminator Loss: 0.004, Generator Loss: 26.891\n",
      "Epoch 7101 - Discriminator Loss: 0.000, Generator Loss: 26.899\n",
      "Epoch 7111 - Discriminator Loss: 0.001, Generator Loss: 26.788\n",
      "Epoch 7121 - Discriminator Loss: 0.000, Generator Loss: 26.596\n",
      "Epoch 7131 - Discriminator Loss: 0.002, Generator Loss: 26.973\n",
      "Epoch 7141 - Discriminator Loss: 0.001, Generator Loss: 26.602\n",
      "Epoch 7151 - Discriminator Loss: 0.000, Generator Loss: 26.633\n",
      "Epoch 7161 - Discriminator Loss: 0.000, Generator Loss: 26.877\n",
      "Epoch 7171 - Discriminator Loss: 0.000, Generator Loss: 26.714\n",
      "Epoch 7181 - Discriminator Loss: 0.000, Generator Loss: 27.094\n",
      "Epoch 7191 - Discriminator Loss: 0.011, Generator Loss: 26.549\n",
      "Epoch 7201 - Discriminator Loss: 0.000, Generator Loss: 26.692\n",
      "Epoch 7211 - Discriminator Loss: 0.000, Generator Loss: 26.959\n",
      "Epoch 7221 - Discriminator Loss: 0.000, Generator Loss: 26.897\n",
      "Epoch 7231 - Discriminator Loss: 0.005, Generator Loss: 27.218\n",
      "Epoch 7241 - Discriminator Loss: 0.002, Generator Loss: 27.158\n",
      "Epoch 7251 - Discriminator Loss: 0.003, Generator Loss: 27.138\n",
      "Epoch 7261 - Discriminator Loss: 0.000, Generator Loss: 27.008\n",
      "Epoch 7271 - Discriminator Loss: 0.012, Generator Loss: 27.013\n",
      "Epoch 7281 - Discriminator Loss: 0.000, Generator Loss: 26.730\n",
      "Epoch 7291 - Discriminator Loss: 0.000, Generator Loss: 26.938\n",
      "Epoch 7301 - Discriminator Loss: 0.000, Generator Loss: 27.104\n",
      "Epoch 7311 - Discriminator Loss: 0.000, Generator Loss: 27.012\n",
      "Epoch 7321 - Discriminator Loss: 0.001, Generator Loss: 27.142\n",
      "Epoch 7331 - Discriminator Loss: 0.000, Generator Loss: 27.100\n",
      "Epoch 7341 - Discriminator Loss: 0.000, Generator Loss: 26.801\n",
      "Epoch 7351 - Discriminator Loss: 0.000, Generator Loss: 27.104\n",
      "Epoch 7361 - Discriminator Loss: 0.014, Generator Loss: 27.171\n",
      "Epoch 7371 - Discriminator Loss: 0.000, Generator Loss: 27.179\n",
      "Epoch 7381 - Discriminator Loss: 0.000, Generator Loss: 27.148\n",
      "Epoch 7391 - Discriminator Loss: 0.001, Generator Loss: 26.880\n",
      "Epoch 7401 - Discriminator Loss: 0.004, Generator Loss: 27.167\n",
      "Epoch 7411 - Discriminator Loss: 0.004, Generator Loss: 27.302\n",
      "Epoch 7421 - Discriminator Loss: 0.001, Generator Loss: 27.099\n",
      "Epoch 7431 - Discriminator Loss: 0.000, Generator Loss: 26.824\n",
      "Epoch 7441 - Discriminator Loss: 0.003, Generator Loss: 26.991\n",
      "Epoch 7451 - Discriminator Loss: 0.000, Generator Loss: 26.905\n",
      "Epoch 7461 - Discriminator Loss: 0.000, Generator Loss: 26.819\n",
      "Epoch 7471 - Discriminator Loss: 0.043, Generator Loss: 27.161\n",
      "Epoch 7481 - Discriminator Loss: 0.000, Generator Loss: 26.712\n",
      "Epoch 7491 - Discriminator Loss: 0.000, Generator Loss: 27.148\n",
      "Epoch 7501 - Discriminator Loss: 0.020, Generator Loss: 27.217\n",
      "Epoch 7511 - Discriminator Loss: 0.008, Generator Loss: 27.056\n",
      "Epoch 7521 - Discriminator Loss: 0.000, Generator Loss: 27.127\n",
      "Epoch 7531 - Discriminator Loss: 0.011, Generator Loss: 26.877\n",
      "Epoch 7541 - Discriminator Loss: 0.000, Generator Loss: 27.010\n",
      "Epoch 7551 - Discriminator Loss: 0.020, Generator Loss: 27.315\n",
      "Epoch 7561 - Discriminator Loss: 0.000, Generator Loss: 26.736\n",
      "Epoch 7571 - Discriminator Loss: 0.000, Generator Loss: 27.180\n",
      "Epoch 7581 - Discriminator Loss: 0.000, Generator Loss: 27.170\n",
      "Epoch 7591 - Discriminator Loss: 0.000, Generator Loss: 27.084\n",
      "Epoch 7601 - Discriminator Loss: 0.000, Generator Loss: 26.718\n",
      "Epoch 7611 - Discriminator Loss: 0.027, Generator Loss: 27.061\n",
      "Epoch 7621 - Discriminator Loss: 0.000, Generator Loss: 27.071\n",
      "Epoch 7631 - Discriminator Loss: 0.000, Generator Loss: 27.053\n",
      "Epoch 7641 - Discriminator Loss: 0.000, Generator Loss: 26.809\n",
      "Epoch 7651 - Discriminator Loss: 0.004, Generator Loss: 27.206\n",
      "Epoch 7661 - Discriminator Loss: 0.000, Generator Loss: 27.019\n",
      "Epoch 7671 - Discriminator Loss: 0.010, Generator Loss: 27.041\n",
      "Epoch 7681 - Discriminator Loss: 0.000, Generator Loss: 26.923\n",
      "Epoch 7691 - Discriminator Loss: 0.000, Generator Loss: 27.085\n",
      "Epoch 7701 - Discriminator Loss: 0.000, Generator Loss: 26.961\n",
      "Epoch 7711 - Discriminator Loss: 0.000, Generator Loss: 26.932\n",
      "Epoch 7721 - Discriminator Loss: 0.000, Generator Loss: 26.982\n",
      "Epoch 7731 - Discriminator Loss: 0.009, Generator Loss: 27.092\n",
      "Epoch 7741 - Discriminator Loss: 0.001, Generator Loss: 27.107\n",
      "Epoch 7751 - Discriminator Loss: 0.004, Generator Loss: 27.094\n",
      "Epoch 7761 - Discriminator Loss: 0.000, Generator Loss: 26.833\n",
      "Epoch 7771 - Discriminator Loss: 0.002, Generator Loss: 27.306\n",
      "Epoch 7781 - Discriminator Loss: 0.000, Generator Loss: 27.258\n",
      "Epoch 7791 - Discriminator Loss: 0.001, Generator Loss: 27.108\n",
      "Epoch 7801 - Discriminator Loss: 0.009, Generator Loss: 27.122\n",
      "Epoch 7811 - Discriminator Loss: 0.002, Generator Loss: 27.261\n",
      "Epoch 7821 - Discriminator Loss: 0.000, Generator Loss: 27.125\n",
      "Epoch 7831 - Discriminator Loss: 0.001, Generator Loss: 27.223\n",
      "Epoch 7841 - Discriminator Loss: 0.011, Generator Loss: 27.314\n",
      "Epoch 7851 - Discriminator Loss: 0.016, Generator Loss: 27.049\n",
      "Epoch 7861 - Discriminator Loss: 0.022, Generator Loss: 27.268\n",
      "Epoch 7871 - Discriminator Loss: 0.001, Generator Loss: 27.471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7881 - Discriminator Loss: 0.000, Generator Loss: 27.164\n",
      "Epoch 7891 - Discriminator Loss: 0.000, Generator Loss: 27.036\n",
      "Epoch 7901 - Discriminator Loss: 0.005, Generator Loss: 27.332\n",
      "Epoch 7911 - Discriminator Loss: 0.001, Generator Loss: 27.360\n",
      "Epoch 7921 - Discriminator Loss: 0.001, Generator Loss: 27.169\n",
      "Epoch 7931 - Discriminator Loss: 0.001, Generator Loss: 27.240\n",
      "Epoch 7941 - Discriminator Loss: 0.000, Generator Loss: 27.288\n",
      "Epoch 7951 - Discriminator Loss: 0.000, Generator Loss: 27.181\n",
      "Epoch 7961 - Discriminator Loss: 0.000, Generator Loss: 27.151\n",
      "Epoch 7971 - Discriminator Loss: 0.001, Generator Loss: 27.355\n",
      "Epoch 7981 - Discriminator Loss: 0.000, Generator Loss: 27.184\n",
      "Epoch 7991 - Discriminator Loss: 0.000, Generator Loss: 27.301\n",
      "Epoch 8001 - Discriminator Loss: 0.000, Generator Loss: 27.158\n",
      "Epoch 8011 - Discriminator Loss: 0.006, Generator Loss: 27.384\n",
      "Epoch 8021 - Discriminator Loss: 0.000, Generator Loss: 27.379\n",
      "Epoch 8031 - Discriminator Loss: 0.003, Generator Loss: 27.307\n",
      "Epoch 8041 - Discriminator Loss: 0.000, Generator Loss: 27.334\n",
      "Epoch 8051 - Discriminator Loss: 0.002, Generator Loss: 27.255\n",
      "Epoch 8061 - Discriminator Loss: 0.015, Generator Loss: 27.272\n",
      "Epoch 8071 - Discriminator Loss: 0.000, Generator Loss: 27.298\n",
      "Epoch 8081 - Discriminator Loss: 0.000, Generator Loss: 27.271\n",
      "Epoch 8091 - Discriminator Loss: 0.015, Generator Loss: 26.965\n",
      "Epoch 8101 - Discriminator Loss: 0.000, Generator Loss: 27.262\n",
      "Epoch 8111 - Discriminator Loss: 0.002, Generator Loss: 27.262\n",
      "Epoch 8121 - Discriminator Loss: 0.002, Generator Loss: 27.228\n",
      "Epoch 8131 - Discriminator Loss: 0.000, Generator Loss: 27.181\n",
      "Epoch 8141 - Discriminator Loss: 0.000, Generator Loss: 27.236\n",
      "Epoch 8151 - Discriminator Loss: 0.000, Generator Loss: 26.956\n",
      "Epoch 8161 - Discriminator Loss: 0.009, Generator Loss: 27.171\n",
      "Epoch 8171 - Discriminator Loss: 0.000, Generator Loss: 27.028\n",
      "Epoch 8181 - Discriminator Loss: 0.000, Generator Loss: 26.833\n",
      "Epoch 8191 - Discriminator Loss: 0.001, Generator Loss: 27.306\n",
      "Epoch 8201 - Discriminator Loss: 0.000, Generator Loss: 27.124\n",
      "Epoch 8211 - Discriminator Loss: 0.000, Generator Loss: 27.086\n",
      "Epoch 8221 - Discriminator Loss: 0.000, Generator Loss: 27.097\n",
      "Epoch 8231 - Discriminator Loss: 0.000, Generator Loss: 27.207\n",
      "Epoch 8241 - Discriminator Loss: 0.000, Generator Loss: 27.202\n",
      "Epoch 8251 - Discriminator Loss: 0.000, Generator Loss: 27.277\n",
      "Epoch 8261 - Discriminator Loss: 0.001, Generator Loss: 27.348\n",
      "Epoch 8271 - Discriminator Loss: 0.020, Generator Loss: 27.311\n",
      "Epoch 8281 - Discriminator Loss: 0.000, Generator Loss: 27.309\n",
      "Epoch 8291 - Discriminator Loss: 0.001, Generator Loss: 27.266\n",
      "Epoch 8301 - Discriminator Loss: 0.000, Generator Loss: 27.307\n",
      "Epoch 8311 - Discriminator Loss: 0.001, Generator Loss: 27.222\n",
      "Epoch 8321 - Discriminator Loss: 0.056, Generator Loss: 27.237\n",
      "Epoch 8331 - Discriminator Loss: 0.001, Generator Loss: 27.215\n",
      "Epoch 8341 - Discriminator Loss: 0.000, Generator Loss: 27.212\n",
      "Epoch 8351 - Discriminator Loss: 0.022, Generator Loss: 27.239\n",
      "Epoch 8361 - Discriminator Loss: 0.000, Generator Loss: 27.248\n",
      "Epoch 8371 - Discriminator Loss: 0.002, Generator Loss: 27.244\n",
      "Epoch 8381 - Discriminator Loss: 0.008, Generator Loss: 27.058\n",
      "Epoch 8391 - Discriminator Loss: 0.000, Generator Loss: 27.145\n",
      "Epoch 8401 - Discriminator Loss: 0.019, Generator Loss: 27.117\n",
      "Epoch 8411 - Discriminator Loss: 0.000, Generator Loss: 27.328\n",
      "Epoch 8421 - Discriminator Loss: 0.001, Generator Loss: 27.134\n",
      "Epoch 8431 - Discriminator Loss: 0.001, Generator Loss: 27.179\n",
      "Epoch 8441 - Discriminator Loss: 0.001, Generator Loss: 27.154\n",
      "Epoch 8451 - Discriminator Loss: 0.000, Generator Loss: 27.174\n",
      "Epoch 8461 - Discriminator Loss: 0.001, Generator Loss: 27.152\n",
      "Epoch 8471 - Discriminator Loss: 0.001, Generator Loss: 27.199\n",
      "Epoch 8481 - Discriminator Loss: 0.000, Generator Loss: 27.318\n",
      "Epoch 8491 - Discriminator Loss: 0.004, Generator Loss: 27.255\n",
      "Epoch 8501 - Discriminator Loss: 0.000, Generator Loss: 27.412\n",
      "Epoch 8511 - Discriminator Loss: 0.001, Generator Loss: 27.219\n",
      "Epoch 8521 - Discriminator Loss: 0.000, Generator Loss: 27.176\n",
      "Epoch 8531 - Discriminator Loss: 0.001, Generator Loss: 27.361\n",
      "Epoch 8541 - Discriminator Loss: 0.002, Generator Loss: 27.288\n",
      "Epoch 8551 - Discriminator Loss: 0.021, Generator Loss: 27.327\n",
      "Epoch 8561 - Discriminator Loss: 0.000, Generator Loss: 27.516\n",
      "Epoch 8571 - Discriminator Loss: 0.001, Generator Loss: 27.128\n",
      "Epoch 8581 - Discriminator Loss: 0.000, Generator Loss: 27.383\n",
      "Epoch 8591 - Discriminator Loss: 0.003, Generator Loss: 27.339\n",
      "Epoch 8601 - Discriminator Loss: 0.000, Generator Loss: 27.181\n",
      "Epoch 8611 - Discriminator Loss: 0.000, Generator Loss: 27.201\n",
      "Epoch 8621 - Discriminator Loss: 0.000, Generator Loss: 27.283\n",
      "Epoch 8631 - Discriminator Loss: 0.000, Generator Loss: 27.266\n",
      "Epoch 8641 - Discriminator Loss: 0.000, Generator Loss: 27.394\n",
      "Epoch 8651 - Discriminator Loss: 0.000, Generator Loss: 27.271\n",
      "Epoch 8661 - Discriminator Loss: 0.000, Generator Loss: 27.283\n",
      "Epoch 8671 - Discriminator Loss: 0.001, Generator Loss: 27.292\n",
      "Epoch 8681 - Discriminator Loss: 0.025, Generator Loss: 27.342\n",
      "Epoch 8691 - Discriminator Loss: 0.013, Generator Loss: 27.282\n",
      "Epoch 8701 - Discriminator Loss: 0.000, Generator Loss: 27.271\n",
      "Epoch 8711 - Discriminator Loss: 0.000, Generator Loss: 27.281\n",
      "Epoch 8721 - Discriminator Loss: 0.000, Generator Loss: 27.393\n",
      "Epoch 8731 - Discriminator Loss: 0.003, Generator Loss: 27.214\n",
      "Epoch 8741 - Discriminator Loss: 0.001, Generator Loss: 27.293\n",
      "Epoch 8751 - Discriminator Loss: 0.001, Generator Loss: 27.169\n",
      "Epoch 8761 - Discriminator Loss: 0.000, Generator Loss: 27.067\n",
      "Epoch 8771 - Discriminator Loss: 0.007, Generator Loss: 26.949\n",
      "Epoch 8781 - Discriminator Loss: 0.000, Generator Loss: 27.134\n",
      "Epoch 8791 - Discriminator Loss: 0.000, Generator Loss: 27.105\n",
      "Epoch 8801 - Discriminator Loss: 0.000, Generator Loss: 26.981\n",
      "Epoch 8811 - Discriminator Loss: 0.006, Generator Loss: 27.339\n",
      "Epoch 8821 - Discriminator Loss: 0.000, Generator Loss: 27.210\n",
      "Epoch 8831 - Discriminator Loss: 0.000, Generator Loss: 27.199\n",
      "Epoch 8841 - Discriminator Loss: 0.003, Generator Loss: 27.237\n",
      "Epoch 8851 - Discriminator Loss: 0.001, Generator Loss: 27.253\n",
      "Epoch 8861 - Discriminator Loss: 0.001, Generator Loss: 27.368\n",
      "Epoch 8871 - Discriminator Loss: 0.000, Generator Loss: 27.216\n",
      "Epoch 8881 - Discriminator Loss: 0.000, Generator Loss: 27.078\n",
      "Epoch 8891 - Discriminator Loss: 0.000, Generator Loss: 27.239\n",
      "Epoch 8901 - Discriminator Loss: 0.000, Generator Loss: 27.065\n",
      "Epoch 8911 - Discriminator Loss: 0.000, Generator Loss: 27.199\n",
      "Epoch 8921 - Discriminator Loss: 0.002, Generator Loss: 27.408\n",
      "Epoch 8931 - Discriminator Loss: 0.000, Generator Loss: 27.333\n",
      "Epoch 8941 - Discriminator Loss: 0.001, Generator Loss: 27.115\n",
      "Epoch 8951 - Discriminator Loss: 0.000, Generator Loss: 27.217\n",
      "Epoch 8961 - Discriminator Loss: 0.000, Generator Loss: 27.299\n",
      "Epoch 8971 - Discriminator Loss: 0.000, Generator Loss: 27.135\n",
      "Epoch 8981 - Discriminator Loss: 0.000, Generator Loss: 27.008\n",
      "Epoch 8991 - Discriminator Loss: 0.000, Generator Loss: 27.254\n",
      "Epoch 9001 - Discriminator Loss: 0.001, Generator Loss: 27.211\n",
      "Epoch 9011 - Discriminator Loss: 0.001, Generator Loss: 27.421\n",
      "Epoch 9021 - Discriminator Loss: 0.000, Generator Loss: 27.059\n",
      "Epoch 9031 - Discriminator Loss: 0.004, Generator Loss: 27.272\n",
      "Epoch 9041 - Discriminator Loss: 0.000, Generator Loss: 27.325\n",
      "Epoch 9051 - Discriminator Loss: 0.000, Generator Loss: 27.226\n",
      "Epoch 9061 - Discriminator Loss: 0.005, Generator Loss: 27.294\n",
      "Epoch 9071 - Discriminator Loss: 0.057, Generator Loss: 27.140\n",
      "Epoch 9081 - Discriminator Loss: 0.012, Generator Loss: 27.240\n",
      "Epoch 9091 - Discriminator Loss: 0.015, Generator Loss: 27.280\n",
      "Epoch 9101 - Discriminator Loss: 0.009, Generator Loss: 27.304\n",
      "Epoch 9111 - Discriminator Loss: 0.002, Generator Loss: 27.445\n",
      "Epoch 9121 - Discriminator Loss: 0.001, Generator Loss: 27.087\n",
      "Epoch 9131 - Discriminator Loss: 0.001, Generator Loss: 27.285\n",
      "Epoch 9141 - Discriminator Loss: 0.002, Generator Loss: 27.497\n",
      "Epoch 9151 - Discriminator Loss: 0.009, Generator Loss: 27.302\n",
      "Epoch 9161 - Discriminator Loss: 0.000, Generator Loss: 27.321\n",
      "Epoch 9171 - Discriminator Loss: 0.001, Generator Loss: 27.399\n",
      "Epoch 9181 - Discriminator Loss: 0.006, Generator Loss: 27.345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9191 - Discriminator Loss: 0.000, Generator Loss: 27.332\n",
      "Epoch 9201 - Discriminator Loss: 0.002, Generator Loss: 27.377\n",
      "Epoch 9211 - Discriminator Loss: 0.000, Generator Loss: 27.298\n",
      "Epoch 9221 - Discriminator Loss: 0.000, Generator Loss: 27.387\n",
      "Epoch 9231 - Discriminator Loss: 0.000, Generator Loss: 27.324\n",
      "Epoch 9241 - Discriminator Loss: 0.000, Generator Loss: 27.159\n",
      "Epoch 9251 - Discriminator Loss: 0.001, Generator Loss: 27.346\n",
      "Epoch 9261 - Discriminator Loss: 0.000, Generator Loss: 27.233\n",
      "Epoch 9271 - Discriminator Loss: 0.002, Generator Loss: 27.183\n",
      "Epoch 9281 - Discriminator Loss: 0.000, Generator Loss: 27.194\n",
      "Epoch 9291 - Discriminator Loss: 0.000, Generator Loss: 27.236\n",
      "Epoch 9301 - Discriminator Loss: 0.000, Generator Loss: 27.295\n",
      "Epoch 9311 - Discriminator Loss: 0.000, Generator Loss: 27.215\n",
      "Epoch 9321 - Discriminator Loss: 0.012, Generator Loss: 27.218\n",
      "Epoch 9331 - Discriminator Loss: 0.000, Generator Loss: 27.158\n",
      "Epoch 9341 - Discriminator Loss: 0.001, Generator Loss: 27.118\n",
      "Epoch 9351 - Discriminator Loss: 0.000, Generator Loss: 26.990\n",
      "Epoch 9361 - Discriminator Loss: 0.000, Generator Loss: 27.279\n",
      "Epoch 9371 - Discriminator Loss: 0.000, Generator Loss: 27.196\n",
      "Epoch 9381 - Discriminator Loss: 0.000, Generator Loss: 27.214\n",
      "Epoch 9391 - Discriminator Loss: 0.000, Generator Loss: 27.076\n",
      "Epoch 9401 - Discriminator Loss: 0.000, Generator Loss: 27.087\n",
      "Epoch 9411 - Discriminator Loss: 0.000, Generator Loss: 27.118\n",
      "Epoch 9421 - Discriminator Loss: 0.006, Generator Loss: 27.369\n",
      "Epoch 9431 - Discriminator Loss: 0.004, Generator Loss: 27.282\n",
      "Epoch 9441 - Discriminator Loss: 0.001, Generator Loss: 27.160\n",
      "Epoch 9451 - Discriminator Loss: 0.000, Generator Loss: 27.168\n",
      "Epoch 9461 - Discriminator Loss: 0.000, Generator Loss: 27.294\n",
      "Epoch 9471 - Discriminator Loss: 0.003, Generator Loss: 27.168\n",
      "Epoch 9481 - Discriminator Loss: 0.005, Generator Loss: 27.118\n",
      "Epoch 9491 - Discriminator Loss: 0.001, Generator Loss: 27.164\n",
      "Epoch 9501 - Discriminator Loss: 0.001, Generator Loss: 27.088\n",
      "Epoch 9511 - Discriminator Loss: 0.000, Generator Loss: 27.120\n",
      "Epoch 9521 - Discriminator Loss: 0.000, Generator Loss: 27.117\n",
      "Epoch 9531 - Discriminator Loss: 0.005, Generator Loss: 27.341\n",
      "Epoch 9541 - Discriminator Loss: 0.001, Generator Loss: 27.304\n",
      "Epoch 9551 - Discriminator Loss: 0.000, Generator Loss: 27.040\n",
      "Epoch 9561 - Discriminator Loss: 0.000, Generator Loss: 27.143\n",
      "Epoch 9571 - Discriminator Loss: 0.000, Generator Loss: 27.154\n",
      "Epoch 9581 - Discriminator Loss: 0.001, Generator Loss: 27.256\n",
      "Epoch 9591 - Discriminator Loss: 0.001, Generator Loss: 27.301\n",
      "Epoch 9601 - Discriminator Loss: 0.000, Generator Loss: 27.277\n",
      "Epoch 9611 - Discriminator Loss: 0.009, Generator Loss: 27.365\n",
      "Epoch 9621 - Discriminator Loss: 0.001, Generator Loss: 27.295\n",
      "Epoch 9631 - Discriminator Loss: 0.000, Generator Loss: 27.241\n",
      "Epoch 9641 - Discriminator Loss: 0.010, Generator Loss: 27.386\n",
      "Epoch 9651 - Discriminator Loss: 0.010, Generator Loss: 27.200\n",
      "Epoch 9661 - Discriminator Loss: 0.004, Generator Loss: 27.282\n",
      "Epoch 9671 - Discriminator Loss: 0.001, Generator Loss: 27.401\n",
      "Epoch 9681 - Discriminator Loss: 0.001, Generator Loss: 27.336\n",
      "Epoch 9691 - Discriminator Loss: 0.000, Generator Loss: 27.123\n",
      "Epoch 9701 - Discriminator Loss: 0.002, Generator Loss: 27.124\n",
      "Epoch 9711 - Discriminator Loss: 0.000, Generator Loss: 27.298\n",
      "Epoch 9721 - Discriminator Loss: 0.001, Generator Loss: 27.348\n",
      "Epoch 9731 - Discriminator Loss: 0.000, Generator Loss: 27.200\n",
      "Epoch 9741 - Discriminator Loss: 0.000, Generator Loss: 27.247\n",
      "Epoch 9751 - Discriminator Loss: 0.001, Generator Loss: 27.407\n",
      "Epoch 9761 - Discriminator Loss: 0.000, Generator Loss: 27.350\n",
      "Epoch 9771 - Discriminator Loss: 0.034, Generator Loss: 27.237\n",
      "Epoch 9781 - Discriminator Loss: 0.000, Generator Loss: 27.262\n",
      "Epoch 9791 - Discriminator Loss: 0.000, Generator Loss: 27.165\n",
      "Epoch 9801 - Discriminator Loss: 0.056, Generator Loss: 27.167\n",
      "Epoch 9811 - Discriminator Loss: 0.000, Generator Loss: 27.224\n",
      "Epoch 9821 - Discriminator Loss: 0.025, Generator Loss: 27.092\n",
      "Epoch 9831 - Discriminator Loss: 0.000, Generator Loss: 27.249\n",
      "Epoch 9841 - Discriminator Loss: 0.001, Generator Loss: 27.155\n",
      "Epoch 9851 - Discriminator Loss: 0.001, Generator Loss: 27.243\n",
      "Epoch 9861 - Discriminator Loss: 0.001, Generator Loss: 27.045\n",
      "Epoch 9871 - Discriminator Loss: 0.008, Generator Loss: 26.909\n",
      "Epoch 9881 - Discriminator Loss: 0.001, Generator Loss: 27.349\n",
      "Epoch 9891 - Discriminator Loss: 0.000, Generator Loss: 26.946\n",
      "Epoch 9901 - Discriminator Loss: 0.001, Generator Loss: 27.209\n",
      "Epoch 9911 - Discriminator Loss: 0.000, Generator Loss: 27.038\n",
      "Epoch 9921 - Discriminator Loss: 0.000, Generator Loss: 26.987\n",
      "Epoch 9931 - Discriminator Loss: 0.001, Generator Loss: 27.032\n",
      "Epoch 9941 - Discriminator Loss: 0.000, Generator Loss: 27.177\n",
      "Epoch 9951 - Discriminator Loss: 0.001, Generator Loss: 27.297\n",
      "Epoch 9961 - Discriminator Loss: 0.000, Generator Loss: 26.953\n",
      "Epoch 9971 - Discriminator Loss: 0.000, Generator Loss: 27.242\n",
      "Epoch 9981 - Discriminator Loss: 0.004, Generator Loss: 26.989\n",
      "Epoch 9991 - Discriminator Loss: 0.000, Generator Loss: 27.026\n"
     ]
    }
   ],
   "source": [
    "# Training DCGANs\n",
    "for epoch in range(num_epochs):\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    synthentic_data = []\n",
    "    for i, fraud_data in enumerate(train_loader):\n",
    "        # Updating the weights of the Discriminator\n",
    "        discriminator.zero_grad() # Initialize gradients of the Discriminator to 0\n",
    "        \n",
    "        mini_batch = fraud_data.size()[0]\n",
    "        \n",
    "        # Wrap data in PyTorch Variable\n",
    "        d_real_data = Variable(fraud_data[0])\n",
    "        y_real = Variable(torch.ones(1))\n",
    "        y_fake = Variable(torch.zeros(1))\n",
    "\n",
    "        # Training the Discriminator with real data\n",
    "        d_real_result = discriminator(d_real_data) # Forward propagate this real data into the neural network\n",
    "        d_real_loss = BCE_loss(d_real_result, y_real) # Compute the loss between the prediction and actual\n",
    "        d_real_loss.backward()\n",
    "    \n",
    "        # Inject fake data to the generator\n",
    "        d_gen_input = Variable(torch.randn(minibatch_size, g_input_size))\n",
    "        d_fake_data = generator(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "        \n",
    "        # Train the Discriminator with a fake data generated by the Generator\n",
    "        d_fake_result = discriminator(d_fake_data.t())\n",
    "        d_fake_loss = BCE_loss(d_fake_result, y_fake)  # zeros = fake\n",
    "        d_fake_loss.backward()\n",
    "        \n",
    "        # Combine discriminator loss from real data and fake data\n",
    "        d_train_loss = d_real_loss + d_fake_loss\n",
    "        \n",
    "        #d_train_loss.backward()\n",
    "        d_optimizer.step()     # Apply SGD to update the weight\n",
    "        d_losses.append(d_train_loss.data[0])\n",
    "        \n",
    "        # Update the weight of the Generator \n",
    "        generator.zero_grad()\n",
    "        gen_input = Variable(torch.randn(minibatch_size, g_input_size))  \n",
    "        g_fake_data = generator(gen_input)\n",
    "        \n",
    "        dg_fake_result = discriminator(g_fake_data.t())\n",
    "        g_loss = BCE_loss(dg_fake_result, y_real)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        g_losses.append(g_loss.data[0])\n",
    "        \n",
    "        synthentic_data.append(d_fake_data.t())\n",
    "        \n",
    "    if epoch % print_interval == 0:       \n",
    "        print('Epoch {} - Discriminator Loss: {:.3f}, Generator Loss: {:.3f}'.format((epoch + 1), \n",
    "                          torch.mean(torch.FloatTensor(d_losses)), torch.mean(torch.FloatTensor(g_losses))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10f891a20>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(d_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10fa2cac8>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.2158,  0.4744, -0.2113, -0.2156,  0.8777, -0.2484, -0.1344,\n",
       "          -0.1324, -0.1341,  2.2702, -0.2136, -0.1338, -1.5878, -0.1558,\n",
       "          -0.1292, -0.6270,  1.3748, -0.1338, -0.1336, -0.1579, -0.0339,\n",
       "          -0.2090, -0.1350, -0.1105,  1.2173, -0.5646, -0.7615,  3.8513,\n",
       "          -0.4996]]),\n",
       " tensor([[-0.2155, -0.2074, -0.6080, -0.1338, -0.1260, -0.1338,  2.8185,\n",
       "          -0.6258, -0.2205,  3.4378, -0.2137, -0.1658, -0.2711, -0.4103,\n",
       "          -0.2159, -0.6138, -0.1338,  0.4030, -0.2804, -0.0928, -0.2156,\n",
       "           0.1333, -1.1650,  3.0356,  0.4991, -0.1344,  0.5234,  1.9947,\n",
       "          -0.1338]]),\n",
       " tensor([[-0.2196,  0.0001, -0.1683,  2.0008, -0.1654, -0.2154, -0.5788,\n",
       "          -1.8874, -0.2160,  2.5405, -0.5856,  1.1708,  0.9101, -0.2131,\n",
       "           0.5511,  1.0289, -0.2154, -0.2156, -0.1338,  0.5224, -0.1338,\n",
       "          -0.1338, -0.2151, -0.1592, -0.1338,  2.6711, -0.0002,  1.2639,\n",
       "          -0.5059]]),\n",
       " tensor([[-0.1399, -0.2074, -0.1338,  0.1544, -0.2130, -1.3545, -0.1407,\n",
       "          -0.1660, -0.2152, -0.2155, -0.1338,  0.8127,  1.6709, -0.1338,\n",
       "          -0.4678, -0.1996, -0.1574, -0.2523,  1.1339, -0.3206, -0.4966,\n",
       "          -0.1883, -0.2146, -0.2143,  0.4978, -0.2145,  2.0155,  3.0311,\n",
       "          -0.5906]]),\n",
       " tensor([[-0.1338,  3.0440, -1.7303,  2.7609, -0.2016, -0.2417, -0.4107,\n",
       "          -0.2137, -0.2300, -0.2709, -0.0160, -0.1338, -0.3166,  1.4371,\n",
       "           1.3552, -0.2045, -0.1360, -0.5861,  0.4144, -0.1338, -0.2156,\n",
       "          -0.2152, -0.2034,  2.9534, -0.1339, -0.1775,  0.3135, -0.2593,\n",
       "          -0.1645]]),\n",
       " tensor([[-0.2155, -0.2470, -0.1308, -0.1333, -0.1352, -1.6985, -0.8495,\n",
       "           1.8228, -0.1718, -0.1210,  0.1616, -0.2141, -0.1338, -0.2153,\n",
       "          -0.6383, -0.1374, -0.2230,  2.3495, -0.1768, -0.2118,  2.2031,\n",
       "           3.2189,  0.4606, -1.3275, -0.2155, -2.0239, -0.2141,  2.9204,\n",
       "          -0.1919]]),\n",
       " tensor([[ 0.7410,  2.9355, -0.1338,  0.0165, -0.1428, -0.1368,  1.0009,\n",
       "          -0.1388,  0.6107,  0.5740,  0.4310,  2.5799,  2.9973, -0.1338,\n",
       "          -0.2156, -1.8505, -0.1845, -0.4038, -0.1338, -1.4520, -0.1338,\n",
       "          -1.1316,  1.8057, -0.1521, -0.5844, -0.2154, -0.2966, -0.1755,\n",
       "          -0.1454]]),\n",
       " tensor([[-0.1338, -0.1364,  0.6443,  3.0528, -0.2158, -1.6400, -0.1440,\n",
       "           2.8075, -0.1269, -0.3448, -0.1445, -0.4006, -0.1340, -0.1332,\n",
       "          -1.1484,  0.5189, -0.2154, -2.0611,  3.5880, -0.2594, -0.1718,\n",
       "           1.7021, -0.2012,  1.7345,  2.5433,  0.0148, -0.1339, -0.1338,\n",
       "           1.5960]]),\n",
       " tensor([[-0.1717,  2.7631,  1.8939, -0.2162, -0.2423, -0.1338, -0.2156,\n",
       "           3.1762, -0.2058,  1.3572, -0.6644,  2.5875, -0.1343,  0.1868,\n",
       "          -0.1356, -0.1338, -0.1342, -0.2155,  1.5674, -0.4825, -1.7295,\n",
       "          -0.5857, -1.4851,  1.4399, -0.4381,  2.5252, -0.0659, -0.1107,\n",
       "          -0.0438]]),\n",
       " tensor([[-1.0925, -0.1346, -0.1347,  0.7132,  1.8157, -0.2170, -0.2122,\n",
       "          -0.5065, -0.1342, -0.2155, -1.2270, -0.8754, -0.8563, -0.2348,\n",
       "          -0.2138,  0.3474,  1.6861, -0.1349, -0.1338, -0.1339,  1.8954,\n",
       "          -0.1656, -1.3198, -0.9259, -0.3303,  1.4725, -0.4007,  2.0195,\n",
       "          -0.4931]]),\n",
       " tensor([[-0.2169, -0.1365, -0.0222, -0.2155, -0.1351, -0.2531, -0.2127,\n",
       "          -0.2084, -0.5306, -0.2156, -0.1903, -0.2163, -0.1645,  0.4149,\n",
       "          -0.0922, -0.5820, -0.4197, -0.2016, -0.2482,  1.1293, -0.1338,\n",
       "          -0.1342, -0.2152, -0.6757, -0.2108, -0.3223,  2.8980, -0.5103,\n",
       "           0.2939]]),\n",
       " tensor([[-0.2138, -0.2356, -0.0876,  2.9916, -0.2081, -0.4076, -0.5250,\n",
       "          -0.3589, -0.2748, -0.1338, -0.2081, -0.0585, -0.1360,  2.6487,\n",
       "          -0.1338, -0.1338, -0.1543,  1.9133, -0.1338, -0.5869, -0.2235,\n",
       "          -0.2133, -0.1430,  1.0296, -0.9498, -0.2152, -0.2287, -0.2157,\n",
       "          -0.4354]]),\n",
       " tensor([[-0.0802,  0.1726, -0.2141, -0.5294,  2.2910,  0.3309, -0.1078,\n",
       "          -0.7986, -0.5869, -0.1611, -0.1668, -0.2144, -0.2947, -0.1355,\n",
       "          -0.1182, -0.2152, -0.1468, -0.1338, -0.1347,  2.0507,  2.3709,\n",
       "          -0.1341,  2.4599, -0.3855, -0.3256,  0.4270,  2.7116, -0.2155,\n",
       "          -0.5837]]),\n",
       " tensor([[-0.6181, -0.2155,  0.3326, -0.4263,  2.4977, -0.2154, -1.6584,\n",
       "          -0.1338,  2.8329, -0.1884, -0.2535, -0.5666,  3.3457, -0.3627,\n",
       "          -0.1607, -0.4356, -0.2019, -0.1219, -0.1947, -1.6300,  1.7454,\n",
       "          -0.5868, -0.4667, -0.1488, -0.1338, -0.1515, -1.2797, -0.2082,\n",
       "           3.1153]]),\n",
       " tensor([[ 0.8644, -0.5852, -0.3507, -0.2154,  1.9398, -0.1590, -0.2149,\n",
       "          -0.2030, -0.1342, -0.1548, -0.2081, -0.1338, -1.6653,  2.0808,\n",
       "           0.7618,  1.0642, -0.2153,  0.2893,  0.4692,  3.1163,  0.0803,\n",
       "           2.3191,  0.7763,  0.4249, -0.2156, -0.2156, -0.2159, -0.1758,\n",
       "          -0.1338]]),\n",
       " tensor([[-0.1338,  0.0805,  0.0837, -0.1339,  0.8856, -0.2154, -0.1994,\n",
       "           1.6417, -1.8900,  2.3066, -0.1338, -0.2149, -2.3953,  0.7353,\n",
       "          -0.2229, -0.2149,  2.3915, -0.1900,  2.8725, -0.1338, -0.1428,\n",
       "          -0.2450, -0.2153,  0.0122, -0.2092, -0.1338, -0.5612, -0.1883,\n",
       "          -0.2376]]),\n",
       " tensor([[ 1.0576,  2.5802, -0.1338, -0.1362,  2.7469,  1.3040, -0.2219,\n",
       "          -0.5522,  0.6240, -0.2142, -0.1338, -0.2095, -0.1274, -0.1822,\n",
       "          -0.2084, -0.5560, -0.2155, -0.1338, -0.3506,  2.9644,  0.2208,\n",
       "          -0.1338, -0.1338,  2.8978, -0.2197,  2.8816, -0.2151, -0.1061,\n",
       "           1.0181]]),\n",
       " tensor([[-1.4796,  0.0546,  1.9554, -0.5525, -0.1343, -0.1370, -0.1397,\n",
       "           0.7562, -0.1791,  2.0406, -1.6749,  0.3047,  1.2064, -0.1338,\n",
       "          -0.0725,  0.9238, -0.1595, -0.2154, -0.0504, -0.5479,  1.8691,\n",
       "          -0.5444,  0.8076, -0.5420, -0.5513,  0.1465,  1.1846, -0.1775,\n",
       "           0.1310]]),\n",
       " tensor([[-0.1937, -2.3476, -0.4907, -0.1936, -0.1375, -0.6727, -0.0251,\n",
       "          -0.2095, -0.2120, -0.1377, -0.2141, -0.0507, -0.2040, -0.5919,\n",
       "          -0.5871, -0.2178,  0.7417,  2.9953, -0.2194, -0.0901, -0.1374,\n",
       "          -0.1375,  0.5525, -0.2144, -0.1380, -0.1588, -0.2122, -0.7725,\n",
       "          -0.1376]]),\n",
       " tensor([[ 0.1330, -0.2158, -0.1394, -0.1455, -0.2328, -0.3875,  2.7369,\n",
       "           0.2480, -0.1394, -0.2194,  1.8977, -0.1223, -0.4485, -0.1641,\n",
       "           1.2462, -0.1042,  1.2198,  0.7914,  2.3209, -0.5916, -0.0337,\n",
       "          -0.2096, -0.1394, -0.4883, -0.5628, -1.4276, -0.1407,  0.1013,\n",
       "          -0.2378]]),\n",
       " tensor([[-0.2226, -0.2225, -0.1044, -0.2678,  0.4444, -0.2224,  0.2202,\n",
       "          -0.0230, -0.1447, -0.5789, -0.1813, -0.2224,  0.3155, -0.5461,\n",
       "          -0.2220, -0.2222,  0.5786,  2.0912, -0.5051, -0.2181, -2.1314,\n",
       "          -1.4820, -0.3034,  2.7984,  0.2957, -0.2583, -0.1678, -0.1463,\n",
       "          -0.2147]]),\n",
       " tensor([[-0.0666, -0.2230, -0.2224,  3.0721,  2.0761, -0.4158, -0.1845,\n",
       "          -0.4249,  2.3452, -0.5945, -0.3044, -0.1851, -0.5933, -0.1408,\n",
       "          -0.1411, -0.5482,  1.0939,  0.3349, -0.0607, -0.2209,  0.7823,\n",
       "          -0.1507, -0.2175, -0.2056, -0.1511, -0.6376, -0.1886, -0.1792,\n",
       "          -0.1408]]),\n",
       " tensor([[-0.4600, -0.6489, -0.2232, -0.2859, -0.1410, -0.1410, -0.1715,\n",
       "          -0.0911, -0.2186, -0.5653,  0.7802,  0.0060,  2.4704, -0.2232,\n",
       "           0.1896,  1.4926,  0.3891, -0.2207, -0.0958,  0.0152, -0.4805,\n",
       "          -0.0379, -0.2143, -0.1410, -0.1518, -0.1896, -0.2138,  1.1903,\n",
       "          -0.1779]]),\n",
       " tensor([[-0.2198, -0.1710, -0.2233, -0.6372,  1.1663, -0.1459, -0.5209,\n",
       "           2.1518, -0.2571,  0.1058,  1.3310,  2.5677, -1.6278, -0.1536,\n",
       "          -0.1745,  2.6530, -0.2234, -0.1694, -0.1484, -0.4847, -0.1615,\n",
       "          -0.2986, -0.3740,  3.2658,  0.1936, -0.0137, -0.1415, -0.1796,\n",
       "           1.2670]]),\n",
       " tensor([[-0.5886, -0.1412, -1.8956, -0.2119, -0.3803,  2.3319, -0.1538,\n",
       "          -1.8307,  2.9034, -0.1608, -0.2605, -0.5857, -0.5887,  2.7250,\n",
       "          -0.5215, -1.5921, -0.1585, -0.1619, -0.1412, -0.1826,  0.5934,\n",
       "           1.6815, -0.4589,  2.1061,  0.3140, -0.1436, -0.6544, -0.2432,\n",
       "          -0.5850]]),\n",
       " tensor([[-0.1480, -0.2383,  2.2532, -0.0917, -0.2189,  2.3127, -0.1385,\n",
       "          -0.1467,  0.6745, -0.1424, -0.1035, -0.1602, -0.2174,  2.0177,\n",
       "           0.8704, -0.4425, -0.1385, -0.5867,  0.2511, -0.2232,  2.9642,\n",
       "          -0.1367, -0.2568, -0.1867, -0.2185, -0.3010,  0.5901, -0.7190,\n",
       "          -0.7163]]),\n",
       " tensor([[-0.1487, -0.1708, -0.2925,  1.6670,  2.3778, -0.1416,  2.7312,\n",
       "           2.3351,  1.7195, -0.2664, -0.1352,  2.7896, -0.1520, -0.1387,\n",
       "          -0.5645, -0.1347,  2.4272,  0.3012, -1.0401,  0.8329, -0.4750,\n",
       "           0.4763,  0.6500, -0.0173, -0.2166,  0.0680, -0.1911, -0.1921,\n",
       "          -0.5793]]),\n",
       " tensor([[-0.2155, -0.6237, -0.1429, -0.9155, -0.5790, -0.2158, -1.3376,\n",
       "           0.3388, -0.4003, -0.1322, -0.2155, -0.2149, -0.1945, -0.1389,\n",
       "          -0.6394, -0.2155, -0.1011,  0.3213, -2.0533, -0.1333, -0.1334,\n",
       "          -0.1337, -0.5755, -0.2117, -0.1141, -0.1436, -0.2859, -0.1927,\n",
       "          -0.2200]]),\n",
       " tensor([[-0.1329,  1.6585, -0.5837, -0.1341,  0.4451,  0.3474, -0.5041,\n",
       "          -0.1328, -0.2035, -0.0864,  3.4717, -0.2200,  1.1423, -0.2149,\n",
       "           2.6206, -0.2388,  2.2252, -0.1396, -1.0481, -0.1508, -0.2059,\n",
       "           2.0783, -0.2681, -0.2149, -0.7695, -0.5613, -0.2149, -0.2049,\n",
       "          -0.1394]]),\n",
       " tensor([[-0.2146, -0.1416, -0.1330,  0.4028, -0.1425,  0.1671,  2.9035,\n",
       "          -0.5853, -0.1249, -0.1351,  2.2621, -0.1799,  0.1938, -0.6205,\n",
       "          -0.4931, -0.2145, -0.2097, -0.2140,  0.0996,  2.2599, -0.5839,\n",
       "           0.1136, -0.1506, -0.0585, -0.1913,  0.1493, -0.1811, -1.9019,\n",
       "           1.9686]]),\n",
       " tensor([[-0.5843, -0.1325,  2.1500, -0.5852, -0.2127,  1.3643, -0.0304,\n",
       "          -0.1324,  2.8776, -0.2140, -0.2121, -0.2237, -0.2043,  1.3745,\n",
       "          -0.1620, -0.1468,  2.8084, -0.2145, -0.1334, -0.1362, -0.1310,\n",
       "          -1.2925, -0.2135, -0.1699, -0.1334, -0.1324,  1.9504, -0.2143,\n",
       "          -0.2141]]),\n",
       " tensor([[-0.0772, -0.1346, -0.1394,  2.2441, -0.3019, -0.1878,  1.8977,\n",
       "          -0.2126, -0.5966, -1.0566,  2.4666, -0.7700, -0.1325, -0.2137,\n",
       "          -0.1436, -0.1331, -0.4341, -0.1352, -0.7203,  1.0285, -0.2764,\n",
       "          -0.1323,  2.2153, -0.5339, -0.1540, -0.1323, -0.1898, -0.7064,\n",
       "          -0.2164]]),\n",
       " tensor([[-0.1451, -0.2084, -0.5237, -0.1329, -0.5852, -0.1422,  0.7173,\n",
       "           1.3626, -1.4053, -1.7124,  0.4377, -0.6320, -0.2143,  1.7281,\n",
       "          -0.5836, -0.5942,  2.1954, -0.2144, -0.1428, -0.2143, -0.0919,\n",
       "           2.2460, -0.2499,  0.3863,  0.8896, -0.0999,  0.4998, -0.4436,\n",
       "          -0.1462]]),\n",
       " tensor([[-0.8136, -0.1346, -0.1323, -0.2143,  1.8998, -0.5850, -0.1891,\n",
       "          -0.0677, -0.2827, -0.2032,  1.4807,  0.9039, -0.4760, -0.1324,\n",
       "          -0.2110, -0.1477, -0.4324,  2.9564, -0.1472,  0.2554, -0.1322,\n",
       "           2.1646, -0.4083,  3.2514, -0.2143,  1.8910, -0.1344,  0.9181,\n",
       "          -0.1322]]),\n",
       " tensor([[ 0.3031, -2.1521, -0.5414, -0.2200,  0.2789, -0.3106, -0.1547,\n",
       "          -0.0329, -0.1611, -0.2490, -0.1322, -0.2070, -0.3135, -0.1322,\n",
       "          -0.1650,  2.7845,  2.0189, -0.4464,  0.4732, -0.4442, -0.3194,\n",
       "          -0.1326, -0.1334, -0.1323,  1.9872,  1.4923,  0.9813,  2.0548,\n",
       "           1.6226]]),\n",
       " tensor([[-0.5832,  0.2352, -0.2120, -0.1875, -0.2930,  2.4395,  2.9549,\n",
       "           1.0264,  0.0508, -0.1434, -0.1328, -0.5448,  2.3118, -1.8088,\n",
       "          -0.4261,  0.8201, -0.6430,  0.5898, -0.1444,  0.0478, -0.1782,\n",
       "          -0.2142, -0.2144, -0.5563, -0.1535, -0.1322, -0.1735,  0.8183,\n",
       "          -0.1322]]),\n",
       " tensor([[ 1.6022, -0.2134,  2.2939,  1.3682, -1.3463, -0.2030, -0.1323,\n",
       "           3.4837,  0.3844, -0.2115, -0.1322, -0.2029, -0.1329, -0.5768,\n",
       "          -0.2250, -0.1321,  0.4554, -0.1789, -0.2095, -0.1324,  2.1562,\n",
       "          -0.1324, -0.5485, -0.0461, -0.2152, -0.1598,  1.2366, -0.1356,\n",
       "          -0.1321]]),\n",
       " tensor([[-0.2142, -0.1342,  1.2628, -0.1243, -1.2881, -0.1322, -0.1953,\n",
       "           0.5224,  2.3411, -0.1330, -0.0241, -0.0914, -0.5868, -0.2144,\n",
       "          -0.1938,  0.7819,  1.7475, -0.3740,  2.4816, -0.7332,  1.8936,\n",
       "           1.4956, -0.2096, -0.1417, -0.2143, -0.1323, -0.2126, -0.1322,\n",
       "           0.0800]]),\n",
       " tensor([[-0.1268,  1.9450, -0.1234, -0.1324, -0.2128, -0.1322, -0.5508,\n",
       "          -0.2140, -0.1455, -0.1358,  2.4490,  1.7054,  2.5347,  0.1822,\n",
       "          -0.1322, -0.2143, -0.2018,  0.6894, -0.1552, -0.3490,  0.2837,\n",
       "           0.1142, -0.1322, -0.1414, -0.1481, -1.7096,  0.3838, -0.5857,\n",
       "          -0.1334]]),\n",
       " tensor([[-0.1990, -0.1322, -0.1322, -1.2607, -0.1322, -0.5139, -0.1367,\n",
       "          -0.1802, -0.2934, -0.1446, -0.2146,  3.3447, -0.1785,  1.7897,\n",
       "           1.8239, -0.2549, -0.1339, -0.1803, -0.1357,  1.5549, -0.1519,\n",
       "          -0.7424,  2.1000, -0.1527, -1.1726, -0.8463,  3.0943, -0.1741,\n",
       "          -0.1329]]),\n",
       " tensor([[ 1.6643,  1.3560, -0.0867,  2.5028,  0.2947,  1.3083, -0.2106,\n",
       "          -0.2090, -0.6658, -0.1322, -0.7625, -0.0725, -0.5519, -0.1734,\n",
       "          -0.2141, -0.2012, -0.1377,  2.5570, -0.2138,  0.8286, -0.6852,\n",
       "           0.3365, -0.2170,  2.5966, -0.4581, -0.2144, -0.0961, -0.2137,\n",
       "           2.0269]]),\n",
       " tensor([[ 0.2891, -0.1322, -0.5515, -0.1320, -0.1324, -2.1221, -0.6556,\n",
       "          -0.1325, -2.6206, -0.1702, -0.5573, -0.1363,  0.2912, -0.1988,\n",
       "          -0.1820,  2.5216, -0.5740,  2.3188,  2.5800,  2.8744,  1.1385,\n",
       "           0.4130,  0.6241, -1.1424, -0.2143, -0.0028, -0.1673, -1.2883,\n",
       "          -0.6578]]),\n",
       " tensor([[-0.1327, -0.1862, -0.5471,  0.8000, -0.9004, -0.1320, -0.1326,\n",
       "          -0.1324, -0.1322, -0.5360, -0.2159, -0.2104, -0.2143,  0.6709,\n",
       "           2.2623,  1.3873, -0.1322, -0.0328, -0.2127, -0.1334, -0.3116,\n",
       "          -0.3200, -0.2172, -0.1198, -0.1322,  1.5918, -0.6805, -0.1305,\n",
       "          -0.1698]]),\n",
       " tensor([[-0.1300, -1.2302, -0.1348,  0.0157,  1.9272, -0.4219, -0.1454,\n",
       "          -0.2290, -0.2160, -0.1780, -0.6966,  2.4035, -0.4979, -0.1348,\n",
       "           3.0771,  1.6128, -0.8982, -0.1351,  0.1354,  2.0412, -0.6639,\n",
       "          -0.2464, -0.2599,  0.0744,  1.9582,  1.8056, -1.6047, -0.1289,\n",
       "          -0.5258]]),\n",
       " tensor([[-1.4049, -0.4596,  1.8680, -0.2041, -0.1362, -0.2437, -0.2177,\n",
       "           0.1572,  3.2411, -0.1408,  0.2272, -0.1636,  0.8066,  1.9232,\n",
       "          -0.5563,  2.1393,  2.1356, -0.1367, -1.0392, -0.1897, -0.0776,\n",
       "          -0.6248, -0.4935, -1.5029, -0.4979, -0.5584, -0.1363,  0.5116,\n",
       "          -0.2073]]),\n",
       " tensor([[-0.2175, -0.3688, -0.0741, -0.2188, -0.4596, -0.1378, -0.2060,\n",
       "          -0.1369, -0.1382,  0.0446, -0.1746, -0.1740,  0.6166, -0.1368,\n",
       "          -0.2188, -0.5828, -2.1036, -0.5725, -0.2680,  2.2755, -0.2405,\n",
       "           0.0264, -0.1368,  0.0459, -0.2183, -0.1368,  1.6427,  0.9308,\n",
       "           0.6084]]),\n",
       " tensor([[-0.1940, -0.4075, -0.2189, -2.0028, -0.1576, -0.5719,  1.5617,\n",
       "           0.0864, -0.1423, -1.9194,  0.3073,  3.5681, -0.1418, -0.2113,\n",
       "           2.3842, -0.1566, -0.1371, -0.1891, -0.3896, -0.1398, -0.1456,\n",
       "          -0.1392,  1.1823, -0.2190, -0.1629,  0.1583, -0.1371, -0.1632,\n",
       "          -0.2190]]),\n",
       " tensor([[-0.1397, -0.1373, -0.1856, -0.1729,  0.4128,  2.0524, -0.5870,\n",
       "          -0.5910, -0.1373, -0.5913, -0.2199, -0.1728, -0.1707,  1.7316,\n",
       "          -0.1396,  2.3876, -0.2125, -0.2200, -0.1381, -2.2422,  2.1936,\n",
       "          -0.5816,  0.5710, -1.3018, -0.1373,  2.3388, -0.2226, -0.7032,\n",
       "          -0.3780]]),\n",
       " tensor([[-0.2187, -0.1390,  2.6195, -0.1380, -0.1374, -0.2193, -0.2193,\n",
       "           1.9046,  0.8529, -0.2257, -0.6781, -0.1374,  2.2182,  2.8579,\n",
       "           2.6242, -0.2175, -0.2126,  0.7465,  3.0565,  2.0881, -0.2193,\n",
       "          -0.1484, -0.1375, -0.1631, -0.5863, -0.2192, -1.0222,  2.3089,\n",
       "          -0.1417]]),\n",
       " tensor([[-0.2083, -0.1976, -1.9865,  3.2459, -0.1374, -0.1559, -0.9862,\n",
       "          -0.2193, -0.2007, -0.2194, -0.4695, -0.2163, -0.1838, -0.1427,\n",
       "          -0.0493, -0.1476, -0.1422, -0.2072,  1.2990, -0.2348, -1.3142,\n",
       "          -0.3117, -0.7753, -0.1374, -0.2194, -0.1374, -0.5503,  1.3557,\n",
       "           2.3568]]),\n",
       " tensor([[ 0.7262, -0.1375, -0.5248, -0.1384, -0.7032,  0.1504,  0.2796,\n",
       "          -0.2057,  1.8936, -0.1383,  2.6135,  1.6910, -0.1627, -0.1326,\n",
       "          -0.5538, -0.2281, -0.5825,  2.3137,  1.0614, -2.0004, -0.2014,\n",
       "          -0.5880, -0.1391,  2.6144, -0.1534, -1.1498,  0.7420, -0.1741,\n",
       "          -0.1506]]),\n",
       " tensor([[-0.1970, -0.2146, -0.1399, -0.4558, -0.1650,  1.7477, -0.2193,\n",
       "          -0.8844, -0.2194,  2.3995, -0.3004,  2.4166, -0.1457, -0.5881,\n",
       "          -0.1443, -0.2194, -0.3948, -0.2152, -1.4039, -0.2194,  2.2954,\n",
       "          -0.5485, -0.1374,  1.6193,  1.0099, -0.0913,  1.8050, -0.2949,\n",
       "          -0.1374]]),\n",
       " tensor([[-0.7259,  2.2059,  3.1388, -0.1374, -0.3698, -0.1414, -0.2089,\n",
       "           0.7047, -0.2620, -0.2194, -0.5759,  0.0242, -0.1788, -0.2160,\n",
       "           2.3838, -0.2194,  2.5466, -0.1463, -1.3663, -0.1480, -0.1393,\n",
       "          -0.5827,  2.4527, -0.4388, -0.2194, -0.2027, -0.1380, -0.2037,\n",
       "          -0.2183]]),\n",
       " tensor([[ 0.2582, -0.1379, -0.3183,  0.4439,  1.3751,  0.8009, -0.5272,\n",
       "          -0.1367, -1.2948,  2.2141,  1.7961, -0.1384, -0.8878, -0.1374,\n",
       "          -0.1389,  0.6107, -0.2251, -0.1374, -0.1923,  0.1757, -0.1168,\n",
       "          -0.3744,  2.3116,  0.7136, -0.2076, -1.1696, -0.2194, -0.0263,\n",
       "           1.7187]]),\n",
       " tensor([[-0.2194, -0.1380, -0.5904,  2.0257,  1.3104, -0.7013,  1.4332,\n",
       "          -0.1849,  2.7115,  1.7849,  0.7282, -0.2190, -0.1382, -0.2194,\n",
       "          -1.0345,  1.6462, -0.2184,  0.5789, -1.2799,  0.3452, -0.1447,\n",
       "          -0.5910,  1.3097, -1.2077, -0.1373,  0.1636,  2.4445, -0.1388,\n",
       "          -0.6740]]),\n",
       " tensor([[-0.5323,  2.7799, -0.1435, -0.6908, -0.1587, -0.2192, -0.1374,\n",
       "          -0.2022, -0.3992, -0.0270, -0.1609,  0.1960, -0.2719, -1.7774,\n",
       "           1.2445,  2.2840, -0.4792, -0.1374, -1.1346, -0.1376, -0.1312,\n",
       "          -0.1374, -0.5253, -0.2180, -0.2035,  0.4126, -0.1549,  2.5273,\n",
       "          -0.1387]]),\n",
       " tensor([[ 1.3752, -0.5992,  2.4854,  0.8044,  2.1198, -1.2030,  0.7059,\n",
       "          -0.1367, -0.5691, -0.1381, -0.4651, -0.1648, -0.2124, -0.5908,\n",
       "           2.7406, -0.2173, -0.0243,  0.9884, -0.2019,  0.7961, -0.1374,\n",
       "           0.3672,  2.3950, -0.5814,  3.4114, -0.5281,  2.6789, -0.1374,\n",
       "           1.8380]]),\n",
       " tensor([[-0.2194, -0.1397, -0.1375,  2.4152, -0.1406, -0.1904, -0.1274,\n",
       "          -0.6811, -0.1638,  2.0748, -0.4721,  1.9459, -0.5920,  2.2586,\n",
       "          -0.4379, -0.1461, -0.1461,  2.2489, -0.1586, -0.2192,  0.9617,\n",
       "          -0.1883,  2.2956,  1.5521,  0.9903, -0.1459,  0.0300, -0.2066,\n",
       "          -0.1450]]),\n",
       " tensor([[-0.1453,  0.3266, -0.1359, -0.1414, -0.2702, -0.9479,  0.5834,\n",
       "           1.6452, -0.2077, -0.2399, -0.3306, -0.1568, -0.5763,  1.9451,\n",
       "          -0.1374,  2.5936, -0.1446, -0.1385, -0.5356, -0.5558, -0.1528,\n",
       "          -0.5901,  3.2792, -0.2150, -0.3869, -0.7069,  2.2488,  1.0666,\n",
       "          -0.2194]]),\n",
       " tensor([[-0.1795, -0.1552, -0.2119, -0.2192, -0.1415, -0.2970, -0.5889,\n",
       "          -0.4902, -0.2399, -0.2199,  1.4958,  0.7798,  2.2616, -0.1388,\n",
       "           2.6848, -0.1428,  2.7868, -0.2177, -0.0512, -0.1374, -0.2194,\n",
       "           0.8910, -0.7092,  2.8240, -0.1374, -0.2431, -0.1375, -0.2187,\n",
       "          -0.5907]]),\n",
       " tensor([[-0.1439, -0.2975,  2.1842, -0.1544, -0.5868,  0.4098, -0.1956,\n",
       "           3.2540, -0.1415, -0.1374, -0.1379,  0.8587, -0.1377, -0.1379,\n",
       "          -1.0791, -0.6720, -0.1523,  1.5548,  2.3215, -0.1377, -0.3169,\n",
       "          -0.5863,  0.3612, -0.5699, -0.3572, -1.1368, -0.4771, -0.1383,\n",
       "          -0.1375]]),\n",
       " tensor([[ 0.2150, -0.2234,  2.7055, -0.1373, -0.2189, -0.0911, -0.0131,\n",
       "          -0.2143,  3.0349, -0.1546,  2.2225,  1.2506, -0.2185, -0.1370,\n",
       "          -0.2195, -0.5777, -0.4341,  0.7692, -0.1390, -0.2187,  1.3224,\n",
       "          -0.1371, -0.2189, -0.9475,  1.2298, -0.1390,  2.1062, -0.1370,\n",
       "           2.2647]]),\n",
       " tensor([[-0.8312, -0.1368, -0.2186, -0.0326,  0.1285, -0.2145, -0.1678,\n",
       "          -0.1707, -0.1475, -0.2181,  1.0658,  2.4114,  0.3899, -0.1377,\n",
       "          -0.4650, -0.1479, -0.1371, -0.1404,  1.3855, -0.3730, -0.5904,\n",
       "          -0.1557, -0.1438,  3.2813,  1.4345, -0.2185, -0.1368, -0.6398,\n",
       "          -0.1368]]),\n",
       " tensor([[-0.5912, -1.0399,  0.5670, -0.1456,  0.2779, -0.8341, -0.1402,\n",
       "          -0.1705, -1.9082, -0.1366,  2.0679, -0.5796,  2.2359,  2.8406,\n",
       "          -0.1390,  2.9573, -0.1970, -0.2079, -0.2171, -0.5838, -0.2127,\n",
       "          -0.2171, -0.4726,  3.2135, -0.5553, -1.2484,  1.8029, -0.1366,\n",
       "          -1.6366]]),\n",
       " tensor([[ 0.3236, -0.1362,  0.9258, -0.2120,  1.7798, -0.2046, -0.1370,\n",
       "           0.6013,  2.1274, -0.4806, -0.1394, -0.1451,  3.1248,  1.0854,\n",
       "          -0.2184,  0.5165, -0.1373, -0.1581, -0.1694,  2.7563, -0.5821,\n",
       "           1.0858, -0.1384,  0.2741, -0.2185,  0.3775, -0.4350,  0.4585,\n",
       "          -0.7211]]),\n",
       " tensor([[ 2.8654, -0.2223, -0.2183,  2.4025, -0.2181, -0.1831, -0.2043,\n",
       "          -0.2184, -0.1768, -0.2170,  0.8836, -0.6056,  0.6458, -0.3588,\n",
       "          -0.5289, -0.2388, -0.1373,  2.1464,  3.2286,  2.3486, -0.1367,\n",
       "          -0.2183,  0.3652, -0.1366,  0.4956, -1.8570, -0.5690, -0.5275,\n",
       "          -1.4846]]),\n",
       " tensor([[ 1.2726, -0.2185, -0.2163, -0.2181, -0.4340,  0.4695, -0.2118,\n",
       "          -0.1991, -0.2077, -0.2562,  0.3995, -0.1678, -0.3023, -0.2185,\n",
       "           0.7292,  0.1268,  1.3316,  0.0823, -0.5870, -1.7654, -0.2140,\n",
       "          -0.2176, -0.6588,  0.2325,  0.1841, -0.2177, -0.0234, -0.1481,\n",
       "          -0.1882]]),\n",
       " tensor([[-0.2192, -0.5923, -0.1044, -0.2677, -0.1701,  0.1496, -0.1384,\n",
       "          -0.0682, -0.1987,  1.6307, -0.2822, -0.4497, -0.1375,  1.7261,\n",
       "          -0.1419, -0.0453, -0.1375, -0.5417, -0.2175,  0.6952,  0.2402,\n",
       "           0.5769, -0.1375,  0.4356, -0.0436, -0.5926, -0.2156,  0.0345,\n",
       "          -0.3690]]),\n",
       " tensor([[-0.2202, -0.0426,  2.7129, -0.2205,  1.5823,  2.9055, -0.1385,\n",
       "          -0.1636,  2.5173, -0.2096, -0.1386, -0.2220, -0.2207, -0.2203,\n",
       "           0.5626, -0.1914,  1.7657, -0.2198, -0.2174, -0.8386, -0.8373,\n",
       "          -0.5499, -0.4017,  2.6633,  1.1418, -1.4998,  2.2598,  2.6488,\n",
       "           1.0529]]),\n",
       " tensor([[-0.1393,  0.7207, -0.2212, -0.6091,  0.6429,  0.4148, -0.3192,\n",
       "           2.7383, -0.4503,  2.8413,  1.8814, -0.1391, -0.2212, -0.5718,\n",
       "          -0.1305, -0.1904, -0.2212, -0.2196, -0.1432, -0.1762,  2.7903,\n",
       "          -0.2212,  1.0953,  0.1715,  3.8306,  2.6577, -0.2173, -0.2406,\n",
       "          -0.1732]]),\n",
       " tensor([[-0.1807, -0.5902, -0.2215, -0.1407, -2.0059,  0.3690,  0.7724,\n",
       "           1.6263, -0.2573, -1.5750, -0.2213,  2.3952, -0.1395, -0.1819,\n",
       "          -0.5617, -0.2215,  2.1923, -0.2017, -0.4459, -0.2215, -0.1448,\n",
       "          -0.2586, -0.1435, -1.4177,  2.2325, -0.1892, -1.2553, -0.2343,\n",
       "          -0.1393]]),\n",
       " tensor([[ 2.6937, -0.1919, -0.2204,  0.8313, -0.1395, -0.4321, -0.1078,\n",
       "           0.0930,  0.5454, -0.1396, -0.5970, -0.5585,  2.0549, -0.1848,\n",
       "          -0.1400, -0.2205,  2.4377,  0.1438, -0.1395,  2.0322, -0.5280,\n",
       "          -0.7869, -0.5883, -0.2225, -0.7230, -0.1522, -0.1395, -0.1395,\n",
       "          -0.1945]]),\n",
       " tensor([[ 2.9700, -0.1395, -0.1726, -0.5938,  2.7602,  2.1697, -0.1397,\n",
       "          -0.1396, -0.2216, -0.2827,  2.4568, -0.1642, -0.3796, -1.5642,\n",
       "          -0.1472, -0.1807,  1.9927,  0.1927, -0.1557, -0.1395, -0.2218,\n",
       "          -0.3089,  0.9855, -0.5831, -0.1595, -0.2210, -0.1461, -0.0989,\n",
       "          -0.2218]]),\n",
       " tensor([[-0.5213,  2.8664,  0.8497, -0.1388, -0.1398, -0.2216, -0.1396,\n",
       "          -0.1401, -0.1416, -0.6025, -0.1396, -0.5903,  2.1401, -0.6890,\n",
       "           3.2864, -0.1644, -0.1396, -0.7149, -0.2248, -0.1492, -0.1397,\n",
       "          -0.2210, -0.7117, -0.1959, -0.1207,  2.9755, -0.1520, -0.1397,\n",
       "          -0.3407]]),\n",
       " tensor([[-0.2675, -0.2199,  1.8454, -0.5947, -0.1396,  2.1714, -0.2097,\n",
       "          -0.3162,  1.7456, -0.1631, -1.0319, -0.1420, -0.4582, -0.0315,\n",
       "           0.5075, -0.2215, -1.8160,  1.5805, -0.1402,  0.0615, -0.1453,\n",
       "           0.0906, -0.2202, -0.1954, -0.1604, -0.6537,  0.0018,  2.8489,\n",
       "           2.2235]]),\n",
       " tensor([[-0.2826, -0.2148, -0.1337, -0.2661, -0.4160,  0.3347, -0.1127,\n",
       "          -0.5728,  0.3290, -0.4775, -0.1995, -0.1338,  2.5253, -0.2054,\n",
       "           2.4244, -0.8693,  2.9096, -0.2299, -0.1337, -1.2882, -0.1409,\n",
       "           2.8176, -0.4892, -0.5184,  3.1707,  2.3923, -0.1671, -0.4876,\n",
       "          -0.1773]]),\n",
       " tensor([[ 1.7502,  0.6368, -0.3720, -0.1312,  1.5440, -0.0733, -0.5934,\n",
       "          -0.2457, -0.3427,  0.0879, -0.6911, -1.1721, -0.2739, -0.5836,\n",
       "          -0.1311, -0.2103,  0.3620, -0.2124, -0.0787, -0.2124,  1.8563,\n",
       "           0.3051, -0.8392, -0.3372, -0.1508, -0.5875, -0.2083,  0.8516,\n",
       "          -0.2068]]),\n",
       " tensor([[-0.0826,  3.6094,  1.6611, -1.2801, -0.1320, -0.4934, -0.1366,\n",
       "          -0.2121, -0.4021, -0.5037, -0.1929, -0.9673,  1.8373, -0.2047,\n",
       "          -0.1961,  1.3992, -0.1504, -0.3352, -0.2074, -0.1306, -0.5998,\n",
       "          -0.5795, -1.1167, -0.6281, -0.1329, -0.1986, -0.1650,  0.7691,\n",
       "          -0.5931]]),\n",
       " tensor([[-0.2108, -0.1322, -0.1304,  2.3376,  0.0906,  2.8115, -0.0569,\n",
       "           0.6057, -0.2104,  0.5818, -0.1303, -0.2076, -0.2112, -0.5792,\n",
       "           2.8233, -0.1307, -0.2128, -0.5784, -2.0522,  0.3811, -0.2121,\n",
       "          -0.3407, -0.1986,  0.1517,  1.2896,  2.5543, -0.2217, -0.2158,\n",
       "          -0.2126]]),\n",
       " tensor([[-0.1392, -0.2116, -0.5865, -0.1305, -0.2124,  2.7773, -1.4972,\n",
       "           1.6958, -0.1908, -0.1472, -0.4137, -0.1309,  0.6257, -0.3262,\n",
       "           1.8134,  2.4183, -0.2122, -0.2124,  2.0550,  2.1128, -0.5426,\n",
       "          -0.1305,  0.9215, -0.0722, -0.3299, -0.2118, -0.5850,  1.3766,\n",
       "          -0.1482]]),\n",
       " tensor([[-0.2126, -0.2126,  3.4350, -0.2113,  0.0900,  1.6914, -0.2125,\n",
       "          -0.1311, -0.1819, -0.2144, -1.2213, -0.1306, -0.1262, -1.2069,\n",
       "          -0.5768, -0.5573, -0.2066,  0.1939,  2.7276, -0.3867, -0.2119,\n",
       "          -0.2115, -0.5574, -0.0763, -0.5849, -0.6244, -0.1306, -0.2233,\n",
       "          -0.1306]]),\n",
       " tensor([[-0.0859,  2.5081, -0.2083, -0.9953, -1.3585, -0.5937,  1.4838,\n",
       "           2.1566, -0.1308, -0.8741, -0.1306, -0.2149, -0.7210,  0.3930,\n",
       "          -0.1307, -0.1410, -0.4371, -0.9748,  2.7320, -0.2127, -0.3921,\n",
       "          -0.3749,  1.8337, -1.9540, -0.1306, -0.0516, -0.1311, -0.1664,\n",
       "           0.7868]]),\n",
       " tensor([[-0.2130, -0.1706, -1.2409,  1.3946,  2.2382, -0.2069,  0.4527,\n",
       "          -0.2591, -0.5920,  0.7579, -0.2307, -0.2110, -0.2129, -0.2129,\n",
       "          -0.9026,  0.1752, -0.1337, -1.6474, -0.2129, -0.1351, -0.1998,\n",
       "          -0.1309,  1.3232,  3.2058, -0.9757, -0.3720,  2.4092, -0.1453,\n",
       "           0.1657]]),\n",
       " tensor([[ 2.9577, -0.5645, -0.1837,  2.1069, -0.5485, -0.1992, -0.7159,\n",
       "          -0.1307, -0.4300,  2.0300, -0.3058, -0.1306,  2.6838, -0.1369,\n",
       "          -0.1328, -0.1311, -0.1833, -0.1306,  0.3819,  0.0704, -0.1271,\n",
       "          -0.2122,  0.3971,  3.3502, -0.5456,  2.1402, -0.1552, -0.4511,\n",
       "          -0.1307]]),\n",
       " tensor([[-0.3056,  0.5982, -0.1306, -0.2064, -0.2039,  2.0286,  0.3312,\n",
       "          -1.7079, -0.3113,  0.7690, -0.4049,  2.5571, -0.2095, -0.1385,\n",
       "           3.2114, -0.2129, -0.2128, -0.1310, -0.2121,  2.7003, -0.1315,\n",
       "          -0.4003,  1.5538, -0.1883, -0.1309, -0.1932, -0.1596, -0.5615,\n",
       "          -0.3343]]),\n",
       " tensor([[-0.1307,  2.2631,  1.8926,  1.9096, -1.3802, -0.2378,  2.2141,\n",
       "          -0.5807, -0.3878, -0.2121, -0.1899, -0.1307, -1.1354, -0.2011,\n",
       "           2.2531,  1.1619, -0.1306,  1.0323, -0.1306, -0.1914, -0.1477,\n",
       "          -0.1383, -0.0718, -0.3510, -0.2127,  2.3762,  0.9040,  0.2337,\n",
       "          -0.2127]]),\n",
       " tensor([[ 1.8092, -1.3298, -0.5954, -0.2081, -0.4968, -0.1368,  0.1854,\n",
       "          -0.5533, -0.2127, -0.2206,  2.4757, -0.1481, -0.2196, -1.3827,\n",
       "          -0.2198, -0.3048, -0.1487,  1.0353, -0.5921, -0.5444, -1.7605,\n",
       "          -0.1876,  1.6726, -0.2139,  1.6186, -0.5454,  0.6090, -0.2190,\n",
       "           2.0886]]),\n",
       " tensor([[-0.5570,  1.7899, -1.9816, -0.3650,  1.0256,  2.4393, -0.6152,\n",
       "          -0.1395,  1.7359, -0.5929, -0.2230, -0.7767,  1.7040, -0.4474,\n",
       "           1.1488, -0.1454, -0.5956, -0.1408, -0.2230, -0.2229, -0.1398,\n",
       "          -0.7092, -0.1563, -0.1398, -0.1396, -0.2679, -0.7047,  0.3778,\n",
       "          -0.1398]]),\n",
       " tensor([[-1.2587, -0.1999, -0.2246, -0.5189, -0.0988,  2.9401, -0.2011,\n",
       "          -0.1905, -0.2242,  1.8735, -0.7283, -0.1414, -0.1416, -0.1450,\n",
       "           2.6629, -0.1415,  2.0844, -0.9361, -0.1417, -1.2908, -0.2244,\n",
       "           3.0348, -1.8964,  0.0139, -0.0319,  3.1649, -0.1415, -0.5182,\n",
       "          -0.3677]]),\n",
       " tensor([[-0.3251,  0.4977, -0.2254,  0.3413, -0.2253, -0.1959,  2.9275,\n",
       "          -0.1522, -0.2254, -0.4190, -0.1433,  2.7880,  1.7117, -0.1294,\n",
       "          -0.1431, -0.2255,  0.0731, -0.1931,  0.2240, -0.5987, -0.1424,\n",
       "          -0.8447, -0.2255, -1.0522,  0.5284, -0.6056,  1.9880, -0.4015,\n",
       "          -0.1633]]),\n",
       " tensor([[ 1.0706,  2.8847, -0.2136, -0.1441, -0.5201, -0.2734, -0.2258,\n",
       "          -0.2141, -0.2207, -0.6279, -0.4738, -0.1425, -0.1426, -0.2258,\n",
       "          -0.2209, -0.1426, -0.2244,  2.7124, -0.2258, -0.1426, -0.1463,\n",
       "           2.7015,  2.1747, -0.1856, -0.1437, -0.1425, -0.1433,  1.2567,\n",
       "           2.0344]]),\n",
       " tensor([[ 0.3839,  2.5664, -0.8945, -1.0615,  0.4743,  2.4408,  0.3793,\n",
       "           1.0816, -0.2260, -0.1620, -0.2224, -0.1427, -0.1428,  0.8951,\n",
       "          -0.5634, -0.2248, -0.2261, -0.7870,  0.7380,  2.4085, -0.1441,\n",
       "          -0.1698, -1.3451, -0.1560, -0.1383, -0.3751, -0.4947, -0.4566,\n",
       "          -0.5855]]),\n",
       " tensor([[-0.3664, -0.1430, -0.1434,  1.0078, -0.5907,  0.5856, -0.1898,\n",
       "          -0.5663, -0.2893,  2.8116, -0.5983,  1.7970, -0.2133, -0.2263,\n",
       "           3.0152, -1.5378, -0.1457, -0.2236, -0.2087, -2.3159, -1.2093,\n",
       "           2.2376, -0.1534, -0.2255,  1.0047, -0.1281, -0.1604, -0.1451,\n",
       "           2.5868]]),\n",
       " tensor([[-0.1458,  0.8927, -0.2049,  1.0604, -0.7758,  0.3978, -0.6835,\n",
       "          -1.3618, -0.4923,  2.9570,  1.2306,  1.5405, -0.1543, -0.1572,\n",
       "          -0.1727, -0.2261,  0.9653, -1.5358, -0.5927,  1.3499, -0.1273,\n",
       "           0.8778,  2.5693,  1.4094,  1.6957, -0.2261, -0.5963,  0.5482,\n",
       "          -0.1429]]),\n",
       " tensor([[-0.1532, -0.1430, -0.0867, -0.5987, -0.2186,  2.2844,  2.7774,\n",
       "          -0.2243, -0.2335,  1.7860,  0.5221,  1.1721, -0.2254,  0.7436,\n",
       "          -0.2263,  3.3734, -0.1522, -0.6537, -0.2321,  3.6081, -0.1486,\n",
       "          -0.5634, -0.2445,  2.7348, -0.2199,  0.4146, -0.4379, -1.1646,\n",
       "          -0.1516]]),\n",
       " tensor([[-0.2172, -0.5885, -2.2476, -0.1410, -0.2238, -2.7208,  0.5520,\n",
       "          -0.1391,  2.1374, -0.1495, -0.2239, -0.1498, -1.3056, -0.2238,\n",
       "          -0.2239, -0.1538, -1.9616,  0.1740, -0.1407, -0.5833, -0.2270,\n",
       "          -0.1936, -0.3740,  1.8319, -0.2239, -0.0719, -0.2186, -0.1530,\n",
       "          -0.2809]]),\n",
       " tensor([[ 1.5347, -1.5809, -0.2092, -0.2227,  0.0525, -0.1396, -0.4643,\n",
       "          -0.4865, -0.1396, -0.1397, -0.1404, -0.2226,  2.6391, -0.5947,\n",
       "          -0.2227, -0.1983,  0.7375, -0.4664, -0.0211, -1.3729,  2.8873,\n",
       "          -0.2250, -0.1617, -0.1489, -0.1351, -0.0987, -0.2226, -0.2227,\n",
       "          -1.2850]]),\n",
       " tensor([[-0.1395, -0.1409, -0.2205, -0.2093, -0.1391, -0.1399,  1.3592,\n",
       "          -0.7783,  0.0980, -0.1475, -0.1394, -0.2293,  1.2494, -0.2129,\n",
       "          -0.1392, -0.2124, -0.1528,  3.2659, -0.2220, -0.1407, -0.2339,\n",
       "          -0.3050,  0.4392, -1.1248, -0.7607,  2.5143,  1.0728,  2.7290,\n",
       "          -0.1391]]),\n",
       " tensor([[-0.1390, -0.1423, -0.1397,  0.1363, -0.1397, -0.1388, -0.2188,\n",
       "          -0.3009, -0.2217, -0.2203,  1.3408, -0.3410, -0.1480, -0.5938,\n",
       "          -0.5689, -0.2287,  2.6286, -0.2218, -0.5539, -0.1466, -0.0830,\n",
       "          -0.2149,  0.5192, -0.1394, -0.2011, -0.3832, -0.1391, -0.6646,\n",
       "           2.9995]]),\n",
       " tensor([[-0.1599, -0.1388, -0.4279, -0.1906, -0.1436,  2.2484, -0.2217,\n",
       "          -0.1387,  0.4658, -0.5559, -0.1387, -0.1268, -0.2030, -0.1381,\n",
       "          -0.7695, -0.2219, -0.5116, -1.5851, -0.1953, -0.5360,  2.3674,\n",
       "          -0.5593, -0.8272,  0.3321, -0.1389, -0.9607, -0.1085, -0.0774,\n",
       "          -0.1389]]),\n",
       " tensor([[ 2.6564, -0.2210, -0.1386, -0.2873,  0.7929,  0.1419, -1.2089,\n",
       "          -0.2179, -0.2176, -0.1494, -0.0663, -0.6626, -0.1387, -0.1986,\n",
       "          -0.1386, -0.1855,  0.0520, -0.4435, -0.1744, -0.5893,  1.2411,\n",
       "          -0.2116, -0.0987,  1.4991, -0.1500, -0.1387, -0.1386,  1.4863,\n",
       "          -0.1387]]),\n",
       " tensor([[-0.3902,  1.0076, -0.1388, -0.5910,  2.2643, -0.2042, -0.1436,\n",
       "           0.1308, -0.2212, -0.1388, -0.2212, -0.2216, -0.1761, -0.1391,\n",
       "          -0.2170,  1.7755, -1.6644, -0.9189, -0.5930,  1.0627, -0.5468,\n",
       "          -0.5298, -0.4023, -0.1883, -0.1386,  0.1237,  1.5672, -2.0991,\n",
       "          -0.2101]]),\n",
       " tensor([[-0.2203, -0.1614,  2.4149, -0.0997, -0.1386, -0.1383, -0.2215,\n",
       "          -0.1386, -0.1535,  0.9696,  2.1839,  0.1163, -0.0922, -0.2344,\n",
       "           1.1717, -1.3844, -0.2211, -0.1374, -0.2193, -1.2294, -0.2220,\n",
       "           0.6121, -0.1386, -0.1407, -0.1456,  2.0403,  0.1084, -0.2214,\n",
       "          -0.2559]]),\n",
       " tensor([[-0.0017,  2.0907, -0.2215, -0.1530, -0.2888, -0.1624, -0.1386,\n",
       "           0.3739,  1.6576, -0.2019, -0.2215, -0.2214, -0.2976,  0.5805,\n",
       "          -0.1399, -0.0777, -0.6281, -0.0925, -0.1386,  0.1227, -0.5103,\n",
       "          -0.1145, -0.1386, -0.2080, -0.2050, -0.4258, -0.1387, -0.2190,\n",
       "          -0.2328]]),\n",
       " tensor([[-0.1404, -0.0707,  0.9650, -0.2036, -0.2094, -0.1452,  1.1302,\n",
       "          -0.1799,  1.2099, -0.2211,  1.0524, -0.1442, -0.6132,  2.3954,\n",
       "           0.9693, -0.5673, -0.1386, -0.1839,  1.8718, -0.2225, -0.5611,\n",
       "          -0.2416, -0.1443,  2.9651,  1.4530, -0.2869,  0.7682, -0.8876,\n",
       "          -0.1401]]),\n",
       " tensor([[-0.1386, -0.1444, -0.2310, -0.2202, -0.2216, -0.1345, -0.2901,\n",
       "          -0.4051,  0.5711, -1.3441, -0.2886, -0.2221, -0.1386, -0.4472,\n",
       "          -0.1398,  1.1357,  0.3905, -0.0679,  2.2185,  2.8619, -0.1386,\n",
       "          -0.4336, -0.1516, -0.4847, -0.1991, -0.1682,  0.5828,  1.0254,\n",
       "          -0.2207]]),\n",
       " tensor([[ 1.9171, -0.1711, -0.0410, -0.2177, -0.5902, -0.9261, -0.1708,\n",
       "          -0.5895,  0.1366, -0.1390,  0.0639, -0.2245, -0.2186, -0.1389,\n",
       "           2.0986,  0.4295, -0.2271, -0.2265, -0.0534, -0.1387, -0.2764,\n",
       "          -0.2493, -0.2603, -0.5935, -0.5455, -0.1721, -0.0404, -0.5601,\n",
       "          -0.2183]]),\n",
       " tensor([[-0.5405, -0.2214,  1.8764, -0.1671, -0.2184, -2.0656, -0.2214,\n",
       "          -0.1389, -0.2202, -0.2024, -0.2160, -0.2265, -0.2214, -0.5934,\n",
       "          -0.5923, -0.3015, -0.2104, -0.2215,  1.2973, -0.0493, -0.1108,\n",
       "          -0.1058, -0.1386, -0.2216, -0.1592, -0.1323, -0.0401,  0.0872,\n",
       "           0.4413]]),\n",
       " tensor([[ 0.1610, -1.4815, -0.2215, -0.3134, -2.2466, -0.6587, -1.9920,\n",
       "          -0.1386, -0.5887, -0.0639, -0.3899, -0.0838, -0.2214,  0.7650,\n",
       "           0.4240, -0.1386, -0.4566,  0.7182, -0.2212,  2.8437, -0.1400,\n",
       "          -0.6974, -0.3190,  1.6790,  1.7249,  2.8265, -0.0303, -0.7422,\n",
       "          -0.2102]]),\n",
       " tensor([[-0.1386, -0.1395, -0.2215,  0.2010,  1.9909,  3.1141,  0.6076,\n",
       "          -0.0378, -0.1413,  0.0233, -0.5406, -0.5506,  2.5106, -0.2836,\n",
       "          -0.2214, -0.2171,  3.2861, -0.0568, -0.1622, -0.4051,  0.8280,\n",
       "          -1.1119, -0.2104,  1.0171, -0.1736,  0.0584, -0.6723,  1.7159,\n",
       "          -0.1940]]),\n",
       " tensor([[ 1.0369,  0.4890, -0.2196, -0.1386,  3.1920,  0.1757, -1.1129,\n",
       "          -0.1878, -0.2075, -0.2216, -0.1387, -0.1384, -0.4985, -1.5888,\n",
       "          -0.3390, -0.5351, -0.5936, -0.1505,  1.6980,  0.9308,  2.7407,\n",
       "          -0.5895, -0.1386,  0.1919, -2.1932, -0.2215, -0.4647, -1.3528,\n",
       "           0.1207]]),\n",
       " tensor([[-0.1399,  3.5160, -0.1386, -0.2205, -0.5914,  3.2062, -0.2214,\n",
       "          -0.1320, -0.2640, -0.1404, -0.1817,  1.9564, -0.2215, -0.0925,\n",
       "          -0.4541, -0.4866, -0.6361, -0.2501, -1.5957, -0.1390, -0.1401,\n",
       "          -0.2215,  0.1118, -0.1591,  2.7462, -0.2213, -0.2213,  0.9280,\n",
       "          -0.5454]]),\n",
       " tensor([[-0.1456, -0.1411, -0.1386, -0.3423, -0.1864,  1.4696, -0.2173,\n",
       "          -1.4165,  2.6689,  0.1784,  2.5004,  1.3703, -0.2215, -0.3399,\n",
       "           1.6289, -0.2188, -0.6495, -0.2194, -0.4557,  1.3321,  0.0078,\n",
       "          -0.0027, -0.1836,  1.8231, -0.1812, -0.1007, -0.2535,  1.7018,\n",
       "           2.8547]]),\n",
       " tensor([[-0.1330,  0.2698,  0.2960,  2.2775, -0.1386, -0.1386, -2.1690,\n",
       "          -0.8391, -0.3129, -0.5827, -0.1693, -0.1386, -0.1386,  0.6890,\n",
       "           0.4934, -0.2081, -0.1388,  3.4040, -0.1513, -0.2198,  0.0321,\n",
       "           0.1404, -0.2270, -0.1387, -0.1215, -0.0698, -1.4510, -0.4032,\n",
       "          -0.2198]]),\n",
       " tensor([[-0.2907, -0.6145, -0.9500, -0.3568, -0.9416, -0.2214, -0.5741,\n",
       "           0.6290, -0.1391,  2.2869,  0.1102, -0.2178, -0.1396, -0.2215,\n",
       "          -0.3540, -0.1396, -0.4189,  2.5178, -0.2796, -0.1388,  1.9462,\n",
       "          -0.1485, -0.8753, -0.2194, -1.5350, -0.1441,  0.3812, -0.1386,\n",
       "          -0.5925]]),\n",
       " tensor([[-0.1510, -0.1536,  1.8929,  0.4424, -0.1387,  1.7925, -0.2210,\n",
       "          -0.9043, -0.1386, -0.1963,  2.1708, -0.2308, -0.6510, -0.3780,\n",
       "           0.8613, -0.4144, -0.1385,  0.7582, -0.7279, -0.1951, -0.2287,\n",
       "          -0.5941,  0.3981, -0.2215,  2.2177, -0.2392,  2.0753, -0.1386,\n",
       "          -0.1415]]),\n",
       " tensor([[ 0.2145, -0.1836, -0.3109, -0.1496,  0.3152, -1.0140, -0.2026,\n",
       "          -0.7079, -0.7971, -0.2202, -0.4256, -0.1387, -0.2213,  1.9487,\n",
       "          -0.1928,  0.2085, -0.0149, -0.4211,  0.9078, -0.5914, -0.2215,\n",
       "          -0.7188, -0.2215,  0.6792,  0.5722,  2.8840, -0.1386, -0.2215,\n",
       "          -1.1320]]),\n",
       " tensor([[-0.2127, -0.5880,  1.0578, -1.7774,  2.1778, -0.0006, -0.2197,\n",
       "          -0.1386,  2.0369, -1.4047, -0.2213, -0.1929, -0.1996, -0.3954,\n",
       "           0.7099,  0.1481, -0.1451, -0.2209,  3.2080, -0.2215, -0.5935,\n",
       "          -0.1584, -0.2204,  1.4010, -0.1386,  0.2068, -0.1423,  0.6841,\n",
       "          -0.2184]]),\n",
       " tensor([[-0.6231, -0.1542,  2.2988, -0.1393, -0.3166,  1.4576, -0.2704,\n",
       "           2.1254,  0.9207, -0.1386, -0.2216, -0.0893,  0.7070, -0.2203,\n",
       "          -0.2214,  0.9434, -0.2215, -0.1391, -0.2215, -0.1277,  1.0548,\n",
       "          -0.1386,  1.2966,  2.1205, -0.2044, -0.1586, -0.1410, -0.1405,\n",
       "          -0.1409]]),\n",
       " tensor([[ 1.3291, -0.1434, -0.2217, -0.2076, -0.2213,  1.7140, -0.4816,\n",
       "          -0.1027, -0.1386,  2.2760, -0.1412,  0.3923, -0.0902,  2.9589,\n",
       "           1.1588, -0.5764, -1.0139, -0.2082, -0.2205, -1.7276, -0.0377,\n",
       "           1.9757, -0.2404,  1.8397, -0.1420, -0.4921,  1.0959, -0.1388,\n",
       "          -0.1745]]),\n",
       " tensor([[-0.1807,  0.0749, -0.4316, -0.1939, -0.2489, -0.2003, -0.1386,\n",
       "          -0.1734, -0.2214, -0.5335,  2.8022,  1.0221, -0.1554, -0.5934,\n",
       "          -0.1386, -0.2195,  0.5041, -0.1386, -0.1420,  0.2475, -0.2208,\n",
       "           3.1833,  2.9756,  0.1060, -0.2720,  3.2876,  2.5536, -0.1386,\n",
       "           0.2929]]),\n",
       " tensor([[-0.2616, -0.7652, -0.1653, -0.0276, -0.9750,  0.2187,  2.1955,\n",
       "          -0.1997,  0.5662, -0.5933,  2.0167, -0.1770, -0.2121,  0.1041,\n",
       "          -0.1384,  0.6426,  1.7379, -0.1386,  1.2330, -0.5933, -0.1324,\n",
       "          -0.1385,  0.4392, -0.2215, -0.1386, -0.1546, -0.0300, -0.1444,\n",
       "           1.1818]]),\n",
       " tensor([[-0.1377,  3.4535, -0.1375,  2.9890, -0.9286, -1.1384, -0.2207,\n",
       "          -0.1377, -0.1528, -0.2990, -0.1842,  0.5782, -0.1650, -0.0325,\n",
       "           1.3230, -1.8749, -0.2183, -0.4727, -0.1423, -0.1382, -0.1543,\n",
       "          -0.2169, -0.2684,  0.0177, -0.5536, -0.1377,  2.6131,  2.9499,\n",
       "          -0.1883]]),\n",
       " tensor([[-1.1662,  1.7424,  2.5848, -0.1373, -0.1221,  2.6402,  0.5027,\n",
       "          -0.2200, -0.1569,  0.0521,  2.5787, -0.2084,  0.2715, -0.4751,\n",
       "          -0.1378,  2.4313, -0.0855,  1.2014,  0.3748, -1.3995, -0.1374,\n",
       "          -0.2084,  2.4544, -1.5912,  2.4789,  0.7285,  0.3248, -0.1373,\n",
       "          -0.4332]]),\n",
       " tensor([[-0.9492, -0.1380, -0.1837,  1.4997, -0.2201, -0.5913, -0.1529,\n",
       "           0.0241,  1.0384, -0.5735,  3.0577, -0.1374, -0.1437, -0.1841,\n",
       "           0.1468,  2.6657, -0.1904, -0.1998,  1.6201,  2.3837, -0.1879,\n",
       "           1.7873, -0.2200, -0.2102, -0.1371,  1.8878, -0.1371, -0.1024,\n",
       "           3.2909]]),\n",
       " tensor([[-0.1370,  1.6576, -0.1409, -0.1997, -0.1370, -0.3457, -0.5909,\n",
       "          -0.2170, -0.1370, -0.5119,  2.2872,  0.0478, -0.1370, -0.1375,\n",
       "          -0.2115, -0.0918, -0.5086, -1.0813, -0.1413,  2.2783, -0.2197,\n",
       "          -0.1372, -0.6964,  2.8656, -0.1372, -0.2200, -0.1370, -0.2116,\n",
       "           0.2103]]),\n",
       " tensor([[-0.2276, -1.0674, -0.1370, -0.2825,  0.4123, -0.2620, -0.2179,\n",
       "          -0.1434, -0.6582,  2.3094, -0.2139, -0.1178, -0.1803, -0.1986,\n",
       "          -0.1370, -0.4305,  2.7894, -0.2190,  2.6098, -0.0251, -0.1566,\n",
       "           0.2508, -0.0681, -0.2026,  2.3075, -0.4692, -0.3301, -0.1371,\n",
       "          -0.1303]]),\n",
       " tensor([[-1.1210,  1.0791, -0.3278,  0.8383,  0.2602, -0.1615, -0.1385,\n",
       "           1.8112, -0.2199,  3.2258, -0.5412, -0.1464, -0.1380, -0.1867,\n",
       "          -0.2198, -0.2199, -0.5914, -0.1561, -0.4419, -0.1388, -0.1393,\n",
       "           0.2576,  0.0850,  1.9512, -0.1435, -0.4000, -0.5699, -0.1378,\n",
       "          -0.1369]]),\n",
       " tensor([[-0.1369,  0.2104, -0.1330, -2.1310, -0.1794,  2.1830, -0.4708,\n",
       "          -0.2199, -0.2200, -0.1369,  1.5566,  2.2282, -0.2601, -0.2924,\n",
       "          -0.2163, -0.1503, -0.1369,  0.6063,  0.2271,  2.0827, -0.2611,\n",
       "           0.3799, -0.2200, -0.1426, -0.1369, -0.2094, -0.1376,  1.7427,\n",
       "          -0.0609]]),\n",
       " tensor([[-0.5909,  0.8645, -0.1378, -0.1393, -0.7170, -0.2198, -0.1371,\n",
       "          -0.1369, -0.3246, -0.1369, -0.5044, -1.3352, -0.1367, -2.3096,\n",
       "          -0.1369, -0.5749, -1.6264,  2.3676, -0.5531, -0.5021,  2.3361,\n",
       "           2.7708, -0.1369,  2.3404, -0.2191, -0.5566,  2.2051, -0.8849,\n",
       "          -0.4038]]),\n",
       " tensor([[ 0.1151,  0.1765, -0.2029, -0.0296,  0.1776, -1.2404, -0.1381,\n",
       "          -0.1396, -0.1384, -0.2168, -0.2174, -1.5806, -0.3440,  0.1304,\n",
       "          -0.1433, -0.6839, -0.6876, -0.6441, -1.2001, -0.1385, -0.1581,\n",
       "          -0.1369, -0.1543, -0.1461, -0.1379, -0.2199, -0.2387, -0.1790,\n",
       "          -0.1371]]),\n",
       " tensor([[-0.4696, -0.8220, -0.1380,  0.1485,  0.4569, -0.6948,  0.3022,\n",
       "          -0.1834,  1.9227, -0.1455, -0.0771, -0.1375, -0.1369,  2.3056,\n",
       "          -0.5844, -0.4008,  2.2558, -0.1828, -0.5027, -0.1369, -0.4762,\n",
       "          -0.1436,  0.1144, -1.0553,  0.5256,  0.1562,  0.0887, -0.2171,\n",
       "           2.5541]]),\n",
       " tensor([[-0.2199,  0.1028,  0.2551, -0.5321, -0.1369, -0.1531, -0.2116,\n",
       "          -0.1370, -0.2116, -0.1377, -0.1662, -0.1369, -0.6050, -0.5909,\n",
       "          -0.1612, -0.1369, -0.1369, -0.1369,  1.9157, -0.4986, -0.5048,\n",
       "          -0.1697, -0.1651, -0.2361, -0.0746, -0.1429,  1.4764, -0.2369,\n",
       "          -0.1516]]),\n",
       " tensor([[ 1.2188, -0.2749, -0.2194,  2.0869, -1.0353,  0.2392, -0.2196,\n",
       "          -1.4246, -0.1369,  1.0681, -0.1491,  2.9874, -0.5966,  1.1496,\n",
       "          -0.4653,  0.0323,  3.3462,  0.2199, -0.5940, -0.1369, -1.3589,\n",
       "           1.5218, -0.1369, -0.5909, -0.1949, -0.2060, -0.7028,  2.5487,\n",
       "          -0.4113]]),\n",
       " tensor([[-0.2438, -0.2201, -0.0605, -0.2199, -0.1370, -0.1401, -0.6324,\n",
       "          -0.2199, -0.2314,  0.4598, -0.2160, -1.9532, -0.2335, -0.5299,\n",
       "           0.7151, -1.4259,  0.7818, -0.3904, -0.2185,  0.8145, -1.6305,\n",
       "           2.4698, -0.2821, -0.2070, -0.1491, -0.1370, -0.3312, -0.1370,\n",
       "           1.2744]]),\n",
       " tensor([[-0.1377, -0.2052, -0.4400, -0.4165,  0.0760, -0.1423, -0.1567,\n",
       "           1.2428, -0.1779, -0.5719, -0.5734, -0.2184, -0.2178, -0.1663,\n",
       "          -0.1371, -0.5990, -0.1466,  0.1234, -0.1953, -0.1373,  2.4031,\n",
       "           2.4119, -0.5781, -1.2613, -0.3212, -0.5144, -0.5995,  2.8924,\n",
       "          -0.1372]]),\n",
       " tensor([[-0.2117,  0.0425,  1.3657, -0.1547,  2.3312, -0.4791,  0.5522,\n",
       "           0.6074, -0.1388, -0.1370, -0.2232,  0.2735, -0.2198, -0.1370,\n",
       "          -0.6369, -0.2199, -0.1375, -0.1521, -0.7662, -0.7100, -0.2200,\n",
       "          -0.1693,  1.5455, -1.2292,  1.0157,  2.4747, -0.2013, -0.0303,\n",
       "          -0.1415]]),\n",
       " tensor([[-0.1379, -0.2077,  0.4827, -0.3288, -0.2134, -1.7304,  2.6364,\n",
       "          -0.2197, -0.2232,  3.3611, -0.2192, -0.1421, -0.1368, -0.6570,\n",
       "          -0.2200,  1.1612, -0.1913, -0.1794, -0.1370,  2.3722,  2.3921,\n",
       "          -0.1921,  3.4479,  0.8446, -1.1567, -0.0368, -0.5918, -0.2010,\n",
       "           1.7596]]),\n",
       " tensor([[-0.2192, -0.1840, -1.8335, -0.5351,  2.7363, -0.2048, -1.3094,\n",
       "          -0.1061,  1.1251, -0.1886,  2.3803, -0.2197, -0.1592, -0.2407,\n",
       "          -0.1887,  3.3797, -0.3391, -1.0961,  0.5618, -0.1803, -0.1612,\n",
       "          -0.4147,  0.6895,  2.1931,  1.1115, -1.0541, -0.0856, -0.1372,\n",
       "          -0.1739]]),\n",
       " tensor([[-0.2200,  0.3723, -0.2132, -1.0200, -0.9627, -0.7002,  0.4206,\n",
       "          -0.2531,  2.5101,  0.4807, -0.2191,  0.9716, -0.0693, -0.2197,\n",
       "          -0.1621, -0.1794, -0.2196,  0.4556, -0.5697,  0.3884, -0.1376,\n",
       "           2.0857, -0.2088,  2.1317, -0.2047, -0.1467, -0.1398,  2.7905,\n",
       "          -0.2504]]),\n",
       " tensor([[-0.2205,  0.8262, -0.1320, -0.2159, -0.1409, -0.5781, -1.5641,\n",
       "          -0.1370, -0.2198, -0.1373, -0.2201, -0.1370,  1.1269, -0.1430,\n",
       "          -0.2120,  2.6155, -0.1371, -0.2204, -0.1383,  1.9087, -0.0953,\n",
       "           0.3402, -0.4584,  0.4055, -0.1503,  1.9608, -0.1371,  1.1594,\n",
       "          -0.1386]]),\n",
       " tensor([[-0.1386, -0.4341,  1.2783, -0.0104,  2.5370, -0.3357, -0.1372,\n",
       "          -0.6074,  2.0919, -0.1385,  1.0455,  1.6680, -0.1375,  0.5930,\n",
       "          -0.1955,  1.1534, -0.7452, -0.1371,  0.0063, -0.5911,  0.1265,\n",
       "          -0.2207,  2.3181, -0.5031, -0.1013, -0.6859, -0.1752,  2.7658,\n",
       "          -0.1432]]),\n",
       " tensor([[-0.2201, -0.1370, -0.2200, -1.6837,  1.0208,  1.2754,  1.5446,\n",
       "          -0.7567, -0.1371, -0.2104, -0.2211,  2.3574, -0.1370,  0.4049,\n",
       "          -0.2142, -0.1370, -1.8752, -0.2200, -0.1828, -0.1372, -0.2181,\n",
       "           1.3317, -0.1392, -0.2215, -0.2200,  1.2166, -0.1370,  0.0639,\n",
       "           2.9836]]),\n",
       " tensor([[-0.5860, -0.4091, -0.1376, -0.1374, -0.2201, -0.1589, -0.2090,\n",
       "          -0.4613, -0.1831, -0.1373,  0.2612, -0.4085, -0.1384, -0.1421,\n",
       "          -0.2491, -1.3837, -0.5187, -0.2223, -0.3606, -0.2127,  1.4041,\n",
       "          -0.5900, -0.1432, -0.1371,  1.1796, -0.1385, -0.1370, -0.5902,\n",
       "           1.3990]]),\n",
       " tensor([[-0.1650, -0.1578,  1.4523, -0.1962, -0.2796, -0.2201, -0.2102,\n",
       "           1.6624, -0.2116, -0.1775, -0.5953, -0.2197, -0.1682,  0.3145,\n",
       "          -0.1552, -0.4417, -0.2188, -0.1889,  2.8707, -0.1371, -0.1373,\n",
       "          -0.1370, -0.2200, -0.5655, -0.1382, -0.2178, -0.5780, -0.2201,\n",
       "          -0.2141]]),\n",
       " tensor([[-0.5741, -0.1624, -0.2040, -0.1479, -0.1583, -0.5910, -0.2197,\n",
       "          -0.1428, -0.9188, -1.3120, -0.2033, -0.2007,  2.2764, -0.2183,\n",
       "          -0.5897, -0.1385, -0.4802, -0.1708,  0.3851,  0.9714, -0.1816,\n",
       "          -0.5878, -0.2197,  2.9597, -0.1370, -0.1729, -0.9702,  1.6518,\n",
       "           0.2188]]),\n",
       " tensor([[-0.6943, -1.4710,  2.3438, -0.1546, -0.1434,  0.7913, -0.1944,\n",
       "          -0.1370,  1.7172, -0.1370, -0.2157,  2.1116, -0.4648, -0.1395,\n",
       "           3.1443,  0.7652,  1.1311,  1.7357, -0.5059, -0.2200, -0.4121,\n",
       "          -0.2201,  0.3237,  2.3737, -1.4408,  0.6123,  2.2958,  1.0793,\n",
       "           2.8534]]),\n",
       " tensor([[ 1.0418,  3.0534, -0.1430, -0.2352, -0.1426, -0.4441,  1.5086,\n",
       "          -0.2211, -0.1855, -0.4219, -0.1395, -0.1371, -0.9511, -0.5248,\n",
       "          -0.1286,  1.9782, -0.1633, -0.1380, -0.3766, -0.1781, -0.2199,\n",
       "          -0.2201, -0.1193, -0.1391, -0.1382, -0.1372, -0.4855,  2.6458,\n",
       "          -0.1670]]),\n",
       " tensor([[-0.3902, -0.1409, -0.5303,  2.9018, -0.1415, -0.3178, -0.1371,\n",
       "          -0.2408, -0.2192, -0.1360,  1.6777, -0.1719,  2.1586, -0.1371,\n",
       "          -0.2054, -0.2201,  1.3634, -0.2157, -0.5809, -1.0206,  1.4121,\n",
       "           0.0658,  0.3548, -0.1566,  1.2255, -0.2201, -0.1578, -0.1370,\n",
       "          -0.2182]]),\n",
       " tensor([[ 0.4194,  1.7169, -0.1372,  1.7141, -0.2119, -0.1855, -0.1394,\n",
       "          -0.1397, -1.1091, -0.2199,  0.6060, -0.1371, -0.2188, -0.1373,\n",
       "          -0.1371, -0.0981,  1.9949, -1.6025, -0.3869, -0.1850,  2.9497,\n",
       "          -0.9806, -0.2169, -1.3490, -0.4862, -0.1075, -0.5856, -0.1263,\n",
       "          -0.5809]]),\n",
       " tensor([[-0.1279, -0.1386, -0.2196, -0.2154,  2.2307, -0.9256, -0.1371,\n",
       "          -0.2198,  1.0603,  2.2180, -0.2195, -0.1406, -0.1311, -0.1336,\n",
       "           1.4684, -0.4439, -1.0054, -0.4545, -0.2084, -0.1371,  0.5353,\n",
       "          -0.2193,  0.1018,  0.1138, -0.1567, -0.1902, -0.1451,  1.0907,\n",
       "          -0.2192]]),\n",
       " tensor([[-0.1810, -0.2199,  0.4470, -0.1676, -0.2174, -0.2132, -0.1471,\n",
       "          -0.2115,  2.3493, -0.1418, -0.2172, -0.1287, -0.2201,  1.9475,\n",
       "          -0.6537, -0.1145, -0.1580,  2.0084, -0.7772, -0.2783, -0.1371,\n",
       "          -1.3172,  2.1689, -0.5385, -0.1413,  0.3575, -0.4054, -0.2200,\n",
       "           2.4996]]),\n",
       " tensor([[-0.2201,  1.4743, -0.1373, -0.4550, -0.5677, -0.2193, -0.2401,\n",
       "          -0.1381, -0.1373, -0.5881,  0.6990, -0.6992, -0.1131, -0.5328,\n",
       "          -0.1371,  2.2248,  1.1546, -0.2199,  0.9628,  1.7613, -0.5832,\n",
       "          -0.2201, -0.1371,  2.4017, -0.5911, -0.2200, -0.1375, -0.2203,\n",
       "          -0.3221]]),\n",
       " tensor([[-0.1371, -0.1190, -0.1837, -0.1371, -0.5293, -0.1371, -0.1371,\n",
       "          -2.0268,  2.7855,  0.8254, -0.5861, -0.2357, -0.1401,  1.3003,\n",
       "          -0.2575,  0.3937, -0.2496, -0.1615, -0.2201, -0.1374, -0.2202,\n",
       "          -0.2135, -0.1371,  0.2480, -0.2146, -0.1652, -0.4665, -1.2280,\n",
       "          -0.2194]]),\n",
       " tensor([[-0.2368,  2.2250, -0.1433, -0.1371,  2.1453, -0.1371, -0.4956,\n",
       "           2.2155, -0.2221, -0.1509, -0.6595, -0.1416,  2.0233, -0.1334,\n",
       "          -1.1448,  2.6505, -0.5564, -0.2201, -0.1546,  2.1490,  2.5936,\n",
       "           0.6959, -0.7290, -0.1456, -0.1448,  1.3025, -0.3755, -0.1372,\n",
       "           0.2141]]),\n",
       " tensor([[-0.1371, -0.1373, -0.4232, -0.2192,  2.6633,  0.8055,  0.1837,\n",
       "          -0.2003, -0.0845, -0.2201, -0.2434, -0.6514, -0.1385, -0.2190,\n",
       "          -0.5833,  1.6381, -0.1898, -1.7775,  1.5524, -0.5477, -0.2043,\n",
       "          -0.4081, -0.1897, -0.1370,  0.2851,  0.2389, -0.5731, -0.1371,\n",
       "          -0.2196]]),\n",
       " tensor([[ 0.6632, -0.4413, -0.1397,  2.7443, -0.1371, -1.1729, -0.6720,\n",
       "          -0.2936, -0.0510, -0.5908, -0.2135,  1.8114, -0.0243, -0.1371,\n",
       "          -0.2370, -0.9617, -0.1371, -0.1375, -1.4071, -1.1785,  2.6371,\n",
       "           2.0415, -0.1508, -0.1372, -0.5110, -1.6106, -0.1371, -0.1373,\n",
       "          -0.1373]]),\n",
       " tensor([[-0.1371,  2.9933, -0.1376, -0.1341, -0.2469, -0.3399, -0.2199,\n",
       "          -0.2076, -0.5230, -0.2185, -0.1371, -0.1446,  2.2249, -0.9940,\n",
       "          -0.2556, -0.7263,  1.5988, -0.2201, -0.4378, -0.5860,  2.4157,\n",
       "          -0.5908, -0.1549, -0.3008, -0.4164, -0.1400, -0.2201, -0.5860,\n",
       "          -0.1391]]),\n",
       " tensor([[-0.2201, -0.9058,  0.1384, -0.2201, -0.2374,  1.0306, -0.1522,\n",
       "          -0.2094,  1.2564, -0.5913,  2.5070, -2.3511,  1.9116, -0.2199,\n",
       "          -0.5613,  0.0803,  2.6229, -0.1425, -0.6699, -1.1420, -0.5869,\n",
       "          -0.1386, -0.2197, -0.3638,  2.5916,  2.3318, -0.1671, -0.5463,\n",
       "          -0.8897]]),\n",
       " tensor([[ 0.9535,  0.4597, -0.1373, -0.1379, -0.1952,  2.8697, -0.1366,\n",
       "          -1.2902, -0.1290, -0.1371, -0.7536, -0.5595, -0.2115,  2.2404,\n",
       "          -0.2199, -0.1357, -0.1462, -0.0549, -0.2527, -0.2228, -0.2171,\n",
       "          -0.2166, -0.2124,  2.7725, -0.1531,  2.1581, -0.1371, -0.0525,\n",
       "          -0.2191]]),\n",
       " tensor([[ 0.2784, -0.9762, -0.1371,  2.7514, -0.1570, -0.5214, -0.5583,\n",
       "          -0.1825, -0.1371,  0.2295, -0.1379, -0.1941, -0.2188, -0.3809,\n",
       "          -0.1371, -0.1522, -0.3301, -0.1466, -0.1372, -0.1721, -0.2162,\n",
       "           1.6518, -0.3234, -0.5494, -0.1417,  0.5049, -0.1371,  2.3031,\n",
       "           0.9182]]),\n",
       " tensor([[-0.1371, -0.1717, -0.5803, -0.3531, -0.1448, -0.1260, -0.1379,\n",
       "          -0.1371, -0.2139, -0.5886,  2.7625, -0.1385, -0.0433,  0.9513,\n",
       "          -0.2201,  2.2827,  1.6739,  1.8727, -0.1371, -0.2951,  2.3395,\n",
       "           0.3415, -0.2197, -0.2201,  0.0147, -0.1745,  2.3984,  0.0195,\n",
       "           2.3627]]),\n",
       " tensor([[-1.8146, -0.4230, -0.7616,  2.1619, -0.1595, -0.1371,  0.4206,\n",
       "          -0.1394, -0.2201, -0.2722, -0.1471, -0.1372, -0.2195, -0.3323,\n",
       "          -0.1400, -0.3639, -0.2116,  3.2286,  0.5954, -0.3102, -0.9859,\n",
       "          -0.1371, -0.2189, -0.5158,  0.7462, -0.0350, -0.2056, -0.1371,\n",
       "          -0.5909]]),\n",
       " tensor([[-0.2200, -0.1371, -0.1082, -0.1371, -0.5692,  0.5491,  0.5814,\n",
       "           2.3489,  0.7144, -0.1371, -1.5628, -0.2194, -1.7595, -0.2201,\n",
       "           1.5556, -0.4063, -0.2206, -0.1373, -0.3561,  2.1721, -0.1371,\n",
       "          -0.2201,  0.3095, -0.9665,  0.1516,  2.6357, -0.2201,  2.5321,\n",
       "           1.7308]]),\n",
       " tensor([[-0.1379,  2.7206, -0.1371, -0.5341,  0.6027, -0.2200, -0.1415,\n",
       "          -0.5933,  2.5277,  0.2634,  1.4271, -0.1499,  0.0302, -0.1747,\n",
       "           1.7672, -0.1437, -0.4334, -0.2192, -0.1389, -1.0391,  2.8543,\n",
       "          -0.2623, -0.1371, -0.2143, -0.1371, -0.6286, -0.5120,  2.2716,\n",
       "           3.1833]]),\n",
       " tensor([[ 2.1946,  2.2565, -0.1376, -1.2489,  2.9988,  0.0187, -0.2133,\n",
       "           1.4611, -0.1953, -1.1761,  1.6668, -0.2197, -0.2798, -0.5419,\n",
       "          -1.7785,  1.8155, -0.1382, -0.2093, -0.2201, -0.1372, -0.1401,\n",
       "          -1.0506,  1.6996, -0.5902, -0.0993,  0.2800, -0.3998, -0.1060,\n",
       "          -0.1797]]),\n",
       " tensor([[-0.2080, -0.1371, -0.1372,  1.5383, -0.8664, -0.2200, -0.1824,\n",
       "           0.5269, -0.1567, -0.8521, -0.3913, -0.2322,  0.4599, -0.2194,\n",
       "          -1.9440, -0.1372, -0.1371, -0.1493, -0.2194, -0.1826, -0.6391,\n",
       "          -0.1405,  0.6434, -0.1437,  2.0606,  0.1105, -0.1371, -0.2201,\n",
       "          -0.5876]]),\n",
       " tensor([[-0.8005, -0.1371, -0.1371,  0.3597, -0.2200, -0.1371, -0.5666,\n",
       "          -0.1371, -0.1761,  0.1794, -0.1371, -0.1421, -0.6796,  3.2266,\n",
       "          -0.0196, -0.1376,  1.1116, -0.1430, -0.1849, -0.2314, -0.0860,\n",
       "          -0.1878, -0.2201, -0.2200, -0.2600, -0.2192,  2.5264,  2.5988,\n",
       "           1.6763]]),\n",
       " tensor([[-0.1446, -0.5891, -0.1216,  0.8176, -0.1372, -0.1371,  0.0236,\n",
       "          -0.1438, -0.2455, -0.1144, -0.8601, -0.5716, -0.2137,  0.8480,\n",
       "          -0.3730, -0.2376, -0.5915, -0.3830, -0.2201,  2.3628,  0.1627,\n",
       "          -0.2198, -0.5078, -0.1371, -1.3152, -0.1371, -0.2201, -0.3515,\n",
       "           2.5731]]),\n",
       " tensor([[-0.1439, -0.2200, -0.5906, -0.0362, -0.0878,  1.1768, -1.2226,\n",
       "          -0.2390,  1.4223, -0.1818, -0.3029,  0.6485, -0.2201, -0.2201,\n",
       "          -0.6586, -0.1642,  3.1715, -0.1883, -0.2231,  2.4781, -0.1382,\n",
       "          -0.1894,  0.5070, -0.5519, -0.2201, -0.1931, -0.7265, -0.1507,\n",
       "          -0.1374]]),\n",
       " tensor([[-0.1433, -0.2748, -0.1415, -0.4781, -0.1431, -0.1418,  3.1620,\n",
       "           0.7394, -0.0921, -0.1417, -0.5769, -0.1991, -0.1406, -0.2487,\n",
       "          -0.2355, -0.5773,  2.1250, -0.7108, -0.1498, -0.5521, -0.1415,\n",
       "           0.1077,  0.5766, -0.1431, -0.1417, -0.1415,  1.6145, -0.3278,\n",
       "          -0.1687]]),\n",
       " tensor([[-0.2269, -0.1438, -0.2268, -0.0825, -0.2020,  3.2983, -0.1512,\n",
       "          -0.5947, -0.1437, -0.2241, -0.1450,  2.5740, -0.4929,  1.0486,\n",
       "           1.4168, -0.1578, -0.1437, -0.2266,  1.3784, -0.1532,  1.9899,\n",
       "           3.3093, -0.2841, -0.2198, -0.1437, -0.1437,  2.9793, -0.2268,\n",
       "          -0.1038]]),\n",
       " tensor([[ 2.8522,  3.3393,  2.4365,  2.6217, -0.2025, -0.1426,  0.3552,\n",
       "          -0.6488,  1.5606,  1.4932, -0.1552, -0.2037, -0.1870, -0.5644,\n",
       "          -0.1660, -0.5970,  2.2641,  0.7635, -0.2214, -0.1997, -0.1426,\n",
       "          -0.5734,  2.1936, -0.7568, -0.2463, -0.1476,  2.9527,  0.8643,\n",
       "          -0.1567]]),\n",
       " tensor([[-0.1463,  0.2615, -0.1421, -0.7005,  0.4250, -0.2252, -0.4718,\n",
       "          -0.1421, -0.3756, -0.1505, -1.9665, -0.5439,  0.4367, -0.6778,\n",
       "          -2.0873, -0.2296, -0.1063, -0.1420,  2.3252,  0.9903, -0.3111,\n",
       "          -0.9391, -0.2531,  2.0670, -0.1764, -1.3850,  2.2620, -0.1442,\n",
       "          -0.1813]]),\n",
       " tensor([[ 3.2111, -0.1417, -0.0082, -0.4186, -0.3569, -1.1174,  1.5168,\n",
       "          -0.4537, -0.5849,  2.4299, -1.5705, -0.4925,  2.2614, -0.3708,\n",
       "           0.5783, -0.1417, -0.1417, -0.2362, -0.2240, -0.2247, -0.1532,\n",
       "           3.4222,  1.6263, -0.2059, -0.5679, -0.2245, -0.2247, -1.4902,\n",
       "          -0.2060]]),\n",
       " tensor([[ 0.4133, -0.5866,  0.8561, -0.4922, -0.1419, -0.2236,  0.3151,\n",
       "          -0.1437,  1.9293, -0.5671,  2.2681,  2.3648,  2.8232,  0.8015,\n",
       "          -1.0021, -0.5970,  0.0488, -0.2160, -0.6032,  0.2117, -0.1864,\n",
       "           2.3577, -0.1407, -0.1416, -0.2284, -0.1467, -0.1455, -0.2239,\n",
       "           0.2455]]),\n",
       " tensor([[-0.1415,  2.2450, -0.2245,  0.5608, -0.2244, -1.3783, -0.2245,\n",
       "          -1.5494, -0.3591, -0.1415,  2.6896, -0.3151,  0.7792,  0.7896,\n",
       "           1.5462, -0.1975,  0.8732, -0.2245,  0.1141, -0.1483,  1.3889,\n",
       "          -0.3220,  0.7801,  2.7973, -0.2236, -0.2044, -0.2246, -0.5582,\n",
       "          -0.2793]]),\n",
       " tensor([[ 0.2071, -0.1976, -0.1408, -0.2073, -0.3019,  2.7923, -0.1633,\n",
       "          -0.1415, -1.3561, -0.1468, -0.1415, -0.2045, -0.1544,  0.1811,\n",
       "          -0.9629, -0.1279, -0.1580, -0.1425, -0.1417, -0.1426, -0.0032,\n",
       "          -0.1414, -0.5966,  2.5920,  0.4896, -0.1517, -0.5057, -0.0877,\n",
       "          -0.1900]]),\n",
       " tensor([[-0.1415,  2.2374, -0.2244, -0.5120,  1.9705,  2.2303,  0.8019,\n",
       "          -0.4041, -0.4396,  2.5843,  0.6068,  2.4849, -1.8054, -0.5545,\n",
       "          -0.7601,  1.1002,  0.2114, -0.5387, -0.1317, -0.1475, -0.2239,\n",
       "           0.7641, -0.1414, -0.1110,  0.9972,  0.8069, -0.5862, -0.2245,\n",
       "           2.8915]]),\n",
       " tensor([[-0.4517, -0.4236,  2.5648, -0.1382, -0.1767, -0.2237, -0.2128,\n",
       "           0.2662, -0.2863,  1.8765, -0.2052, -0.0526,  0.1267, -0.2309,\n",
       "           2.5833, -0.1391,  0.4779,  0.1521,  0.7486, -0.2291,  1.9419,\n",
       "           2.7335, -0.2223, -0.2183, -0.1415, -0.2244, -0.5968,  1.9081,\n",
       "          -0.2221]]),\n",
       " tensor([[ 0.1546, -0.1414, -0.1513,  1.4685, -0.5861, -0.0426,  2.2678,\n",
       "          -0.1480, -0.2226,  2.9494, -0.2122, -0.1419, -0.6680, -0.1414,\n",
       "          -0.1414, -0.2526, -0.1383, -0.1582, -0.5968,  2.2669, -0.2074,\n",
       "           0.6966, -0.1415, -0.2244, -0.5602, -0.1686,  0.5829, -0.5649,\n",
       "          -0.2211]]),\n",
       " tensor([[ 3.4448,  0.3670, -1.1021,  0.5503,  2.0682, -0.4392,  1.6871,\n",
       "          -0.1701,  2.4184,  1.7481, -0.3276, -0.3106,  2.2210,  2.2698,\n",
       "          -0.2220, -0.1962, -0.1414, -0.1631, -0.1575, -0.2491, -0.5322,\n",
       "          -0.1419, -0.2432, -1.6254,  0.7570,  1.8567, -0.1132, -0.1646,\n",
       "          -0.2226]]),\n",
       " tensor([[-0.1000,  2.8884, -0.1641, -0.2228,  1.1267, -0.2132,  0.8562,\n",
       "          -0.1414,  0.2379, -0.5728, -0.1554, -0.6329, -0.1594, -0.4758,\n",
       "          -0.1414, -0.0547,  1.8256, -0.3109,  2.2458, -0.1423,  0.1113,\n",
       "          -0.1787, -0.6371,  1.6630,  2.8049,  2.6736, -1.0122, -0.5721,\n",
       "          -0.5634]]),\n",
       " tensor([[-0.5896, -0.1414, -0.1415, -0.1414, -0.1414,  1.0539,  0.7463,\n",
       "          -0.1512, -0.6206, -0.2243,  0.4910,  2.5979, -0.2177, -0.2075,\n",
       "          -0.0454, -0.0224, -0.1414, -0.1414, -0.5683,  2.7047, -0.2160,\n",
       "          -0.2248, -0.0919,  2.6364,  0.6851, -0.7048,  2.8222, -0.2240,\n",
       "          -0.1415]]),\n",
       " tensor([[-0.2945, -0.0536, -0.1415, -0.1414, -1.7989, -0.1414, -0.1878,\n",
       "           0.2161,  0.3810,  2.0611, -0.1780,  2.9015, -0.3336, -0.5684,\n",
       "          -1.1152, -0.1348, -0.2247,  1.3906, -0.2207,  0.2123, -0.1414,\n",
       "          -0.4933,  3.0130, -0.2240,  0.6936,  2.4404,  0.0041, -0.5459,\n",
       "           1.8009]]),\n",
       " tensor([[-0.1421, -0.2244,  0.6467, -0.2400, -0.1660, -0.2249, -0.5969,\n",
       "           1.3218,  1.0321, -0.1414,  1.1135, -0.2207, -0.5580, -0.2236,\n",
       "          -0.1438, -0.2243, -0.2397, -0.1915,  2.1060,  0.2619, -0.0141,\n",
       "          -0.2242, -0.5979, -0.2028, -0.1413,  2.7710, -0.6027,  1.4538,\n",
       "           1.6839]]),\n",
       " tensor([[-0.3866, -0.2752, -0.0013, -0.2146, -0.1851, -0.1414, -0.2244,\n",
       "          -0.1468, -0.1414, -1.4551, -0.0097, -0.1545, -0.1402, -0.2247,\n",
       "          -0.2252, -0.1533, -0.1566,  0.5691, -0.3706,  0.7925, -0.2263,\n",
       "          -0.5675,  0.3498, -0.1418, -0.1550,  2.3049, -0.1472, -0.1414,\n",
       "          -0.1417]]),\n",
       " tensor([[-0.2134, -0.2202, -0.1414, -0.5331, -0.4498, -0.2113, -0.1906,\n",
       "          -0.2019, -0.1477,  2.2970, -0.1673, -0.5843,  1.6902, -0.2732,\n",
       "          -0.2106, -0.2498, -0.1414, -0.5159, -0.2069,  0.5550,  0.5097,\n",
       "           2.2670,  0.1135,  0.2654,  2.9441,  1.1359, -0.1421,  2.9606,\n",
       "          -0.2244]]),\n",
       " tensor([[-0.1431, -0.2226, -0.3879, -0.2457, -0.1954, -0.5779, -0.1414,\n",
       "          -0.5709, -0.5516, -0.2244, -0.2008, -0.2620, -0.2242, -0.5571,\n",
       "          -0.3186,  2.4553,  0.8603,  0.2554, -0.3438, -0.1433, -2.1349,\n",
       "          -0.1445,  0.2236, -0.1768,  0.5268, -1.4318, -0.1414,  0.5332,\n",
       "          -0.2216]]),\n",
       " tensor([[-0.0548, -0.1947, -0.0825, -0.2330,  2.9159, -0.2959, -0.3882,\n",
       "          -0.8189,  2.4018, -0.2460, -0.1610, -1.2808, -0.5818, -0.2114,\n",
       "          -0.7735, -0.0889,  2.3785,  2.6829,  0.0658,  0.3632, -0.2230,\n",
       "           2.1619, -0.1414, -0.1414, -0.2242, -0.2983, -0.1415,  1.5859,\n",
       "          -0.2244]]),\n",
       " tensor([[-0.5887, -0.1438, -0.1415, -0.2165, -0.1414,  0.0502, -1.3168,\n",
       "          -0.7948,  1.4332, -0.2244, -0.1919, -0.3975, -0.2210,  0.4179,\n",
       "           0.2030, -0.1414, -0.7963, -0.7740, -0.1584, -0.4671, -0.2244,\n",
       "           0.5400,  1.5384, -1.1847,  0.8042, -0.1419, -0.7175,  2.8275,\n",
       "          -0.2243]]),\n",
       " tensor([[ 1.9956,  2.5211, -0.2245,  1.9964, -0.1483, -0.1783, -0.1414,\n",
       "          -0.2244, -0.1414, -0.1417, -0.7600,  1.8239,  3.1956,  1.6319,\n",
       "          -0.1900,  2.6144, -0.5965, -0.4283, -0.0743, -0.2381, -0.1539,\n",
       "          -0.5139,  0.1332, -0.5862, -0.2241,  2.2822, -0.1435, -0.7464,\n",
       "          -0.9834]]),\n",
       " tensor([[-0.7044, -1.1555, -0.1179, -0.5257, -0.1451,  2.4296,  2.2926,\n",
       "           0.6807,  2.9369,  1.7757, -0.1418, -1.4129, -1.1278,  0.9214,\n",
       "          -0.3306,  0.2977,  2.3498, -0.2310, -0.1414, -0.1974, -0.9582,\n",
       "          -0.1724, -0.1419,  1.7593, -0.2127,  2.5323, -0.5150, -0.4185,\n",
       "          -0.1414]]),\n",
       " tensor([[-0.2244, -0.1125,  2.4299, -0.4752, -0.2245, -0.2214, -0.5459,\n",
       "          -0.7059, -0.1414,  2.4991, -0.2209, -0.2139, -0.1540, -0.1927,\n",
       "          -0.2395, -0.0431, -0.4748, -0.2232, -0.1698, -0.5611, -0.2583,\n",
       "           0.8703, -0.1462, -0.5091, -0.6099,  2.3025, -0.0748, -0.2177,\n",
       "          -0.2428]]),\n",
       " tensor([[-0.4917,  0.1954,  0.3610,  0.1850,  0.5596, -0.2239,  0.2189,\n",
       "          -0.2244, -0.6272, -0.5091, -0.1635, -0.6106,  2.0746,  0.1959,\n",
       "          -0.5854,  0.0843, -0.1416, -0.6016, -0.5603,  2.3656, -0.5971,\n",
       "          -0.1429,  1.0347, -0.2088,  0.1735, -0.1659,  0.6619, -0.2589,\n",
       "          -0.2240]]),\n",
       " tensor([[ 2.9515, -0.1412, -0.1407, -0.0155,  1.0211, -0.4912, -0.6499,\n",
       "          -0.2415, -0.2181, -0.7858, -0.1439, -0.1871, -0.3545, -0.4144,\n",
       "          -0.1925, -0.1414, -0.1414, -0.5660, -0.7725,  0.2023, -0.0377,\n",
       "          -0.2243,  0.6017, -0.1782, -0.0970, -0.2553, -0.5207, -0.5208,\n",
       "          -0.2159]]),\n",
       " tensor([[-0.1534, -0.1415, -1.5222,  0.7494, -0.2244, -0.2249,  0.1913,\n",
       "          -0.2035, -0.1111, -0.2211, -0.4108, -1.2897, -0.1414, -0.1553,\n",
       "           2.2564, -0.3974, -0.2244, -0.1414, -0.2072, -0.1419, -0.2227,\n",
       "          -0.1417, -0.5665, -0.1414, -0.1416, -0.1415, -0.1416,  1.3801,\n",
       "          -0.1161]]),\n",
       " tensor([[ 2.2514, -0.2225, -0.1485,  2.4900, -0.1421, -0.2193, -0.5968,\n",
       "          -0.4009, -0.6763, -0.1324, -0.6649, -0.1473,  0.7615,  2.3088,\n",
       "           1.5970, -0.5935,  2.5754, -0.2238,  1.4267, -0.2242, -0.1416,\n",
       "          -0.1414, -0.0999, -0.4819,  1.6057,  1.7270,  1.1566,  0.3001,\n",
       "           1.8829]]),\n",
       " tensor([[ 1.2582, -0.1414, -0.5179, -0.3201,  0.8971,  2.5152,  2.4379,\n",
       "          -0.1416, -0.1342, -0.1417,  1.3429, -1.2487, -0.2141, -0.5431,\n",
       "          -0.2280, -0.1414,  0.4967,  2.4557, -0.2244, -1.0707,  0.9035,\n",
       "           2.4257,  2.0706, -0.1423,  0.2884, -0.2242, -0.1418, -1.0129,\n",
       "          -0.1415]]),\n",
       " tensor([[-0.2257, -0.1414, -0.1414, -0.1414,  1.6682,  0.5955, -0.1417,\n",
       "          -0.2244, -0.1541, -0.1414, -0.2244, -0.2148,  2.8953,  0.4774,\n",
       "           0.6324, -0.1425, -0.2218, -0.5148, -0.1901, -0.0253, -0.8860,\n",
       "          -1.0185, -0.6244, -0.2222, -0.1414, -0.8101,  0.1671, -0.1423,\n",
       "           1.9416]]),\n",
       " tensor([[ 2.6042, -0.1417,  2.6425, -0.2071, -0.1414, -0.2340, -0.1414,\n",
       "          -0.2244, -0.2130, -0.1414,  0.0047,  1.6517, -0.4324, -0.1422,\n",
       "          -0.4673, -0.1611, -0.5400, -0.2235, -0.1416,  2.8293, -0.1414,\n",
       "          -0.1591, -0.1424, -0.4243,  2.4126,  2.4533, -0.5675, -0.1419,\n",
       "          -0.1414]]),\n",
       " tensor([[-0.2226,  1.4291, -0.1419, -0.1174, -0.1862, -0.1657, -0.2240,\n",
       "           1.2744, -0.1706, -0.2244, -0.1931, -0.1496,  2.6706, -0.2383,\n",
       "          -0.2345,  0.3076, -0.5398, -0.1414,  1.6419, -0.6503, -0.5865,\n",
       "          -0.6534, -0.0358,  2.2605, -0.3620, -0.1058, -0.5806, -0.1517,\n",
       "           2.1575]]),\n",
       " tensor([[-0.1414, -0.4559, -0.1415,  1.5402, -0.1414, -0.1461, -0.4733,\n",
       "          -0.2302, -0.2137, -1.7053, -0.2245, -0.2236, -0.2237, -0.1461,\n",
       "          -0.1414, -0.0559, -0.1471, -0.1415, -0.3288, -0.1626, -0.1811,\n",
       "          -0.2186,  2.3873, -0.3458,  1.3858,  1.9976, -0.1467, -0.5601,\n",
       "           0.3605]]),\n",
       " tensor([[ 1.0907, -0.3106,  1.1930, -0.2347, -0.1423, -0.1424, -0.1524,\n",
       "          -0.1592, -0.2322, -0.5968, -0.2191, -0.5851, -0.4054, -1.6812,\n",
       "           0.2504, -0.2294,  2.1186, -0.4498, -0.5925,  2.0142, -0.1423,\n",
       "           1.5082,  1.4851, -0.4664, -1.5562, -0.3963,  0.8807, -0.2245,\n",
       "           2.7429]]),\n",
       " tensor([[-0.2226, -1.2782, -0.2208, -0.5325, -0.1456, -0.5995, -0.1418,\n",
       "          -0.1415, -0.4532,  0.5709,  2.0782, -0.3725, -0.2807,  1.6024,\n",
       "          -2.1012, -0.1508, -0.4743,  1.0952, -0.5615, -0.1436, -0.5749,\n",
       "           2.8322, -0.1434, -0.8860, -0.2193, -0.1843, -0.1493,  0.0137,\n",
       "          -0.5970]]),\n",
       " tensor([[-0.1741, -0.1469, -0.1420, -0.1415, -0.2245,  0.8535, -2.2846,\n",
       "          -0.3278, -1.8202, -0.1419,  2.8725, -0.2245,  0.1571, -0.1415,\n",
       "          -0.1419, -0.2028, -0.1497, -0.2284, -0.5676,  3.3687, -0.5924,\n",
       "          -0.4568, -0.2241, -0.9386, -0.1968, -0.2249,  0.0853, -0.1791,\n",
       "          -0.2245]]),\n",
       " tensor([[ 2.5573,  0.2287, -0.4217, -0.1950,  2.1594, -0.1977, -0.2095,\n",
       "          -0.0751, -0.2224,  0.5906,  0.2407,  2.7238,  0.0938, -0.2482,\n",
       "          -0.1417, -0.2812, -0.5889, -0.1500, -0.1319, -0.1417, -0.0724,\n",
       "          -0.2243, -0.2255, -0.2245,  2.2838, -0.6469, -0.1415, -0.1416,\n",
       "           0.9725]]),\n",
       " tensor([[-0.0194, -0.1455, -0.2228, -0.5812, -1.4013, -0.0478,  1.5997,\n",
       "           0.5748,  0.4574, -1.2162, -0.1454, -0.3133, -0.2278, -0.2517,\n",
       "          -0.2288,  0.1295, -0.0809,  0.7263,  0.2607, -0.0914, -0.0611,\n",
       "          -0.1454, -0.2283, -0.1454,  0.9453,  2.1025,  2.1891, -0.1454,\n",
       "          -0.5933]]),\n",
       " tensor([[-0.1477, -0.2288, -0.1473, -0.1659, -0.6013,  2.7218,  1.7730,\n",
       "           2.9785, -0.1608, -0.1563, -1.3176, -0.2309,  0.1482, -0.7220,\n",
       "          -0.1479,  1.9849, -0.1683, -0.2308, -0.1986, -0.1956,  2.2423,\n",
       "           1.0146, -0.1004, -0.1473,  1.4140, -0.1477,  3.3895, -0.6116,\n",
       "           0.1979]]),\n",
       " tensor([[-0.5021, -0.7288,  1.4623,  0.0387, -0.6437, -0.1538, -0.1483,\n",
       "          -0.1495, -0.6582, -0.2337,  2.4747, -0.2678,  2.4667, -0.1495,\n",
       "          -1.3784, -0.2316, -0.7518, -0.6972, -0.1483, -0.2321, -0.1483,\n",
       "          -0.2121, -0.1491, -0.1560, -0.2337,  0.3451,  2.9875, -0.5391,\n",
       "          -0.1483]]),\n",
       " tensor([[-0.2743, -0.1853, -0.1631, -0.6616,  0.2914, -0.1487,  0.0100,\n",
       "           0.5533,  2.3794,  0.2394,  2.7500,  0.6348, -0.7930, -0.0957,\n",
       "          -0.2001, -0.1938,  1.4067,  1.1581, -1.2270, -0.2858, -0.5482,\n",
       "          -0.5507,  3.1649, -0.2212, -0.5386, -0.2320,  2.3031, -0.1696,\n",
       "          -0.0071]]),\n",
       " tensor([[-0.1490, -0.1521, -0.2329, -1.2264, -0.1513, -0.1679, -1.7386,\n",
       "          -0.2626,  1.8659, -0.1495, -0.1492, -0.2834,  1.1758, -2.3452,\n",
       "          -0.3564, -1.2406,  0.9404, -0.2330,  0.4663,  3.3269,  1.5242,\n",
       "           2.2512,  3.3752, -0.0538,  2.2449, -0.1850,  2.4950, -0.8476,\n",
       "          -0.2179]]),\n",
       " tensor([[-0.1587,  0.2670,  0.7269, -0.2269,  1.9271, -0.5322, -0.1383,\n",
       "          -0.1597, -0.2257, -0.1498,  0.3645, -0.4445, -0.1505, -0.2880,\n",
       "           0.7463, -0.2331, -0.6355, -0.1710, -0.1494, -0.2331, -0.1453,\n",
       "           2.1052, -0.2326, -0.2333,  0.1575, -0.2314,  0.5254, -0.1491,\n",
       "           0.7504]]),\n",
       " tensor([[-0.2343,  2.1763, -0.0223, -0.1724, -0.1672, -0.1515, -0.1647,\n",
       "          -0.1492, -0.2104, -0.2281, -0.2141,  1.6345, -0.2173, -0.1492,\n",
       "          -0.1714,  1.8042, -0.1493,  3.3079, -1.2019, -0.3140, -0.3438,\n",
       "          -0.2190, -0.2511,  0.5334,  0.8656, -0.1447,  0.2719,  0.0856,\n",
       "           0.2457]]),\n",
       " tensor([[ 0.3319, -0.2771,  2.1645, -0.5911, -0.5722,  1.8273, -0.2017,\n",
       "           1.2213, -0.1559,  1.5315, -0.2690, -0.2243, -0.2332, -0.5637,\n",
       "          -0.1777, -0.1493, -0.1493, -0.1492,  3.2871,  1.8668, -0.6034,\n",
       "          -0.2306, -0.2138, -0.2308,  0.3494, -0.4599, -0.1525, -0.1496,\n",
       "          -0.1283]]),\n",
       " tensor([[-0.1505, -0.2331, -0.0762, -0.2332, -1.8169,  1.1185, -0.0940,\n",
       "           0.0702,  2.5805,  0.0048,  3.0150, -0.6244, -0.1551, -0.1493,\n",
       "           0.0409,  2.6288, -0.4682,  3.6266,  2.1199,  0.2215, -0.1798,\n",
       "          -0.5877,  0.2182,  1.3134, -0.2109, -0.0853, -0.1654, -0.1493,\n",
       "           0.0243]]),\n",
       " tensor([[-0.2310, -0.1472, -0.2131, -0.3491, -0.7864,  0.1755, -0.1264,\n",
       "          -0.9306, -0.1478, -0.2258, -0.1718, -0.1473,  1.8364, -0.2301,\n",
       "          -1.2527, -0.1836, -0.1511, -0.2317,  2.0956, -0.1474, -0.2211,\n",
       "          -0.1778, -0.1319,  0.6472, -1.5051,  2.8936,  3.2955,  0.6152,\n",
       "           3.5830]]),\n",
       " tensor([[ 0.1306, -0.5449, -0.2356, -0.1586,  0.9104, -0.2297,  2.1629,\n",
       "          -0.2620, -0.0647,  2.3449,  2.3981,  0.5702, -0.3259, -1.3959,\n",
       "          -0.0456,  1.0872, -0.3117, -0.2793, -1.3510,  0.4737, -0.9881,\n",
       "          -0.2712, -0.2370,  0.2248,  2.0601,  0.4520, -0.1461,  0.0833,\n",
       "          -0.2252]]),\n",
       " tensor([[-0.1871, -0.9970,  0.8519, -0.4741, -0.1586, -0.1487, -0.1828,\n",
       "          -0.2724, -0.1563,  0.0569, -0.4379, -0.1667, -0.1489,  2.4225,\n",
       "          -0.1765, -0.2294, -0.2299, -0.2330, -0.4587, -0.1488, -0.1515,\n",
       "          -0.1487, -0.1503, -0.3151, -0.1195,  1.6552, -0.2325, -0.1788,\n",
       "           2.3743]]),\n",
       " tensor([[-0.1518, -0.4801,  2.9913, -0.2344,  1.8807, -0.1501,  3.0045,\n",
       "          -0.1502,  0.1078,  0.4417,  1.3281, -0.1793, -0.2367, -0.2279,\n",
       "          -0.1843, -1.2620,  0.1394, -0.1748, -0.2033, -0.2283, -0.2181,\n",
       "          -0.1224,  3.0988,  2.1190,  0.5307, -0.5006, -0.1600,  0.3085,\n",
       "           3.1348]]),\n",
       " tensor([[-0.1574,  1.0521,  0.0155, -0.5365, -0.1638, -0.2430,  0.5092,\n",
       "           2.1522,  0.3378,  0.5161, -0.1538, -0.2503,  2.6468, -1.1378,\n",
       "          -0.2353, -0.2429, -1.6585,  0.2794, -0.1581, -0.1613, -0.1659,\n",
       "          -0.1594, -0.2425, -0.1584,  0.3081, -0.2429, -0.1612,  0.3317,\n",
       "           1.8615]]),\n",
       " tensor([[-0.2636, -0.2472, -0.3497,  1.6209,  2.9952, -1.3914, -0.1634,\n",
       "          -0.6206, -0.1622,  2.6991,  0.1527, -0.5231,  0.1687, -0.2419,\n",
       "          -0.1626,  3.1936, -0.2338,  2.3242, -2.0007,  2.4763, -0.3897,\n",
       "          -0.1623, -0.2695, -0.1622, -0.1617, -0.2475, -0.5427, -0.2471,\n",
       "          -0.1624]]),\n",
       " tensor([[-0.5564, -0.1644, -1.7579, -0.5091, -0.5896,  2.5128,  2.1824,\n",
       "           1.7241, -0.1451,  2.9617,  2.2360, -0.2378,  0.5686, -0.2417,\n",
       "          -0.1643, -0.2483,  1.5471, -0.6194, -0.6426, -0.1643, -0.2493,\n",
       "          -0.2492, -0.2470, -0.1643,  2.3155, -1.1690,  0.4158,  2.5685,\n",
       "           0.0197]]),\n",
       " tensor([[-0.2502, -0.2460, -0.3102, -0.3829, -0.1655, -0.3956,  2.2105,\n",
       "          -0.1653, -0.5118, -0.1657, -0.1941, -1.8744, -0.6193, -0.2525,\n",
       "          -0.1624, -0.2204, -1.4721, -0.2499, -0.3461, -0.2511, -0.1698,\n",
       "          -0.2222, -0.5444, -0.1912, -0.6175, -0.1653, -0.5792,  0.0708,\n",
       "          -0.1240]]),\n",
       " tensor([[-0.2505, -0.1658, -0.6172, -0.1670, -0.3069,  2.2332, -0.1250,\n",
       "          -0.8550, -0.1935, -0.2484,  0.4171, -0.2165, -1.0089, -0.5791,\n",
       "           1.7777, -0.2000, -0.0391,  0.3645,  2.3065,  1.3953, -0.6211,\n",
       "          -0.2226, -0.3521, -0.1659, -0.2672,  2.5267, -0.6228, -0.1425,\n",
       "          -1.3870]]),\n",
       " tensor([[-0.2079,  1.8130, -0.2476,  0.4932, -0.2495, -0.1085,  0.0638,\n",
       "          -0.2101, -0.2518,  0.1131, -0.3107, -0.2399, -0.2123,  2.8360,\n",
       "           1.4176, -0.2508, -0.0595, -0.7609, -0.2401, -0.1661, -0.6191,\n",
       "          -0.7178,  1.6392, -0.6214,  1.5516,  0.9666, -0.1873, -0.1996,\n",
       "           2.4106]]),\n",
       " tensor([[ 0.1200, -0.1662, -0.6356, -0.1554, -0.1672, -0.2498, -0.2514,\n",
       "           0.8171,  1.7139,  0.7724,  0.5953, -0.2390, -0.1662, -0.2501,\n",
       "          -0.1824, -0.1662, -0.7766, -0.1921, -0.2045, -0.2512,  1.8283,\n",
       "           0.9524, -0.2525, -0.6134,  3.1630, -0.1470,  3.6382, -0.1777,\n",
       "          -0.1662]]),\n",
       " tensor([[-0.9353, -0.1663, -0.1667, -0.2028, -0.1723,  2.4704, -0.1772,\n",
       "           0.6133, -0.6826, -0.1663,  0.2395, -0.1663, -0.1714, -0.1731,\n",
       "          -0.3628,  0.7691,  2.1691, -0.1533, -0.2803, -0.2172, -0.1524,\n",
       "          -1.4492, -0.1663, -0.0057, -0.6183,  1.7593, -0.2500, -0.2304,\n",
       "           2.5393]]),\n",
       " tensor([[-0.0926, -0.2511, -0.2525, -1.8224,  0.4850, -0.2513,  1.8607,\n",
       "           0.4924, -0.1663,  0.9137, -0.2188, -0.1663, -0.2510, -0.1905,\n",
       "          -0.1195, -0.2513, -0.1755, -0.6216,  0.9650, -0.1662, -0.2580,\n",
       "          -0.1663, -0.1663, -1.9908, -0.5386,  1.6048, -1.8339, -1.5188,\n",
       "          -0.2513]]),\n",
       " tensor([[-0.1663,  2.4658, -0.2122, -0.1663, -0.2491,  3.0473, -0.6024,\n",
       "          -0.1663, -0.2515, -1.7285, -0.8230, -0.1213, -0.2910, -0.2242,\n",
       "          -0.1663,  1.3173, -0.2442, -0.5492,  2.2607,  0.4076, -0.2515,\n",
       "           0.2210,  1.1453, -0.4501, -0.6214, -0.1672, -0.5629,  3.1408,\n",
       "           3.2202]]),\n",
       " tensor([[-0.5009,  0.2972, -2.0166, -0.1663, -0.3818, -0.4840,  2.1056,\n",
       "          -0.2649, -0.2515, -0.2436,  3.0156, -0.1672, -0.1665, -0.4472,\n",
       "          -0.6104, -0.1683, -0.1919, -0.2255, -1.8013, -0.2483,  0.2993,\n",
       "           0.7567, -0.1668, -0.1681, -2.4111, -0.2118, -0.1768, -0.1645,\n",
       "          -0.3154]]),\n",
       " tensor([[ 1.6898, -0.2502,  0.1317, -0.3517, -0.2563,  0.0424,  0.0140,\n",
       "          -0.1418, -0.6216, -0.6224, -0.2273, -2.2513, -0.2993, -0.3682,\n",
       "          -1.1221, -0.1813, -0.2515, -0.5378,  3.5636,  0.4846, -0.5697,\n",
       "          -0.1754, -0.2520,  1.1617, -0.1843, -0.4397, -0.1665, -0.2303,\n",
       "          -0.5832]]),\n",
       " tensor([[ 0.2592, -0.1667, -0.2501,  0.9019,  0.1233, -2.0315, -0.6185,\n",
       "          -0.1663, -0.1663, -0.1663, -0.2078, -0.2076, -0.6217, -0.1666,\n",
       "          -0.0741, -0.1680, -0.3209,  1.0974, -0.1634,  3.0265, -0.1664,\n",
       "           0.5420, -0.3927, -0.1796, -0.2516, -0.2620, -0.2516, -0.5412,\n",
       "          -0.3031]]),\n",
       " tensor([[-0.2438,  0.1263,  2.5460, -0.1676, -0.1668, -0.9201, -0.1908,\n",
       "          -0.1664, -0.1676,  0.4184, -0.2516, -0.2515,  1.1085,  2.0584,\n",
       "          -0.2514, -0.1663,  0.0851, -0.1664, -0.1696, -0.1452, -0.1660,\n",
       "          -0.2515,  2.2353, -0.2516, -0.1664, -0.2706, -0.2350,  0.4784,\n",
       "          -0.5187]]),\n",
       " tensor([[-0.8058,  2.9890,  0.1629, -0.6217,  0.8912,  1.9677, -1.4936,\n",
       "          -0.2275,  0.0945, -0.2501, -0.4232, -0.2094, -0.6188,  2.5116,\n",
       "           2.2966,  0.3037, -0.1964,  0.8088, -0.1844,  2.6028, -0.2515,\n",
       "          -0.1678, -0.1664, -0.1823, -1.3497,  2.3103,  2.0417, -0.2515,\n",
       "          -0.1036]]),\n",
       " tensor([[ 3.3197, -0.1663, -1.1360, -0.1330, -0.2632, -0.1663,  0.6259,\n",
       "          -0.1790, -0.5746,  0.7767, -0.1664, -0.2516,  0.4219, -0.1664,\n",
       "          -0.6050, -0.1788, -0.1664, -0.2307, -0.1781, -0.2287,  0.0741,\n",
       "           1.5366, -0.2476, -0.3978, -0.7449, -0.4461, -0.1899, -0.6215,\n",
       "          -0.5745]]),\n",
       " tensor([[-0.1665, -0.6212, -0.2499,  1.8311,  2.1275, -0.2540, -0.1670,\n",
       "          -0.1734, -0.2781, -0.2431,  0.3702, -0.1664,  1.8944, -0.1935,\n",
       "          -0.4753, -0.2516,  1.5720,  1.4927,  1.1896, -0.2324, -0.1667,\n",
       "          -0.2515, -0.1664, -0.1664, -0.4649, -0.1787, -0.7398, -0.5995,\n",
       "          -0.1688]]),\n",
       " tensor([[ 2.6972, -0.2514,  0.2240,  1.4019, -0.2440, -0.1664, -0.4703,\n",
       "          -0.2516, -1.3306, -1.4584, -0.1825, -0.5247, -0.2516, -0.2442,\n",
       "          -0.3058,  1.5117, -0.6133, -0.3437, -0.0374, -1.7045, -0.1670,\n",
       "          -0.1641, -0.3597, -0.1682, -0.2469,  0.9271, -1.1982, -0.2493,\n",
       "          -0.2486]]),\n",
       " tensor([[-0.4642, -0.8417, -0.2464, -0.4439, -0.5589, -0.2515, -0.2490,\n",
       "          -0.1663, -0.1664,  0.7496,  0.9654, -0.1817, -0.2511,  2.7861,\n",
       "           0.3008, -0.6199, -0.5320, -0.2277, -0.1752, -0.2085, -0.8456,\n",
       "          -0.1664,  0.2980, -0.9431, -1.8881, -0.1886, -0.2516, -0.1670,\n",
       "          -0.6006]]),\n",
       " tensor([[-1.6277,  1.1283,  0.7128,  2.3827,  2.8336, -1.8218, -0.6747,\n",
       "          -0.2399,  2.8341, -0.4598, -0.5550, -0.1664, -0.2492,  0.3023,\n",
       "          -0.6498,  0.7585,  0.8757, -0.5100,  2.7198, -0.2355, -0.2516,\n",
       "          -0.1664,  2.6837, -0.1664,  2.9233,  2.1583, -0.5445,  3.3066,\n",
       "          -0.4515]]),\n",
       " tensor([[-0.1663,  0.4717,  0.4111,  3.7861,  1.8527, -0.1664, -0.2454,\n",
       "          -0.1788,  0.3020, -0.2523,  2.6157,  0.0280, -0.1664, -0.1663,\n",
       "          -0.2450, -0.1673, -0.4444, -0.1721, -0.1666, -0.3087, -1.1755,\n",
       "          -0.2509, -0.3670, -0.1664, -0.6219,  1.2875, -0.2028, -0.1664,\n",
       "          -0.2484]]),\n",
       " tensor([[-0.5256,  1.5072, -0.5984, -2.0982,  1.5819,  0.5019, -0.4136,\n",
       "          -0.5658, -2.2911, -0.1716, -0.1663, -0.5896,  3.3195, -0.2516,\n",
       "          -0.1664, -0.2514, -0.5903, -0.5231, -0.1855, -0.1844, -0.8011,\n",
       "          -0.4297, -0.2515, -0.1668,  0.0694, -0.1667, -0.2751,  0.8078,\n",
       "          -0.7703]]),\n",
       " tensor([[-0.1663,  0.6198, -0.5848, -0.5742, -0.2934, -0.1663, -0.1729,\n",
       "          -0.2857, -0.1713, -0.2379, -0.3679,  2.4879, -0.1667,  1.1612,\n",
       "          -0.1459, -0.2498, -0.2370, -0.2079,  1.1435,  2.4760, -0.1663,\n",
       "           1.4864, -0.5663, -0.1663, -0.2532, -0.2383, -0.1399,  2.7622,\n",
       "           0.4238]]),\n",
       " tensor([[-0.1979, -0.6066,  0.0611, -0.1663,  1.7178, -0.2488,  0.6672,\n",
       "          -0.1611, -0.1663,  0.5109, -0.2581, -0.5010,  0.1863, -0.1663,\n",
       "          -0.1665,  2.2912, -0.1670,  0.4158, -0.6174,  2.4433,  1.8797,\n",
       "           2.9904, -0.2495, -0.1721, -0.2453, -0.2342, -0.0629, -0.2102,\n",
       "          -0.3945]]),\n",
       " tensor([[-0.3410, -0.1675, -0.2439, -0.2465,  0.0402, -0.2340, -0.1663,\n",
       "          -0.6099, -0.2109, -1.2587, -0.1664, -0.1909,  0.2525, -0.1658,\n",
       "           0.9555,  2.9169, -0.1664,  0.9889, -0.1666,  0.8327, -0.0866,\n",
       "          -0.1663, -0.1666, -0.1822, -0.1825, -0.1665, -0.2515, -0.2076,\n",
       "           0.8671]]),\n",
       " tensor([[-0.1663, -0.6152, -0.1914, -0.2443, -0.6230, -2.3632, -0.3066,\n",
       "          -0.2048, -1.9765, -0.1788, -0.2409, -0.2514, -0.6215, -0.2484,\n",
       "           2.4030,  0.7163, -0.0300, -0.1724, -0.2425,  2.4900,  2.8483,\n",
       "           2.2284, -0.5404, -0.1021, -0.6593, -0.3506, -0.1664,  0.0043,\n",
       "           1.6988]]),\n",
       " tensor([[-0.1849, -0.1745, -0.5179, -0.1675, -0.1880,  0.6402, -0.1668,\n",
       "          -0.9792,  0.1867, -0.1663, -0.1292,  0.1442, -0.5842, -0.3972,\n",
       "          -0.2512, -0.2047, -0.6077, -0.1668, -0.1663, -0.1669, -0.2515,\n",
       "           2.9446, -0.6917, -0.6497, -0.1772, -0.4864, -0.2494, -0.5900,\n",
       "          -0.5644]]),\n",
       " tensor([[-0.2921,  1.8465, -0.1663,  3.1363, -0.1887,  1.6930, -0.1939,\n",
       "          -0.1663, -0.4322, -1.2527, -0.2517,  1.3089, -0.1846,  1.6156,\n",
       "          -0.2110, -0.0263, -0.1813, -0.0473, -1.0607, -0.2515, -0.2359,\n",
       "          -0.2623,  2.4378, -0.1656,  1.4919, -0.1755,  0.0331,  1.9848,\n",
       "          -0.6198]]),\n",
       " tensor([[-0.2491, -0.1846, -0.2439, -0.1664, -0.2385, -0.2557,  0.4660,\n",
       "          -0.2514, -0.3049, -0.2516, -0.2375, -0.1958, -0.1677, -0.1924,\n",
       "          -0.5475,  0.9089, -0.1662,  2.2422, -0.1665, -0.1663,  0.7399,\n",
       "          -0.2134,  1.4780, -1.9867, -0.2516, -0.1983, -0.6211,  2.1109,\n",
       "          -0.0739]]),\n",
       " tensor([[-0.2516,  2.4911, -0.2005, -0.1910, -0.2493, -0.2296, -0.1857,\n",
       "           1.5926, -1.4369, -0.3137,  2.2747, -1.3174,  2.6251, -1.0912,\n",
       "           0.1683,  0.7195,  0.3071, -0.2514, -0.2943, -0.1534, -0.4039,\n",
       "          -0.2498, -0.1681, -0.1667, -0.1663, -1.7809, -0.1779, -0.4061,\n",
       "          -0.0104]]),\n",
       " tensor([[-0.1737, -0.1648, -0.2440, -0.2864, -0.1647, -1.2533, -0.2493,\n",
       "           1.0814,  2.7598, -0.1792,  1.5493, -0.7528, -0.2449,  2.2958,\n",
       "           2.3435,  1.9803, -0.2494, -0.2104, -0.2372, -0.6147, -0.1647,\n",
       "          -0.5928,  1.0281, -0.1647,  0.0412, -0.1647, -0.6828, -0.1689,\n",
       "           0.1182]]),\n",
       " tensor([[-0.1718,  2.6080, -0.0520, -0.2242, -0.6798, -1.9425, -0.2559,\n",
       "          -1.5643,  0.3791, -0.1639, -0.3972,  1.4888, -0.4326,  0.1458,\n",
       "          -0.4714, -0.2483, -0.2471,  2.0566,  0.7197, -0.4657,  1.1996,\n",
       "          -0.1914, -0.1696, -0.2598, -0.2012, -0.2394, -0.2501, -0.5513,\n",
       "          -0.1682]]),\n",
       " tensor([[-0.1635, -0.2443, -0.1636, -0.1637, -0.1636, -0.1656, -0.3905,\n",
       "          -0.0629, -0.7256, -0.2433, -0.1780,  2.9918, -0.1669, -0.1989,\n",
       "           0.5299, -0.5192, -0.2677, -0.1636, -1.2602, -0.1508, -0.1648,\n",
       "           2.8151, -0.1817, -1.1926, -0.1871, -0.1627, -0.2058, -0.5211,\n",
       "          -0.1951]]),\n",
       " tensor([[-0.1634, -0.2479, -0.5841,  1.5681, -0.8971,  0.1381, -1.7250,\n",
       "           0.5867,  2.6477, -0.6559, -0.6463, -0.8123, -0.1016, -0.3714,\n",
       "          -0.1966,  0.2670,  1.9192, -0.2503,  0.4136, -0.5014, -0.2400,\n",
       "          -1.0411,  1.0286, -0.1460, -0.6874, -0.6182,  3.0422, -0.5754,\n",
       "          -0.2160]]),\n",
       " tensor([[-0.0450, -0.3395, -0.1659, -0.6182, -0.3421,  2.5252, -0.8962,\n",
       "          -0.2414, -0.1763, -1.1339, -0.4312, -0.1081, -0.2286,  0.4255,\n",
       "          -0.1962,  0.9480, -0.1632, -0.2232,  0.5885,  1.4503, -0.1640,\n",
       "          -0.1590, -0.2464,  0.4564,  0.5128, -0.2474, -0.5482, -0.2474,\n",
       "          -0.1803]]),\n",
       " tensor([[-0.5983, -0.3746, -0.1325, -0.2471,  2.8901, -0.1632, -0.0596,\n",
       "          -0.6085, -0.1632, -0.1767,  0.6169,  0.4132, -0.2469, -0.0382,\n",
       "          -0.3049, -0.1827, -1.2649, -0.1019, -0.1865, -0.4052,  1.5075,\n",
       "          -0.2451,  1.6431, -0.1632,  1.6562, -0.6187,  2.7279, -0.3702,\n",
       "          -0.1632]]),\n",
       " tensor([[-0.1636, -0.2300,  0.1748, -0.2173, -0.1120, -0.5034, -0.2547,\n",
       "          -0.4790,  0.8227, -0.1631, -0.1634, -0.2181, -0.1632,  0.1387,\n",
       "          -0.2100, -0.2473,  3.9646, -0.1465, -0.1798, -0.1706, -0.6184,\n",
       "          -0.2492, -0.2872, -0.2194, -0.2494, -1.7878,  3.2522, -0.2354,\n",
       "          -2.8132]]),\n",
       " tensor([[ 1.4783, -0.1631, -0.1589, -1.8043, -0.3694, -1.4091, -0.2783,\n",
       "          -0.1835, -0.1646, -0.1682, -0.5810, -1.9869, -0.1653, -0.1968,\n",
       "          -0.0144, -0.1733, -0.1631, -0.1631, -0.5464,  0.3753,  2.2878,\n",
       "           1.5471, -0.1947, -0.3563, -0.1630, -0.0408, -0.1631,  1.1138,\n",
       "          -0.5548]]),\n",
       " tensor([[ 0.7844, -0.2225, -0.2301, -0.5286, -1.2975, -0.1425, -0.2113,\n",
       "          -0.1478, -0.2302, -0.4873,  0.3223, -0.1711, -0.1471,  2.5084,\n",
       "          -0.5871, -0.2460,  0.1526, -0.6467, -0.6014, -0.3072,  2.8402,\n",
       "           2.7341, -0.1473, -0.6645, -0.6019, -0.1339, -0.1475, -0.6025,\n",
       "          -0.4967]]),\n",
       " tensor([[-1.1204,  0.8721, -0.7175, -0.5301, -0.1484, -0.5481,  0.9181,\n",
       "           2.7510,  1.1247, -0.2170, -0.1468, -0.2173, -0.1360, -0.1364,\n",
       "          -0.2180, -1.4743,  0.5418, -0.1825,  0.3273,  2.0126,  0.3097,\n",
       "          -0.1369,  2.2887,  1.0720,  2.4730,  2.0786,  2.0622, -0.3888,\n",
       "           0.4437]]),\n",
       " tensor([[ 2.4267,  0.1098, -0.2120,  1.5788,  2.2010,  0.8366, -1.3058,\n",
       "          -0.2117, -2.4005, -0.2557,  1.2707, -1.4974, -0.0881, -0.1294,\n",
       "           0.0178,  0.8084, -0.6309, -0.0601,  1.6812, -0.1295,  2.4174,\n",
       "          -0.6335,  1.2107, -0.1400, -0.1301,  2.5258,  0.9793,  2.3003,\n",
       "          -0.2113]]),\n",
       " tensor([[-0.2469, -0.1272, -0.5842, -0.2088,  1.8818, -0.1274, -0.3495,\n",
       "          -0.1263, -0.2078, -0.4968, -0.2557, -0.1918, -0.1544, -0.1778,\n",
       "          -1.2467,  2.4375,  2.0282,  1.5037, -0.1603, -0.5417, -0.1391,\n",
       "          -0.1736, -0.1297, -0.2046, -0.2632,  2.7448,  1.9251,  2.3851,\n",
       "           0.2271]]),\n",
       " tensor([[ 1.9805,  1.0979, -0.2062, -0.1237, -0.2048,  1.6882, -0.0965,\n",
       "           2.4876, -0.0830, -0.1241,  2.6584, -0.1244, -0.2059, -0.1239,\n",
       "          -0.1278, -0.2049, -0.2062, -0.3406, -0.1946, -0.1578, -0.1990,\n",
       "          -0.5860, -0.1988,  1.8222, -0.5464, -0.1240, -0.5509,  2.2094,\n",
       "          -0.2500]]),\n",
       " tensor([[-0.2474,  2.1750, -0.8180, -0.1694, -0.5770,  1.1419,  1.6597,\n",
       "           0.6598,  2.0382, -0.0380,  0.1454, -0.1545, -0.1224,  1.3337,\n",
       "          -0.2008, -0.0137,  0.2059, -0.1934, -0.0040, -0.2053,  0.3606,\n",
       "          -0.1224,  1.9518, -0.3772,  0.8235, -0.0603,  0.1082, -0.1455,\n",
       "           0.1082]]),\n",
       " tensor([[-0.1471,  0.5474, -0.1235,  3.2030, -0.1218, -0.1999, -0.2045,\n",
       "          -0.2121, -0.4279, -0.5514, -0.4834, -0.2045, -0.2045, -0.7699,\n",
       "          -0.3997, -0.5462, -0.1217, -0.5616, -0.1316, -0.1214, -0.2020,\n",
       "          -0.1152, -0.1217, -0.2046, -0.1717, -0.9314, -0.5664, -0.1599,\n",
       "           0.8207]]),\n",
       " tensor([[-0.2086, -0.1214, -0.1217,  0.0674,  1.2907, -0.1282,  0.7574,\n",
       "          -0.5123, -0.1920, -0.1214, -0.2043, -0.5203,  0.2384, -0.1214,\n",
       "          -0.1249, -0.2061,  3.0011, -0.2129, -0.6333, -0.1214,  1.8130,\n",
       "          -0.1218, -0.1316, -1.0664, -1.5263, -0.2042, -0.2041, -0.1214,\n",
       "          -0.5378]]),\n",
       " tensor([[-0.2032, -0.1244, -0.2039,  0.5468, -1.1242, -0.2006,  2.5850,\n",
       "          -0.1212, -0.1215, -0.1216, -0.1212, -0.5688, -0.1659, -1.8116,\n",
       "          -0.5764,  0.5454, -0.2031, -0.2019,  0.5958,  0.5580,  0.2389,\n",
       "          -0.2034, -0.2041, -0.2042,  3.1258, -0.1517, -0.2037, -0.2870,\n",
       "           1.4034]]),\n",
       " tensor([[ 0.9933, -0.3967, -0.2040,  2.4397, -0.7495, -0.1212, -0.1212,\n",
       "           1.5221, -0.1214,  1.1936, -0.1212, -0.1296, -0.5365,  1.9421,\n",
       "          -0.1212, -0.2041, -0.3600, -0.0943, -0.1577, -0.1711, -0.1712,\n",
       "          -0.0950,  3.0842, -0.1212, -0.2027, -1.6630,  0.1489, -0.1208,\n",
       "          -0.2047]]),\n",
       " tensor([[-2.1820, -0.1210, -0.2028,  2.2719,  0.4314, -0.1432, -0.1205,\n",
       "           2.3908, -0.0955,  1.9157, -0.2035,  3.3681, -0.4717, -0.1216,\n",
       "           1.6990, -0.2040, -0.2038, -0.1234, -1.6920,  0.2025, -0.2020,\n",
       "           0.3841, -0.9767,  0.1065,  0.3453, -0.1211,  0.9252, -0.1557,\n",
       "           2.3162]]),\n",
       " tensor([[-0.1244,  2.7213, -0.1172,  1.3924, -0.0179,  0.8110, -0.1222,\n",
       "          -0.4255, -0.1214,  1.5520, -0.2037, -0.4627,  2.5242,  2.5920,\n",
       "           0.8982, -0.0675, -0.1247,  0.0771, -0.1606,  0.2902, -0.7521,\n",
       "          -0.4000, -0.2039,  2.2486, -0.2028, -0.1211, -1.0993, -0.7184,\n",
       "           0.8910]]),\n",
       " tensor([[-0.1211,  2.6310, -0.2042, -0.1976, -0.2048, -0.5012,  0.1916,\n",
       "          -0.1231, -0.1211, -0.4423,  3.2659,  1.8898, -1.0780,  0.9144,\n",
       "          -0.2969,  1.6585, -0.1211, -0.1211,  0.5400, -0.0960,  0.9482,\n",
       "          -0.1214, -0.1393, -0.6039,  3.0334, -0.1322,  0.2973,  1.7718,\n",
       "           2.6157]]),\n",
       " tensor([[-0.1970, -0.5841,  0.3854, -0.0877,  0.2130, -0.1863,  0.0994,\n",
       "          -0.2001,  1.3148, -0.2044, -0.2454,  0.2412, -0.0833, -0.1214,\n",
       "          -0.6738,  1.8783, -0.1211, -0.3216, -1.5338, -0.2027, -0.2999,\n",
       "          -0.1199,  3.0381, -0.1934, -0.5557, -0.2040, -0.3514,  0.3919,\n",
       "           0.8078]]),\n",
       " tensor([[-0.1298, -0.2617, -0.4234, -0.1279, -0.2039, -0.1229, -0.2038,\n",
       "          -0.1211, -1.2670,  2.6516,  1.0732, -0.2038,  0.6813, -1.1351,\n",
       "          -0.1987,  0.5568, -0.1211,  1.7840, -0.1518, -0.1132, -0.5015,\n",
       "          -0.0592,  1.0729, -0.6951, -0.1334, -0.1211,  2.4633,  0.7143,\n",
       "          -0.2573]]),\n",
       " tensor([[ 2.9282, -0.1424, -0.5153, -1.4015,  2.2891, -0.2040, -0.1401,\n",
       "          -0.4404,  3.0132, -0.0660, -0.1968, -0.1218,  2.3082, -0.1355,\n",
       "          -0.1401,  0.0938, -0.1214, -0.1403, -0.8254,  3.5316, -0.1211,\n",
       "          -0.2037, -0.1255, -0.1984,  0.2028, -0.1211,  3.1501, -0.5923,\n",
       "           0.8906]]),\n",
       " tensor([[ 0.1696, -0.1329, -1.9218, -0.1212, -0.2030, -0.1214, -0.2038,\n",
       "           0.3945,  2.4348, -0.4410, -0.1552,  2.2445, -0.1781, -1.1727,\n",
       "          -0.1211, -0.1213, -0.1211, -0.1209,  1.0721,  2.8113, -0.2040,\n",
       "          -1.1853, -0.4872,  1.6538, -0.2040, -1.7883, -0.6836,  2.1375,\n",
       "          -0.1498]]),\n",
       " tensor([[ 1.0205, -0.1954, -2.0859, -0.1762, -1.7211, -1.4839, -0.1457,\n",
       "          -0.2035, -0.1263, -0.5736, -0.4000, -0.2040,  2.4977,  0.0015,\n",
       "          -0.1214, -0.1835, -0.4015, -0.1328, -0.7114, -0.1211,  2.5931,\n",
       "          -0.2031, -0.1198, -0.1213,  2.8859, -0.1844, -0.2329, -0.2038,\n",
       "          -0.1208]]),\n",
       " tensor([[-0.1919, -0.1630, -0.1236, -0.1978, -0.2002, -0.2938, -0.1948,\n",
       "           0.2317,  0.1646, -0.1704, -0.2888,  0.6470,  1.0743, -0.2181,\n",
       "           0.3825, -1.6999, -0.2046, -1.2904, -0.1211, -0.2023, -0.2037,\n",
       "          -0.2000, -0.1245,  2.8150, -0.1228, -0.2514,  0.9031,  2.4762,\n",
       "           2.0274]]),\n",
       " tensor([[-0.1345, -0.1134, -0.5945, -0.1945,  2.3029, -0.2035,  2.2764,\n",
       "          -0.2035, -1.1953, -0.1825, -0.2403,  2.4432, -0.3769, -0.1985,\n",
       "          -0.1942, -0.4709, -0.0938, -0.1207, -0.2334, -0.2016,  3.2577,\n",
       "           3.4281, -0.2035, -0.0718, -0.2080, -1.4705,  0.2697, -0.2035,\n",
       "           3.0834]]),\n",
       " tensor([[-0.4449, -0.1207, -0.5601,  1.1496, -2.0973,  2.3948,  0.5255,\n",
       "          -0.2030, -0.1711,  2.6510, -0.1260,  1.6062,  0.0755, -0.1827,\n",
       "          -0.1616, -0.1210,  2.6840,  3.3018, -0.1938, -0.2022,  0.5327,\n",
       "          -0.1208, -0.4603, -0.5624, -0.1207,  0.5108,  0.6791, -2.1204,\n",
       "          -0.2029]]),\n",
       " tensor([[ 2.2610, -0.4149,  1.4338, -0.2002, -0.1962,  2.5321, -0.3053,\n",
       "          -0.4563, -0.4113, -0.1285,  0.7993, -0.1207,  2.2271, -0.5519,\n",
       "           1.5686, -0.2030, -0.1310, -0.5730,  1.2581,  0.1375, -0.1207,\n",
       "           2.1815, -0.2032, -0.4571,  0.7845, -0.9345, -0.2023,  3.1791,\n",
       "          -0.1214]]),\n",
       " tensor([[ 0.8416, -0.1207, -0.2022, -0.7777, -0.6152, -0.2038, -0.2384,\n",
       "          -0.1207, -0.4114, -0.1927, -0.1207,  1.6640, -0.9278, -0.2450,\n",
       "          -0.2031,  2.4545,  2.5475, -0.1238, -0.1902, -0.2163, -1.6442,\n",
       "           2.3100, -0.1205, -0.2013, -0.1725, -0.0164, -0.1396,  0.1298,\n",
       "          -0.2031]]),\n",
       " tensor([[-0.5795,  0.4265, -0.1332, -0.0767,  0.6891, -0.0490, -0.1249,\n",
       "          -0.2063, -0.1638, -0.1206, -0.1207, -0.1213, -0.1206,  2.4140,\n",
       "          -0.1258, -0.1207,  0.9103, -0.5266, -0.3110, -1.2243, -0.3436,\n",
       "           0.6739,  0.3952, -0.2015,  2.3957, -0.6541, -0.5753,  0.2015,\n",
       "           1.7313]]),\n",
       " tensor([[-0.1263, -0.2031, -0.4558,  2.3984, -0.1358, -0.5753,  1.6848,\n",
       "           1.2231, -0.2025, -0.1235, -0.1215, -0.1854, -0.1868, -0.1258,\n",
       "          -0.1560, -0.2221, -0.1432,  0.9527, -0.1921, -0.4250,  2.2528,\n",
       "           0.3727, -0.1609, -0.5039, -0.1219,  0.1928, -0.1207,  1.7322,\n",
       "           0.2866]]),\n",
       " tensor([[-0.2205,  1.2192,  1.9232,  2.4182, -0.0965, -0.2093, -0.2022,\n",
       "          -0.1256, -0.1675, -0.2557, -0.2094, -0.1397, -0.1659, -0.2066,\n",
       "           2.2730, -2.0295, -0.1270,  1.8689, -0.3067,  1.7533, -0.1270,\n",
       "          -0.5770, -0.2086, -0.1879,  2.0283,  2.5375,  0.3195,  0.2754,\n",
       "          -0.1869]]),\n",
       " tensor([[-0.5310,  0.9067,  0.9049,  2.2088,  2.7970, -0.5432, -0.1299,\n",
       "           1.5101, -0.1299,  0.5585, -0.5855, -0.1517, -0.2007, -0.1299,\n",
       "          -0.5505,  0.2082, -0.2551,  2.1739, -0.1314,  2.1800,  1.8440,\n",
       "          -0.2705, -0.2105, -0.5943, -0.1299, -1.1850,  0.6756, -0.1717,\n",
       "          -0.7930]]),\n",
       " tensor([[ 0.9478,  1.4078, -0.2694, -0.1290, -0.1331, -0.4491,  1.8688,\n",
       "          -0.1453, -0.1750, -0.1970, -0.7242, -0.5846,  0.4177, -0.1294,\n",
       "           0.0258,  0.4583, -0.1315, -0.5647,  2.7743,  2.2809, -0.2076,\n",
       "          -0.2115, -0.1687, -0.1368, -0.5506, -0.1284, -0.2106, -0.5841,\n",
       "          -0.3308]]),\n",
       " tensor([[-0.2033, -2.3668, -0.5539, -0.2109, -0.0231,  0.3850, -0.0738,\n",
       "          -0.1283, -0.1274, -0.6828, -0.1645,  0.0731,  0.9556, -0.1276,\n",
       "           0.5532,  1.2696, -0.1276, -0.6795, -0.5805, -1.0026,  0.0421,\n",
       "          -0.2109,  0.6132, -0.2101,  1.2879, -1.3251, -0.1276, -0.6558,\n",
       "          -0.1298]]),\n",
       " tensor([[-0.3274, -0.1356, -0.0173, -0.2425, -0.2111,  0.8411, -0.3277,\n",
       "           0.1988, -0.2168,  2.1516,  1.1501, -1.1090,  2.0229, -0.2111,\n",
       "           0.7081, -0.2057,  1.8710,  3.3228, -0.1272, -0.1274, -0.2106,\n",
       "          -0.5732, -0.4175, -0.2236, -0.4603, -0.4355, -0.1272, -0.1272,\n",
       "          -0.2105]]),\n",
       " tensor([[-0.4869, -0.5813,  1.5754, -0.8070, -0.5758, -0.2111, -0.1277,\n",
       "          -0.1472,  0.6598,  2.5370, -0.5819,  2.5263,  1.9073, -0.1277,\n",
       "           1.5458, -0.2109, -0.1299,  2.5237,  2.4825, -0.1278, -0.1718,\n",
       "          -0.1720, -0.5467, -1.1627,  2.2341, -0.2100, -0.2123, -0.1946,\n",
       "          -0.1281]]),\n",
       " tensor([[ 1.5622, -0.1280, -0.1281, -0.2611,  2.0015, -0.3943, -0.3708,\n",
       "           2.2205, -0.2105, -0.0452, -0.1439, -0.5789,  0.7174, -0.1810,\n",
       "          -0.1764, -0.1280, -0.3551, -0.2114,  0.8341, -0.3570,  0.2181,\n",
       "          -0.2152,  1.9455, -0.1909,  2.8251,  0.4636,  1.8669, -0.1280,\n",
       "           1.6857]]),\n",
       " tensor([[-0.1284, -0.1320,  0.9527, -0.2115,  3.0942,  2.2172, -0.0791,\n",
       "          -0.5215, -0.1465, -0.1471, -0.2935, -1.0869, -0.1286,  2.9729,\n",
       "           0.7302, -0.2186, -0.1468, -0.1794,  0.1299, -0.1288, -0.1289,\n",
       "           0.7193,  0.7151, -0.1399, -0.0046,  1.9172, -0.4027, -0.3172,\n",
       "          -0.1442]]),\n",
       " tensor([[-0.5423, -0.2032, -0.2281, -0.1290,  0.7652, -0.1318, -1.0749,\n",
       "          -0.2115, -0.1362,  2.4790, -0.1284, -0.1937, -1.9928, -0.2206,\n",
       "           0.0774, -0.2390,  2.8646, -0.1966, -0.4222, -0.1241, -0.5465,\n",
       "          -0.1282, -0.2116, -0.1270,  1.9457,  0.1527, -0.1209, -0.1944,\n",
       "          -0.2116]]),\n",
       " tensor([[-0.1283, -0.3158, -0.1118, -0.0422, -0.1845, -0.1790,  2.3328,\n",
       "           2.8907,  2.9269, -0.2116, -0.1284, -0.1283, -0.1924, -0.1283,\n",
       "          -0.2085,  0.1308, -0.5642,  1.7286, -0.2232,  1.4736, -0.1283,\n",
       "           0.0403, -0.3186,  3.3963, -0.2081, -0.1631, -1.0044, -0.1283,\n",
       "           2.3089]]),\n",
       " tensor([[-0.2143,  2.6427, -0.0369,  2.3796,  2.7499, -0.2111,  0.6774,\n",
       "           0.7634,  1.1700, -0.0448, -0.2005, -0.2101, -0.2103, -0.2918,\n",
       "          -0.2007,  2.8567, -0.1292, -0.0484, -0.2446, -0.1344, -0.2116,\n",
       "          -0.1171, -0.1288, -0.1283,  1.8108, -0.5856,  3.1461, -0.1562,\n",
       "          -0.2859]]),\n",
       " tensor([[-0.1893, -0.1414,  0.5336, -0.1283, -0.2275, -0.1412, -0.2228,\n",
       "          -0.2116,  3.4479, -0.6860, -0.5845, -0.2105, -1.1436,  0.5867,\n",
       "          -0.1283, -0.5856, -0.3636, -0.2962,  1.1989, -0.0441, -0.1298,\n",
       "          -0.7481, -0.2077, -0.8446,  1.0025, -0.5842,  1.7314, -0.1283,\n",
       "          -0.6033]]),\n",
       " tensor([[-0.1276,  2.6243, -0.6412, -0.1479, -0.2022,  2.3430,  1.3388,\n",
       "          -0.2080, -2.0924,  1.7067,  0.0714, -0.1317, -0.2191, -0.2294,\n",
       "          -1.4016, -0.2840, -0.2109, -0.4774,  3.0244, -0.2216,  0.4686,\n",
       "          -0.5154, -0.7405, -0.1323, -0.1276,  0.1351,  1.0368, -0.1364,\n",
       "          -0.4707]]),\n",
       " tensor([[-1.3315,  2.3989,  0.0161,  0.7981, -0.2360, -0.1376, -0.1640,\n",
       "           0.0635, -0.2107, -0.5853,  0.0653, -0.1534, -0.2099, -0.2727,\n",
       "          -0.2041,  1.6597,  0.0871, -0.3152, -0.1915, -0.1302, -0.8638,\n",
       "          -0.2120, -0.5981, -0.1678,  0.0335,  0.0449, -0.3520, -0.1274,\n",
       "          -0.1925]]),\n",
       " tensor([[-0.1336, -0.2052, -0.2094,  2.7041, -0.2105, -0.1271, -0.2102,\n",
       "          -0.1667, -1.6427, -0.2097, -0.1519, -1.6276, -0.1995,  2.1199,\n",
       "          -1.8106, -0.2039, -0.1272, -0.2123, -0.1271, -0.0270, -0.1293,\n",
       "           2.5253, -0.5690,  0.4454, -0.2057, -0.5422, -0.1361, -0.2105,\n",
       "          -0.1896]]),\n",
       " tensor([[-0.1270,  0.4003,  1.6631, -0.1609, -0.1948, -0.1270, -1.2800,\n",
       "           2.6834, -0.1270, -0.2084,  1.0270,  0.1197, -0.2013,  0.1895,\n",
       "          -0.1874, -0.2104, -0.1513,  2.5518, -0.2103, -0.5940, -0.2141,\n",
       "           0.0831, -0.1965, -0.1642, -0.1271, -0.1746,  1.3025, -0.2148,\n",
       "          -1.5169]]),\n",
       " tensor([[-0.2378, -0.2104, -0.1313, -0.1269, -0.7034,  0.2808, -0.4491,\n",
       "          -0.5469,  2.9964, -0.1269,  1.5526, -0.2103, -0.2098, -0.3646,\n",
       "          -0.2077, -0.1269, -0.2103, -0.3641,  0.5226, -0.1274,  0.2864,\n",
       "          -0.2078, -1.2535, -0.1269, -0.5841,  0.2078, -0.1081, -0.1270,\n",
       "           0.7228]]),\n",
       " tensor([[ 0.4789,  2.6225,  0.8352, -1.6774, -0.1269, -0.5932, -0.1365,\n",
       "          -0.4717,  2.4651, -0.1269, -0.1269, -0.2011, -0.1294, -0.2096,\n",
       "          -0.1528,  0.3794, -0.5841, -0.3209,  0.1571, -0.1464,  2.3900,\n",
       "          -0.4917, -0.2018, -0.3428, -0.1371,  0.3838, -0.3171, -0.2104,\n",
       "          -0.1269]]),\n",
       " tensor([[ 3.1685, -0.1304, -0.1833,  2.6081,  1.5039, -0.1805, -0.3335,\n",
       "           0.7198,  1.3769, -0.1510, -0.2097,  0.8071, -0.1276,  2.1581,\n",
       "          -1.3517, -1.7997,  2.8149,  1.5311, -0.1270,  2.1619,  0.8550,\n",
       "          -0.5345, -0.1506, -0.2097, -0.2105, -0.2072, -0.1640, -0.0621,\n",
       "           2.1264]]),\n",
       " tensor([[-0.2110,  0.6707,  2.0353, -0.2102, -0.3792,  3.6745, -0.2102,\n",
       "          -0.1260, -0.1380, -0.3215, -0.7427, -0.1374, -0.1301, -0.1098,\n",
       "          -0.2104, -0.0055,  2.3536,  0.5204, -0.1269, -0.1854,  0.3590,\n",
       "           2.1768, -0.2097, -0.2103, -0.0260, -0.1947, -0.5419, -0.2616,\n",
       "          -0.1895]]),\n",
       " tensor([[-0.2103, -0.3435, -0.2380, -0.2101, -0.1127,  2.4958, -0.1352,\n",
       "          -1.5133,  1.2634, -0.2175, -0.1688, -0.2724, -0.7364, -0.1281,\n",
       "           2.5782, -0.1270, -1.6560,  2.0917, -0.2872, -0.1269, -0.5270,\n",
       "           0.0906, -0.5434, -0.1983,  2.8353,  0.4949,  3.0651, -0.2081,\n",
       "          -0.2081]]),\n",
       " tensor([[-1.0013, -0.1318, -0.2123, -0.2900,  1.3191, -0.5538,  0.0793,\n",
       "          -1.0077, -0.2122,  2.3429,  3.0618,  2.3684,  1.7513, -0.1296,\n",
       "          -0.3016, -0.2137, -0.2245,  1.5166, -0.5091,  1.4884,  2.0658,\n",
       "           2.6541, -0.1322, -0.1289, -0.1290,  0.1200, -0.4617,  0.4013,\n",
       "           0.1538]]),\n",
       " tensor([[-0.0485, -0.2775,  0.2628, -0.1544, -0.3451,  1.9260, -0.2127,\n",
       "          -0.7370,  1.3219, -0.0204, -0.1301, -0.2100, -0.2980, -0.1300,\n",
       "          -0.5576, -0.3670, -0.2559,  0.2742, -0.5861,  2.0379, -0.1296,\n",
       "          -0.1409, -1.4775, -0.1299, -0.1701, -0.2132, -0.1372,  2.1412,\n",
       "          -1.4948]]),\n",
       " tensor([[-0.1640, -0.2131, -0.3966,  0.4915, -0.1514, -0.2133, -1.0549,\n",
       "           0.1294, -0.1314, -1.0711,  3.7160, -0.2099, -2.1303,  3.4633,\n",
       "          -0.3139, -0.1329, -0.0483, -0.0230, -0.2138, -0.1305, -0.2214,\n",
       "          -0.1316,  2.4885, -0.5229,  2.9799, -0.2336, -0.5139,  2.3205,\n",
       "          -0.2144]]),\n",
       " tensor([[-0.1341,  1.6882, -0.1307, -1.2903, -1.6216, -0.2140,  3.2577,\n",
       "          -0.1306, -0.5876, -0.1288, -0.1306, -0.4159, -0.1349,  1.4092,\n",
       "          -0.2282,  1.0858,  1.3526,  0.2505,  2.9422, -0.6820, -0.5364,\n",
       "           0.1045, -0.2185, -0.1395,  0.3774, -0.2143,  1.2399,  1.0519,\n",
       "          -0.1482]]),\n",
       " tensor([[-0.1383,  3.1234, -0.2075, -0.0049, -1.8249, -0.1871, -0.1309,\n",
       "          -0.1313, -1.4499, -0.1307, -0.2168,  1.6771,  2.0317, -0.1376,\n",
       "          -0.2140, -0.1649, -0.7641,  1.9568,  2.2491,  2.3300, -0.5435,\n",
       "           0.6872,  1.4181,  1.6828, -0.1974,  1.7259, -0.4445, -0.0654,\n",
       "          -0.5877]]),\n",
       " tensor([[-1.0444, -0.2133,  2.9747,  0.7865,  2.5749, -0.1309,  2.7996,\n",
       "          -0.0878, -0.1733,  0.5094, -0.2142,  0.6072, -0.9013, -0.1307,\n",
       "          -0.6456,  2.1969, -0.5851, -0.5830, -0.4337, -0.1380, -0.4567,\n",
       "          -0.1318,  2.3399, -0.2142,  2.0496, -0.1486, -0.0458,  1.1572,\n",
       "          -0.1314]]),\n",
       " tensor([[-0.1309, -0.1366,  0.0351, -0.1309, -0.1309, -0.5881, -0.1314,\n",
       "          -0.1309,  1.3457, -2.0895,  0.3556,  0.7222, -0.4116, -0.6984,\n",
       "          -0.1856, -0.6500, -0.1308, -0.2130, -0.5903, -0.1312, -1.2818,\n",
       "           0.0452,  1.1762, -2.0496, -1.2994,  0.2141,  0.6203, -0.4400,\n",
       "          -0.3960]]),\n",
       " tensor([[-0.1406,  0.0859, -0.2088,  2.1448, -0.3586,  2.7286, -0.3609,\n",
       "           0.6181, -0.5879,  3.0746, -0.1309, -1.1958,  2.1351,  0.4091,\n",
       "          -0.1312, -0.2316,  0.1847, -0.4482, -0.2214,  0.5287, -1.7843,\n",
       "          -0.2135, -0.1348,  1.2361,  0.1306,  2.1047, -0.4174, -0.1560,\n",
       "           3.2766]]),\n",
       " tensor([[-0.2117, -0.4859, -0.2110, -0.2141, -1.1480, -0.2136, -0.8688,\n",
       "          -0.4599, -0.3949, -0.2029, -0.3200, -0.0934, -0.1309, -1.6171,\n",
       "          -0.1309, -0.1882, -0.2088,  0.7510, -0.2119,  2.8955, -0.0176,\n",
       "           3.0558, -0.1917, -0.2128, -0.1311, -0.2410, -0.1404,  2.1128,\n",
       "           3.9695]]),\n",
       " tensor([[-0.5430, -0.2126,  0.4729, -1.0522, -0.5973, -0.2138, -0.1327,\n",
       "          -0.9617,  0.7067, -0.1481,  0.3346, -1.5546, -0.1619,  2.6345,\n",
       "           0.7723, -0.1668, -0.1576, -0.2120, -0.1213, -0.2992, -0.1315,\n",
       "          -0.1316, -0.0529,  1.9277,  1.9176, -0.0027, -0.1898, -0.1319,\n",
       "          -0.2013]]),\n",
       " tensor([[-0.1453,  1.7703, -0.2151,  2.3297,  2.3691,  2.4121, -0.1320,\n",
       "          -0.3859,  2.0299, -0.1319,  2.3099, -0.1433, -0.2146, -0.5807,\n",
       "           0.3980, -0.1822,  2.0769,  3.3195, -0.2067, -0.1755, -0.0726,\n",
       "          -0.0552, -1.3350, -0.2139, -0.2170, -0.2898, -0.6842,  2.5506,\n",
       "          -0.2100]]),\n",
       " tensor([[-0.3918, -0.0765, -1.5100,  2.4748,  2.6857, -0.1481,  0.6492,\n",
       "          -0.1992, -0.3156, -0.5271, -0.1322, -0.1326, -0.2157, -0.5839,\n",
       "           0.3809, -0.4560, -0.1320, -0.2877, -0.2452,  1.9730,  0.3795,\n",
       "          -0.1873, -0.5223, -0.1359, -0.1320, -0.1322,  2.4675, -0.1319,\n",
       "          -0.4995]]),\n",
       " tensor([[ 0.4749,  0.4503, -0.1320, -0.1728, -0.0702, -0.2487, -0.1493,\n",
       "           3.0680, -0.1068, -0.2152,  2.6319,  0.1931,  2.1670, -0.2154,\n",
       "          -0.1320, -0.1342, -0.1715,  0.8335, -0.1320, -0.1321, -0.0447,\n",
       "          -0.1322, -0.5579, -0.4097, -0.1323,  3.0644, -0.2522, -0.2141,\n",
       "          -0.7005]]),\n",
       " tensor([[-0.1321, -0.2146,  1.2190, -1.6877,  2.7248, -0.6462, -0.3106,\n",
       "          -0.5509,  0.2254, -0.1742, -0.2155,  0.2618, -0.2211, -0.2169,\n",
       "          -0.2155,  1.7823, -0.5562,  1.5401, -0.5589, -0.8751, -0.1405,\n",
       "           2.1475, -0.2155, -0.2145,  2.8086, -0.2652, -0.1320,  1.1124,\n",
       "          -0.1320]]),\n",
       " tensor([[-0.0958, -0.1325,  2.7718, -0.1320, -0.3071, -0.1334,  0.2691,\n",
       "          -0.1304, -0.1513, -2.6402, -0.1321, -0.1321,  1.1172, -0.1336,\n",
       "          -1.3225, -0.4165,  1.1163, -0.5552,  0.5120, -0.1330, -0.1499,\n",
       "          -0.1321,  2.3012,  0.0452, -0.5242, -0.1350, -0.1437, -0.1527,\n",
       "          -0.0130]]),\n",
       " tensor([[ 3.0648, -0.1321, -0.1323, -0.2157, -0.1096, -0.3620, -0.2247,\n",
       "          -0.0552,  2.2640, -0.6856,  0.2265, -0.2154, -0.5578,  0.7108,\n",
       "          -0.7770, -0.2140, -0.5896, -2.2717, -1.2196, -0.1392, -0.4756,\n",
       "           2.4233, -0.5869, -0.4059,  2.0095, -0.1307, -0.2156, -0.5646,\n",
       "          -0.2156]]),\n",
       " tensor([[ 2.4971, -0.2156, -0.1321,  1.7815, -0.1321, -0.1420,  0.7683,\n",
       "          -0.2247, -0.5463, -0.1557, -0.1321,  0.8868, -0.1977,  1.7800,\n",
       "           1.0892, -0.5891, -0.7138, -1.7568, -0.1330,  2.1943, -0.2140,\n",
       "           0.8851,  0.0315, -0.2153, -0.3807, -0.1323, -0.2155, -0.1343,\n",
       "          -0.1417]]),\n",
       " tensor([[-0.5706, -0.2156, -0.5890, -0.1311, -2.3031, -0.1728, -0.1323,\n",
       "           0.1000, -0.2158,  1.3061, -0.4149,  2.6337, -0.2156, -0.1604,\n",
       "           0.2774, -1.4050,  2.1072,  2.5090, -0.2156, -0.1322,  2.0462,\n",
       "           0.9203, -0.1330, -1.3101,  1.3981, -0.5396, -0.4800, -0.1321,\n",
       "          -0.1001]]),\n",
       " tensor([[-0.5736, -0.2372, -0.2585, -0.1405,  2.1235, -0.9420, -0.1340,\n",
       "          -0.0452,  2.1684, -0.4378, -0.1321, -0.2154,  0.3200, -0.1277,\n",
       "          -0.2019,  1.9583,  0.0114, -0.2156, -0.9236,  1.7302, -0.5004,\n",
       "          -1.6702,  1.8423, -1.5004, -0.1325,  0.9515, -0.2091,  2.6027,\n",
       "          -0.4037]]),\n",
       " tensor([[ 2.3326, -0.1321,  0.8174,  2.1091, -0.1266, -0.1466,  0.0397,\n",
       "          -0.1574,  1.7602, -0.1322, -0.3752, -0.2142, -0.2054, -0.4362,\n",
       "          -0.2023, -0.1321,  0.0439,  1.8654, -0.2156, -1.3703,  0.1565,\n",
       "          -0.0357, -0.1321,  0.1781,  1.3931, -0.6369, -0.1987, -0.5378,\n",
       "          -0.6391]]),\n",
       " tensor([[-0.5368,  1.3082, -0.1342, -0.2167, -0.1321, -0.1321, -0.2083,\n",
       "          -0.1770, -0.6315, -0.1987, -0.2156, -1.3866, -0.1780, -0.1936,\n",
       "           2.7257, -0.9728,  0.0780,  1.8342, -0.1321, -0.1323, -0.3341,\n",
       "          -0.7556, -0.4508, -0.1401, -0.2177, -0.2079, -0.1324,  0.7786,\n",
       "          -0.1550]]),\n",
       " tensor([[-0.1443, -0.1321, -0.2078, -0.1884,  0.3509, -0.2662, -0.1568,\n",
       "          -0.1476,  2.1076, -0.1450,  0.4010,  0.1626, -0.6163,  2.7832,\n",
       "          -0.2155, -0.1321, -0.1704, -0.1321, -0.2086,  1.9384, -0.1482,\n",
       "           2.3028, -0.2422,  3.7036, -0.8079, -0.1324, -0.1701, -0.2152,\n",
       "          -0.0465]]),\n",
       " tensor([[-0.5311,  0.5056, -0.4992,  1.4239, -0.1320, -0.3878,  2.8481,\n",
       "          -0.2257,  2.2738, -0.2155, -0.2116, -0.2150, -0.1913, -0.1321,\n",
       "          -0.2109, -0.4998,  0.1759,  1.1443, -0.0998, -0.3875, -0.3336,\n",
       "          -0.0721, -0.1740, -0.5047, -0.1492, -0.1331, -0.6335,  0.2722,\n",
       "           2.0893]]),\n",
       " tensor([[-0.2472,  1.0754,  0.6232, -0.1315, -0.2147, -0.1480, -0.0195,\n",
       "           1.8214, -0.2142,  2.5559, -0.2127, -0.1551, -0.1801, -0.1315,\n",
       "          -1.5969,  2.3163, -0.1324, -0.1315, -0.1386, -1.8889, -0.1360,\n",
       "          -0.1315, -0.1378, -2.4574, -0.5308, -0.1315, -0.2114, -0.1315,\n",
       "          -0.2149]]),\n",
       " tensor([[-0.1349, -0.1319,  1.4818,  0.7431, -0.1311, -0.2558, -0.1312,\n",
       "          -0.2124, -0.1899, -0.2220, -0.5777, -0.1994,  0.1176,  0.5429,\n",
       "          -0.5153, -0.2131,  0.3788, -0.6243, -0.1377, -0.4335,  1.8559,\n",
       "           2.4447, -0.1312, -0.1907,  3.0285, -0.4799, -0.0519, -0.5887,\n",
       "           2.0651]]),\n",
       " tensor([[-0.1377,  1.9032, -0.2143, -0.6123,  0.5711,  0.4058, -0.5836,\n",
       "          -0.1310,  0.0313, -0.1321, -0.4333, -0.2121, -0.1533, -0.2145,\n",
       "           0.2551, -0.2144, -0.2133,  1.9725, -0.2137, -0.5834, -0.4427,\n",
       "           2.0817, -0.1950, -0.1647, -0.4737,  0.8938, -0.1310,  1.0410,\n",
       "          -0.0331]]),\n",
       " tensor([[-0.7923, -0.1313, -1.0191, -0.2084, -0.1383,  0.0408, -0.1311,\n",
       "          -0.1610,  2.3465, -0.2137,  1.0676, -0.1302, -0.1469,  1.4129,\n",
       "          -0.3929, -0.2144, -0.1311, -0.1308, -0.2131, -0.5407, -0.1317,\n",
       "          -0.1334, -0.5139,  3.4370,  0.2379, -0.0695, -0.1999,  0.1351,\n",
       "          -0.2138]]),\n",
       " tensor([[-0.1310, -0.5519, -0.4350, -0.1317,  0.3085, -0.1566,  0.0112,\n",
       "          -0.4775, -0.2143,  3.0013, -1.7415,  0.9612, -0.1216, -1.3293,\n",
       "          -0.2128, -0.5609,  2.4641, -0.1648, -0.1610, -0.5787,  0.1502,\n",
       "           2.2341, -0.7962, -0.1309,  0.4658, -0.1328, -0.2115, -0.2116,\n",
       "          -0.1309]]),\n",
       " tensor([[-0.2122, -0.3657, -0.2046, -1.5281,  1.2443, -0.4811,  1.2190,\n",
       "           0.2734, -0.1809, -0.1933, -1.0606, -0.1308,  2.5284, -0.6896,\n",
       "          -0.6344, -0.1491, -0.3081,  0.2462,  2.3869,  1.7633, -0.2737,\n",
       "           2.0742, -0.3974,  0.2149, -0.2143, -0.1036, -0.0983, -0.1316,\n",
       "           0.6423]]),\n",
       " tensor([[ 0.9167, -0.2293,  2.3185, -0.7544, -0.4864,  2.5630, -0.2142,\n",
       "          -0.1310, -0.2139, -0.2143, -0.0234, -0.8754, -0.2108, -0.5880,\n",
       "          -0.1959, -0.2106, -0.1896, -0.1363,  2.3559, -0.2000, -0.1308,\n",
       "          -0.2182,  0.6043, -0.1327, -0.1313,  1.3409,  1.8543,  2.0156,\n",
       "           1.8920]]),\n",
       " tensor([[-0.5463, -0.5389, -0.1317, -0.2139, -0.2109, -0.5192,  3.2696,\n",
       "          -0.2143,  2.2669,  2.1151, -0.4489,  2.4085, -0.5871, -0.1641,\n",
       "          -0.1872, -0.5504,  0.2776,  0.3378, -0.1308,  1.7001, -0.2137,\n",
       "          -0.1308, -0.2144, -0.1310, -0.0662,  1.6589, -0.1399,  2.3256,\n",
       "           1.4123]]),\n",
       " tensor([[ 0.3908, -0.1306, -0.2390, -0.1370, -0.1689, -0.2228,  2.7687,\n",
       "          -0.1308, -0.1327, -0.1444,  0.0083, -0.2142,  2.7285,  0.2930,\n",
       "          -0.1373, -0.0783,  1.6955,  2.2687, -0.1956,  0.7556,  2.8265,\n",
       "          -0.1308, -0.6265,  0.1132,  2.7894, -0.1498, -0.2142, -0.1309,\n",
       "           0.2776]]),\n",
       " tensor([[-0.4133,  0.1758, -0.1308, -0.1308,  1.0548, -0.5543, -0.1308,\n",
       "          -0.1322, -0.1309,  0.1203, -0.9152, -0.2188, -0.5433,  0.6007,\n",
       "          -0.6115, -0.4592, -0.2984, -0.5884, -0.4096, -0.2143,  0.2584,\n",
       "           2.4612, -0.0549, -0.2428, -0.2143, -0.1427, -0.1368, -0.2135,\n",
       "          -0.3744]]),\n",
       " tensor([[-1.3264,  2.5274, -0.2142, -0.2137, -0.1074,  2.8699,  0.2320,\n",
       "           2.2768,  2.3135, -0.2561, -0.1310, -0.3195, -0.1308,  1.8964,\n",
       "          -2.1949, -0.1338, -0.2143, -0.2141, -0.1520, -2.2018,  1.9858,\n",
       "          -0.8245,  2.8049, -0.2028, -0.1348, -0.1372, -0.2143, -0.1308,\n",
       "          -2.5934]]),\n",
       " tensor([[-0.5883,  0.8168,  0.8723, -0.8559,  1.2917, -0.2143, -0.1528,\n",
       "          -1.2823,  1.6458, -0.6507, -0.0213,  1.9213, -0.0863, -0.2135,\n",
       "           0.3469,  0.2085,  0.0210, -0.3709, -1.6298, -0.2076, -0.0479,\n",
       "           1.8307, -0.2142, -0.2143, -0.5883, -1.1037, -0.1310, -0.5516,\n",
       "           3.1253]]),\n",
       " tensor([[-0.2143, -0.0535, -0.1325, -0.1948,  0.2524, -0.1332, -0.1841,\n",
       "          -0.1358, -0.0752, -0.2135, -0.1308, -0.3943, -0.5766, -0.0721,\n",
       "          -0.1320, -0.1407, -0.1310,  1.5070, -0.0727, -0.1755, -2.6575,\n",
       "           2.9186,  0.1348, -0.1308, -0.1168,  0.5250, -0.2143, -0.5542,\n",
       "           0.7722]]),\n",
       " tensor([[-0.5690, -0.0692, -0.1353, -0.1428, -0.2142, -0.2139, -0.1308,\n",
       "          -0.1745, -0.2143, -0.1254, -0.1577,  3.1222,  0.5685,  2.1462,\n",
       "           3.0490,  2.8272, -0.1418,  0.3460, -0.1346, -0.2142, -0.1396,\n",
       "           0.8660, -0.1309, -0.2122, -0.0888, -0.8200,  0.8150,  0.2976,\n",
       "           2.0273]]),\n",
       " tensor([[-0.1308, -0.2128, -0.1538, -0.0911, -0.1309, -0.2182, -0.2871,\n",
       "          -0.1459, -0.5568,  0.9560,  1.1485, -0.6298, -0.0160, -0.5839,\n",
       "          -0.1936, -0.1308, -0.3612, -0.5882, -1.5403, -0.2268, -1.0158,\n",
       "          -0.1465, -0.1107, -0.2346,  2.3756, -0.2737, -0.8525,  2.5264,\n",
       "           2.6668]]),\n",
       " tensor([[-0.2143, -0.6359, -0.1810, -0.2065, -0.0342,  3.3304, -0.4827,\n",
       "          -0.2644, -0.1309,  1.2603,  1.8955,  0.8805,  2.8260,  2.8524,\n",
       "          -0.1308, -0.3429,  3.0258, -0.2142, -0.2133, -0.1321,  0.4066,\n",
       "          -0.1370, -0.1243, -0.1359,  0.2513,  2.2504, -0.3400, -0.1675,\n",
       "           0.0619]]),\n",
       " tensor([[ 0.4072, -1.1603, -0.1363,  2.2048,  0.9741, -0.1777,  0.5869,\n",
       "          -0.4459, -0.6379,  3.5515, -0.6277,  3.4726,  2.0327, -1.0661,\n",
       "          -0.1281, -0.2157, -0.1308,  2.4209, -0.4716,  2.7029, -1.2804,\n",
       "          -0.4195,  0.2899, -0.2142, -0.4595,  2.9370, -0.1309, -0.3476,\n",
       "           2.5601]]),\n",
       " tensor([[-0.8789,  1.0259, -0.4195, -0.4717, -0.1934, -2.0391, -0.1339,\n",
       "          -0.1308, -0.3030,  0.7502, -0.4726, -0.1308, -0.2137,  0.2529,\n",
       "          -0.2131,  2.2087, -0.1310, -0.1308, -0.0627,  2.3720, -0.2976,\n",
       "          -0.1399, -0.5667, -0.5867, -0.1312, -0.5886, -0.2137, -0.1565,\n",
       "          -0.2131]]),\n",
       " tensor([[ 0.0677, -0.0240, -0.2211,  0.5475, -0.2172, -0.2143,  2.7376,\n",
       "          -1.2711, -0.7382, -0.6320, -0.1426,  1.7811, -0.2037, -0.1778,\n",
       "           1.3149, -0.1323, -0.2143,  2.2719, -0.1325, -0.2059, -0.1309,\n",
       "          -0.1333, -1.0566, -0.1427, -0.1314, -0.1317,  2.2076, -0.1767,\n",
       "          -0.4154]]),\n",
       " tensor([[ 2.0477, -0.8361, -0.1308, -0.1288, -0.2143, -0.2144, -0.2145,\n",
       "           2.2506,  1.9961, -0.2287,  2.2789, -0.2166, -0.0714,  0.6056,\n",
       "          -0.2143, -0.5733, -0.2166, -0.1875, -0.1308, -0.2346, -0.1853,\n",
       "          -0.5504, -0.2058,  1.8925, -0.5162, -0.1273,  2.4846, -0.6049,\n",
       "          -0.2143]]),\n",
       " tensor([[-0.4128, -0.2250, -0.2431,  1.4879, -0.1308, -0.1309,  0.4395,\n",
       "          -0.1309, -0.7075,  0.0315, -0.1308, -0.1406, -0.1966, -0.1308,\n",
       "          -0.4031, -0.4556, -0.1299,  2.5414, -0.2464,  2.7192, -0.1668,\n",
       "          -0.1282,  0.3701, -0.2142,  2.5253,  0.9593, -0.5252, -2.1482,\n",
       "          -0.4912]]),\n",
       " tensor([[ 0.7574,  0.4556, -0.2141, -1.9588, -0.5805, -0.6681, -0.1516,\n",
       "          -0.2142, -0.1310, -0.4444,  1.9278, -0.1312, -0.1309,  1.1148,\n",
       "           1.8186, -0.5882,  3.0208,  2.1016, -0.2141,  0.8569,  2.6048,\n",
       "          -0.2140, -0.1308, -1.3294,  2.5562,  1.5243, -0.1056, -0.4052,\n",
       "           1.5519]]),\n",
       " tensor([[-0.1343,  1.3442, -0.0995, -0.2132, -0.2143, -0.1308, -0.1310,\n",
       "          -0.0399, -1.3238, -0.2139, -0.2143,  0.3115, -0.2198, -0.1311,\n",
       "           2.4724,  2.5456,  0.7830,  1.8582, -0.2103, -1.9472,  2.0809,\n",
       "          -0.0957, -0.1609, -0.2143,  2.2996, -0.1309, -0.2090, -0.2177,\n",
       "          -0.1421]]),\n",
       " tensor([[ 0.7865, -0.2542, -0.2128, -0.2004, -0.1481, -0.6210, -0.5421,\n",
       "          -0.5791,  2.6607,  1.1624, -0.5861, -0.6079, -1.3289, -0.1313,\n",
       "          -0.0859, -0.1490, -0.1295, -0.2108, -1.0222,  2.7183,  2.2626,\n",
       "          -1.3249, -1.8655, -0.1930,  0.2642, -1.5892, -0.5199,  2.6965,\n",
       "          -0.2418]]),\n",
       " tensor([[ 0.8570, -0.6197, -0.1289,  0.5239, -0.5488,  1.1326, -1.1696,\n",
       "          -0.2104,  0.1357, -0.2140, -0.2122,  0.2901, -0.1354,  0.7912,\n",
       "           2.1297,  1.2582, -0.2107, -0.2116, -0.1289, -0.2118, -0.1289,\n",
       "           0.7582, -0.2229,  1.6200,  0.0484,  3.0029, -0.2365, -0.1299,\n",
       "           0.4909]]),\n",
       " tensor([[-0.3503, -0.2116,  0.7249, -0.1785,  0.3986,  1.0306, -0.1286,\n",
       "          -0.0407, -0.4461,  0.2724,  1.1277,  0.0792,  0.0902, -0.3194,\n",
       "          -0.1290,  2.4902,  2.0265,  2.8340, -0.2115,  0.6733, -0.1411,\n",
       "           0.6175, -0.1729, -0.1287,  1.8418, -0.1286, -0.2040,  0.4762,\n",
       "          -0.2103]]),\n",
       " tensor([[-0.2085, -0.2085, -0.5704, -0.1291, -0.1477, -0.1258,  0.7871,\n",
       "          -0.0349, -0.5819, -0.1261, -0.1255, -0.1833,  1.7223, -0.1256,\n",
       "           2.7726, -0.0132, -0.1212, -0.1357, -0.2102, -0.0591, -0.1094,\n",
       "          -0.5876, -1.6871, -0.5795, -0.1848, -0.1255, -0.2008, -0.5292,\n",
       "           2.8272]]),\n",
       " tensor([[ 2.7667, -0.3329, -1.7262,  2.8980, -0.5627,  1.4702, -0.5353,\n",
       "          -0.2069,  1.9602,  2.2723, -0.2069, -0.1915, -0.2985, -0.2068,\n",
       "           1.3901, -0.1240, -2.0330, -0.1241,  0.7657, -0.2736, -0.2069,\n",
       "           2.4585,  0.6804, -0.2492,  2.5478,  1.9384,  2.5772, -0.5686,\n",
       "           2.2284]]),\n",
       " tensor([[-0.1557, -0.1234, -0.5854, -0.0613, -0.5237, -0.1493, -0.2696,\n",
       "          -0.1232, -0.5134, -0.1234, -0.1232, -0.1295,  2.3673, -0.2060,\n",
       "           2.4956,  2.5454, -0.1290, -0.1850, -0.5513,  1.7796, -0.1237,\n",
       "          -0.5378, -0.2060, -0.1828, -0.2605, -0.1217,  0.8155, -0.5754,\n",
       "          -0.2030]]),\n",
       " tensor([[-0.6679, -0.1975, -1.7454,  0.0997,  0.5071,  0.8733, -0.5395,\n",
       "           1.5439,  1.2795, -0.2267, -0.4523,  1.7641, -2.5665, -0.6245,\n",
       "          -0.1228,  0.5481, -1.1916, -0.2102, -0.1962,  0.0893, -0.1240,\n",
       "          -0.1229, -1.4117, -1.4630, -2.0539, -0.1377,  0.4401, -0.1663,\n",
       "          -0.6209]]),\n",
       " tensor([[-0.1752,  2.0501, -0.1442,  0.4419, -0.1620, -0.2058, -0.6787,\n",
       "           1.0827, -0.1227, -0.3620, -0.1236,  2.1401, -0.2054,  2.2851,\n",
       "          -0.1229, -2.1519,  2.3151, -0.1866, -0.1241,  0.2371, -0.1601,\n",
       "          -0.5780,  0.2688,  3.2444, -0.2161,  0.0547,  1.8297,  0.0699,\n",
       "          -0.1743]]),\n",
       " tensor([[-0.0705,  0.5183, -0.5721, -0.1272, -0.6228,  2.8857, -0.1915,\n",
       "          -0.1247, -0.3212, -0.5804, -0.2038, -0.1236,  2.5385, -0.2763,\n",
       "          -0.2625, -0.1201,  0.0295, -0.1426, -0.1445, -0.0408, -0.1174,\n",
       "           2.7351, -0.0953, -0.2038,  0.3520, -0.0095, -0.5798,  0.0624,\n",
       "          -0.5545]]),\n",
       " tensor([[-0.2052, -0.3162, -0.1230,  0.7552, -0.2049, -0.1225, -0.1428,\n",
       "          -0.0255,  0.7869, -0.2054, -0.1316, -0.1226, -0.1227, -0.1227,\n",
       "          -0.1323, -0.5072, -0.6619, -0.1216, -0.2173, -0.1262, -0.1347,\n",
       "          -0.1308,  1.0217, -0.1215, -0.1247, -0.1348, -0.1824,  2.6582,\n",
       "           2.6898]]),\n",
       " tensor([[ 1.8748, -0.2082,  0.0049, -0.2062, -0.1266, -0.2046, -0.1288,\n",
       "          -0.5340, -0.1226, -0.1225, -0.4233, -0.2030, -0.1233,  2.5507,\n",
       "           1.9063,  0.9013, -1.2710, -0.5772, -0.2010, -0.0088, -0.2049,\n",
       "           1.4250, -0.1177, -0.1225, -0.8104, -0.5064, -0.1625, -0.1896,\n",
       "          -0.3435]]),\n",
       " tensor([[-0.2047, -0.8096, -0.1321, -0.6773,  1.4712, -0.5801, -0.2037,\n",
       "          -0.1517, -0.2052, -0.1472,  1.3351, -0.4013,  1.6819, -0.2050,\n",
       "          -0.1558, -0.1225, -0.2053,  2.2716, -0.1072, -0.0935, -0.6783,\n",
       "          -1.8133,  0.2126, -0.5620, -0.1369, -0.4661, -1.7783, -0.1230,\n",
       "          -0.2052]]),\n",
       " tensor([[-0.1266, -0.5691, -0.1959,  2.2423, -0.2135, -0.1586, -0.2948,\n",
       "          -0.2052, -0.1259, -0.2032, -0.2021, -0.2052,  0.2730, -0.1225,\n",
       "          -0.1479, -1.2867, -0.2011,  0.7483,  0.7557, -0.3758, -0.2612,\n",
       "           2.3181, -0.2038,  2.2899, -0.2052, -0.4268, -0.1233, -0.4922,\n",
       "          -0.1227]]),\n",
       " tensor([[-0.2052,  2.0000, -0.2052,  1.5351,  2.0575, -0.1233, -0.1225,\n",
       "          -0.5819, -0.2117, -0.1243, -1.2992, -0.2281, -0.1226, -0.1235,\n",
       "           2.5911, -0.2052,  1.8747,  2.5936,  2.1901,  2.0377,  0.5964,\n",
       "          -0.4744, -0.1225, -0.2036,  1.8687,  1.6977,  2.7814, -0.1232,\n",
       "          -0.1580]]),\n",
       " tensor([[-0.1228, -0.3836, -1.2274, -0.2031, -0.1225, -0.1467,  2.5169,\n",
       "          -0.5049, -0.2269, -1.7982,  2.7871, -0.1236, -0.2662, -0.1225,\n",
       "          -2.0637, -0.1299, -0.5275, -1.0247, -0.1548, -0.2045, -0.2040,\n",
       "          -0.1968,  2.1748,  1.2601, -0.2052, -0.1959, -0.1429, -0.2209,\n",
       "          -0.2049]]),\n",
       " tensor([[-0.2075, -0.1215,  0.7021,  2.3575, -0.1450, -0.1809, -0.0755,\n",
       "          -0.1937,  0.8335, -1.1405, -0.8697, -0.2052, -0.2024,  1.6137,\n",
       "           1.4548,  1.9196, -0.1585,  0.2314, -0.5262,  0.0764, -0.5739,\n",
       "           0.5008,  0.0597, -0.1930, -0.2050, -0.7324, -0.1258,  2.1865,\n",
       "          -0.5799]]),\n",
       " tensor([[ 3.0102, -0.1471, -0.1225,  0.1450, -0.1661,  0.3789, -0.1872,\n",
       "          -0.1226, -0.4638, -0.2052,  0.8017, -0.9718,  2.7102, -0.5802,\n",
       "          -0.2042, -0.1225, -0.1314, -0.1255, -0.7915, -0.5031, -0.1315,\n",
       "          -0.2052,  0.3270, -0.4356, -0.1225, -0.1259, -1.9984, -0.2030,\n",
       "          -0.1621]]),\n",
       " tensor([[-0.1674, -0.1225, -0.2053, -1.5276, -0.1281, -0.5795, -0.6799,\n",
       "          -0.2049, -0.1019, -0.2608,  2.1075, -0.1225, -0.2052,  2.1546,\n",
       "          -0.2052,  0.2841,  0.8690, -0.1225,  0.3137, -0.1225, -0.1225,\n",
       "           0.1861, -0.1366,  0.1664, -0.3824, -0.1346, -0.1225,  0.3591,\n",
       "          -0.1233]]),\n",
       " tensor([[ 0.4970, -0.5778,  3.2095, -0.1252, -0.2052, -0.1225,  0.4763,\n",
       "           2.2358, -0.2052, -0.1100, -0.8867,  1.0047, -0.2052, -0.1936,\n",
       "           3.0895, -0.1227,  1.0672, -0.2058, -0.5788, -0.1368, -0.0996,\n",
       "           0.6224, -0.2047, -0.1773, -0.0553, -1.9473, -0.5845, -0.2053,\n",
       "          -0.0927]]),\n",
       " tensor([[-0.0613, -0.1094,  0.1019, -0.2385,  0.4094,  2.7074, -0.1237,\n",
       "           1.7206, -0.2043, -0.1449, -0.5736, -0.1234, -0.2041, -0.5273,\n",
       "          -0.7131, -0.1222, -0.1224,  2.6885,  2.5088, -0.1253, -0.1219,\n",
       "          -0.1312, -0.1222, -0.5633, -0.1157, -0.6212, -0.1965, -0.7532,\n",
       "          -0.1270]]),\n",
       " tensor([[-0.2050, -0.2047, -0.1723, -0.1221, -0.1278, -0.9984, -0.1226,\n",
       "          -0.0022, -0.4019, -0.1221, -0.1235,  0.2086, -0.2040, -1.1698,\n",
       "          -0.0655, -0.1905,  1.0657,  1.4950,  0.3156, -0.1584,  1.0361,\n",
       "          -0.1225,  0.5128, -0.1284,  2.6935, -0.5874, -0.1221,  0.3075,\n",
       "          -0.2032]]),\n",
       " tensor([[-1.7315, -0.1563,  1.4578,  1.5918, -0.1226,  0.2476, -0.1173,\n",
       "           3.1537,  0.3839, -0.0353, -0.1222, -0.1682, -0.4479, -0.4891,\n",
       "          -0.1548, -0.1221, -0.1228,  2.0782, -0.5493,  2.2978, -0.5353,\n",
       "          -0.5796, -0.2028, -0.1844,  3.4151,  0.0193, -0.2047, -0.1328,\n",
       "          -0.1466]]),\n",
       " tensor([[ 2.2950, -0.1826,  1.4842, -0.1007, -0.3675, -0.1220, -1.2745,\n",
       "          -0.5417, -0.1236, -0.1970,  1.5957, -1.3412, -0.3983, -0.1223,\n",
       "           0.3727, -0.4467, -0.1714, -0.5245,  1.5978,  0.9465, -0.5788,\n",
       "           0.2893,  0.0037, -0.1007, -0.4798, -1.4467, -0.1220, -0.4524,\n",
       "          -0.2415]]),\n",
       " tensor([[-0.1220, -0.2047,  0.3420, -0.2050,  0.8120,  1.4657,  0.7399,\n",
       "          -1.8591, -0.3562, -0.2046, -0.1549, -0.2789, -0.1220, -0.1429,\n",
       "           1.6695, -0.1329, -0.1220, -0.5037,  0.5108,  0.4683, -2.1609,\n",
       "          -0.2044, -0.1841,  0.7486,  0.2505, -0.2037, -2.1990, -0.1220,\n",
       "          -0.1843]]),\n",
       " tensor([[-0.1169,  2.0474, -0.2015, -0.1329, -0.1163, -0.1163, -0.1121,\n",
       "          -0.1576, -0.1976, -0.1843, -0.1603,  1.9034, -0.2257, -0.1986,\n",
       "           0.2992, -0.2370, -0.3233, -0.6090, -0.1952, -0.2008,  1.1386,\n",
       "           2.8743, -0.1164,  2.2697, -0.9419, -0.5335, -0.1525, -0.1129,\n",
       "           0.7790]]),\n",
       " tensor([[ 3.0760,  0.0874, -0.1748, -0.1780, -0.1944,  2.3157,  2.1628,\n",
       "          -0.1139,  1.0914, -0.1951,  1.1318,  0.4637, -0.4460,  1.8350,\n",
       "          -0.1135,  0.0010, -0.1135, -1.9454, -0.1966, -0.0375, -0.1951,\n",
       "           0.5166, -0.1137, -0.5424, -0.5705, -0.5274,  0.1960, -0.1135,\n",
       "           0.0161]]),\n",
       " tensor([[-0.6044, -0.0706, -0.1940, -0.4221,  3.2111, -1.4755, -0.1910,\n",
       "           0.8347,  3.1173, -0.1938, -0.1121, -0.0347, -0.1516, -0.1962,\n",
       "          -0.1205, -0.1185,  1.3109, -0.4575,  2.5525, -0.1504, -0.0658,\n",
       "           1.6409, -0.0180,  2.3906,  0.5130, -0.5345,  0.3589, -0.3079,\n",
       "          -0.1162]]),\n",
       " tensor([[-0.1768, -0.1113, -0.1930, -0.1805, -0.1113, -0.2123, -0.4190,\n",
       "           0.9157, -0.1932, -0.2506, -0.1930, -0.1932, -0.4139, -0.1123,\n",
       "          -0.1360, -0.1113, -0.7606, -0.1912,  3.3729,  2.8745,  1.9539,\n",
       "          -0.1594,  0.2616, -0.2131, -0.0869, -0.1964,  2.0617, -0.4837,\n",
       "          -0.1247]]),\n",
       " tensor([[-1.8429, -0.4614, -0.1918,  3.3906, -0.1481, -0.1876, -0.1120,\n",
       "          -0.1290, -0.5664,  2.9600, -0.1106, -0.0434, -0.1114, -0.1132,\n",
       "          -0.1950,  0.6400,  2.8590,  0.4114, -0.1115, -0.1929, -0.1708,\n",
       "          -0.1925, -0.1064, -0.1394, -0.1114,  0.8175,  0.5204, -0.1583,\n",
       "          -0.3337]]),\n",
       " tensor([[-0.0420, -0.1206, -0.1172, -0.6156, -0.1927, -0.1924, -0.1121,\n",
       "          -0.1653,  2.1393,  1.1756, -0.7175, -0.1912, -0.2035, -0.4174,\n",
       "          -0.0397, -0.1136, -0.1897,  1.4790, -0.1435, -0.1108,  1.9323,\n",
       "          -0.2089, -0.4510, -0.6090,  0.4641, -0.3019, -0.1019, -0.1320,\n",
       "          -1.7116]]),\n",
       " tensor([[-0.3173, -0.1103, -0.1776, -0.1108,  2.5982, -0.0423, -0.2381,\n",
       "           3.1925, -0.1131, -0.4905, -0.1110,  1.3204, -0.1107, -0.1778,\n",
       "           0.9453, -0.1475, -0.1951, -0.9581, -0.7385, -0.1148, -1.0497,\n",
       "          -0.1107, -0.1606, -0.1823, -0.1108,  2.3628, -0.5663, -0.1133,\n",
       "           2.0139]]),\n",
       " tensor([[-0.1924, -0.1629, -0.5388, -0.1144, -0.1107, -0.5037, -0.1109,\n",
       "          -0.8309, -0.1904,  2.2726, -0.4601, -0.1107,  2.3771, -0.1795,\n",
       "          -0.5257,  0.3436, -0.1924, -0.0418, -0.4158, -0.7383, -0.1923,\n",
       "           1.2192, -0.1127, -0.5673, -0.1824, -0.0577, -0.1926,  1.7163,\n",
       "          -0.2240]]),\n",
       " tensor([[ 1.5255, -1.1064, -0.1107, -0.1107,  1.0131, -0.1896, -0.1820,\n",
       "          -0.3144, -0.1202, -0.2102, -1.6917,  2.7412,  0.7888, -0.1925,\n",
       "          -0.1723,  3.3054, -0.5996, -0.1202, -0.1799, -0.1169, -0.1107,\n",
       "          -0.1574, -0.5719, -0.5128, -0.1108, -0.9880,  0.0965,  0.9362,\n",
       "          -0.1873]]),\n",
       " tensor([[-0.1393,  0.8893, -0.0740, -0.1269, -0.1925, -0.3030, -0.1107,\n",
       "           1.5846, -0.1114, -0.1921, -0.0589, -0.2393, -1.3244, -0.1103,\n",
       "           2.1994,  2.6562, -0.2937, -0.1862, -0.1109, -0.2798, -0.1601,\n",
       "           0.2435, -0.1783, -0.5861, -1.3346, -0.4049,  1.2184, -0.1147,\n",
       "          -0.3872]]),\n",
       " tensor([[ 1.8271,  0.4003, -0.3449, -0.5302, -0.1730, -0.6585,  2.9710,\n",
       "           0.3758, -0.5642, -0.1685, -0.1749, -0.0604,  2.5079,  0.8322,\n",
       "          -0.1111,  1.5566,  3.3240,  3.0060,  0.6884, -0.6879,  1.5886,\n",
       "          -0.1900, -0.1913, -0.1919, -0.1842, -0.1690,  0.0429, -0.3865,\n",
       "          -0.1198]]),\n",
       " tensor([[-0.1106,  0.8282,  2.5590, -0.5464, -0.1113, -0.1106, -0.1655,\n",
       "          -0.1665, -0.2200, -0.0866, -0.1329, -0.0687,  2.0280, -0.1310,\n",
       "          -0.5281, -0.1925, -0.1925, -0.1925,  0.7276, -1.5054,  0.5844,\n",
       "           2.5498, -0.1107, -0.1813,  3.2322,  0.8479, -0.1866, -0.4610,\n",
       "          -0.1154]]),\n",
       " tensor([[ 0.0047,  0.0374, -0.1107, -0.5092, -1.5616, -0.1107, -1.5014,\n",
       "           0.9910, -1.1725,  3.0549,  2.3464, -0.1107, -0.1925, -0.1106,\n",
       "          -0.1590,  0.5435, -0.4974, -0.1578, -0.5619, -0.1107, -0.5255,\n",
       "          -0.1106,  2.6549, -0.7132,  0.0264, -0.1678, -0.1106,  1.7575,\n",
       "          -0.1106]]),\n",
       " tensor([[-0.1574,  1.7796, -0.1953, -0.8396, -1.2982, -0.2812, -1.9255,\n",
       "           3.3994,  1.1713,  0.0921, -0.1106, -0.7560, -0.1922, -0.4736,\n",
       "           0.4642,  0.3734, -0.5670, -0.4632, -0.2944,  2.3020,  2.2645,\n",
       "           1.9749, -0.1106,  1.4232,  2.1980, -0.1280, -0.8565, -0.1127,\n",
       "           0.0451]]),\n",
       " tensor([[-0.1434, -0.1924, -0.3875, -0.1891,  3.5281, -0.1918, -0.1925,\n",
       "          -0.4010, -0.2262, -1.6829, -0.1111, -0.1915, -0.2711, -0.1891,\n",
       "          -0.5063, -0.5038, -0.1110, -0.1185, -1.2100, -0.1191,  3.1432,\n",
       "          -0.5089,  3.0721,  2.6945, -0.1925, -0.2906, -1.1770,  2.4434,\n",
       "          -0.1114]]),\n",
       " tensor([[ 1.5404, -0.1939,  1.9645,  0.7149, -0.1110, -0.1925, -0.4791,\n",
       "           1.0373,  0.1161, -0.5680, -0.1558,  1.2294, -0.1943, -0.2891,\n",
       "           1.0313, -0.1107, -0.7611,  1.4733, -0.4362, -0.1107, -0.3679,\n",
       "          -0.1918, -0.1106, -0.1137,  3.2014, -0.1810, -0.1239, -0.6893,\n",
       "           0.3436]]),\n",
       " tensor([[-0.8571,  1.9197, -0.1543, -0.1111, -0.0108,  0.3203,  0.3389,\n",
       "           2.5209, -0.1107, -0.1876, -0.1134, -0.1238, -0.1902, -0.4816,\n",
       "          -2.2975,  1.3037, -0.5577, -0.2012, -0.1551, -0.7506,  1.1808,\n",
       "           0.3441, -0.1921,  3.3701, -0.1106,  2.0115, -1.2150,  1.7865,\n",
       "          -1.0850]]),\n",
       " tensor([[-0.1134,  3.0910, -0.1982,  1.4490, -0.1218, -0.1493, -0.1126,\n",
       "          -0.1115, -0.0862, -0.1147, -0.1106, -0.1885,  0.0089, -0.1106,\n",
       "          -0.1595, -0.1541, -0.4297, -0.1940, -0.5133,  1.0082,  1.9816,\n",
       "          -0.1126, -0.1702, -0.5533,  1.6023, -0.1107,  0.8236, -0.1608,\n",
       "          -0.4891]]),\n",
       " tensor([[-0.5298, -0.6542,  2.6101, -0.1112, -0.5670, -0.1108, -0.1218,\n",
       "          -0.1028, -0.1107, -0.1917, -0.1111,  0.8752, -0.1925,  1.5959,\n",
       "          -0.1897,  1.8478, -0.1109, -0.5671,  2.4841, -0.1738,  0.0791,\n",
       "           2.0202,  2.4433,  2.3896,  1.8539, -1.9797,  0.0950, -0.5673,\n",
       "          -0.1106]]),\n",
       " tensor([[ 2.2352, -0.1202, -0.5080,  1.6042, -0.1106, -0.9992,  2.2508,\n",
       "          -0.1917, -0.1924, -0.1909, -1.6046,  0.1899, -0.1156, -0.1107,\n",
       "          -0.5119,  0.2824, -0.1735,  0.7625, -0.2963,  1.5595, -0.1928,\n",
       "          -0.1112, -1.6879, -0.2362, -0.5362, -0.1113, -0.1145,  1.0924,\n",
       "           0.4752]]),\n",
       " tensor([[ 0.0290, -0.1923, -0.1743,  3.0363, -0.1335, -0.1917, -0.1111,\n",
       "          -0.5727,  0.5507, -0.2335, -0.1925, -0.4746, -0.1917,  0.1114,\n",
       "           1.4174, -0.1317,  1.8521, -0.5636,  0.1037, -0.1909, -0.1867,\n",
       "           0.2427,  2.0686, -0.1113,  2.5274, -0.1125, -0.1108, -0.1887,\n",
       "          -0.8105]]),\n",
       " tensor([[-0.3624, -0.3021, -0.1076, -0.1077, -1.4440, -0.1453,  2.0678,\n",
       "          -0.7250,  2.2480, -0.1103, -0.1888, -0.1076, -0.1882, -0.1831,\n",
       "          -0.5279, -0.1888,  2.5047, -0.1078, -0.1076, -0.1080, -0.3297,\n",
       "           1.4124, -0.1113, -0.1888, -0.5059, -0.1080,  2.5732, -0.8408,\n",
       "          -0.1076]]),\n",
       " tensor([[ 2.7850, -0.2644, -0.1843, -0.5406, -0.5767, -0.0308, -0.1061,\n",
       "          -0.1871,  1.5792, -0.1866,  0.7491, -0.3008,  1.0160, -0.3339,\n",
       "          -0.5380, -0.9306, -0.1860,  0.6689, -0.1943, -0.3766, -0.1715,\n",
       "          -0.1286, -0.1871,  1.2190, -0.1297, -0.1061, -2.0846, -0.5832,\n",
       "          -0.1819]]),\n",
       " tensor([[-0.1742, -1.6612, -0.1344,  0.1571, -0.1810,  0.5112, -0.1009,\n",
       "           0.2041, -0.1805, -0.2472, -0.1775, -0.1714, -0.1443, -0.1024,\n",
       "          -0.1009,  3.4117,  0.7794, -0.1008, -0.1157, -0.2290, -0.4014,\n",
       "          -0.1798,  0.4267, -0.1963, -0.1254, -0.1435, -0.0473, -0.1589,\n",
       "           0.1329]]),\n",
       " tensor([[-0.0386,  1.3077,  1.6853, -0.6924,  0.3051, -0.0982, -0.1785,\n",
       "          -0.1732, -0.2269, -0.0982, -0.1637,  3.0747, -0.4641,  0.2000,\n",
       "           0.7692, -0.0982, -0.0802, -0.1154, -0.1587, -0.7079,  2.6111,\n",
       "          -0.2330, -0.1710, -0.0995, -0.5506, -0.2617,  1.9851,  1.7117,\n",
       "           0.2952]]),\n",
       " tensor([[-0.1756, -0.0986,  1.1919, -0.1290, -0.2963,  2.8564, -0.0973,\n",
       "          -0.1771, -0.1020, -0.4746, -0.0969,  0.8451,  0.4995, -0.1772,\n",
       "           0.3923, -0.1365,  0.0005, -0.9638, -0.1243,  0.5894,  1.0908,\n",
       "           2.3516, -0.0969, -0.1048, -0.1759,  0.8734, -0.1392,  1.1738,\n",
       "           0.2477]]),\n",
       " tensor([[-0.0640,  0.9085, -0.1814, -0.0801, -0.2770, -0.1562, -0.1764,\n",
       "           0.3015, -0.4180, -0.0963,  1.3365, -0.1415, -0.5421, -0.0962,\n",
       "          -0.4195,  0.0933, -0.6617, -0.4037, -0.2844, -0.0962,  0.9430,\n",
       "          -0.0962, -0.1335, -0.1756,  1.7014,  2.6651,  1.8803, -0.5124,\n",
       "          -0.1760]]),\n",
       " tensor([[-0.2228, -0.2195, -0.1745,  2.8141,  1.0084, -0.0756, -1.7416,\n",
       "          -0.1815, -1.3198,  2.1540,  2.7671, -0.1743,  0.1861,  1.4166,\n",
       "          -0.1110, -0.5521, -0.1191, -0.1727, -0.0947,  2.5717, -0.3987,\n",
       "          -0.0959,  1.6951, -0.0959, -0.1423,  0.0146, -0.2463, -0.1415,\n",
       "          -0.2334]]),\n",
       " tensor([[ 0.1797, -0.5737,  0.1425,  2.3872, -0.2644,  0.7731, -0.0967,\n",
       "          -0.6843, -0.0654,  2.4599, -0.3746, -0.5562, -0.3810, -0.3314,\n",
       "          -0.4053, -0.3424, -0.1762, -0.0086,  0.3112, -0.1754,  2.4034,\n",
       "          -0.1073, -0.4408, -0.1584,  0.8056,  0.1723, -0.1246,  3.0978,\n",
       "           1.4966]]),\n",
       " tensor([[ 2.4799, -0.4491, -0.4919, -0.4452, -0.1755, -0.1702,  0.8081,\n",
       "          -0.1497,  0.0576,  1.6570, -0.3942, -0.5518,  0.1892,  1.6880,\n",
       "          -0.0965, -0.0963,  2.4077, -0.0057, -0.0954,  2.7275, -0.0956,\n",
       "           1.3275, -0.1757, -0.0957,  0.1866,  1.4605, -0.1679, -0.0956,\n",
       "           1.4288]]),\n",
       " tensor([[-2.0650, -0.1742, -0.0943, -0.0225,  0.0478, -0.1747,  2.8674,\n",
       "          -0.0943,  0.0373,  0.7900, -0.1738,  0.8297, -0.0945, -0.5122,\n",
       "           0.7214, -0.5486, -0.0943, -0.0952, -0.0943, -0.2766,  2.1416,\n",
       "          -0.2932, -0.0656, -0.1711, -0.4513,  1.6874,  2.8627, -1.3082,\n",
       "          -0.0984]]),\n",
       " tensor([[-0.1197,  1.5185, -0.1789,  2.4456,  2.2795,  2.8177,  0.8197,\n",
       "          -0.5193, -0.0976,  0.5882, -0.5511, -0.1732, -0.5826, -0.1594,\n",
       "           2.9276,  1.4756, -0.0995, -0.5840, -0.1096,  2.9173,  0.3958,\n",
       "          -0.4876, -0.2887, -0.1478, -0.1755, -0.1757,  2.7396, -0.1730,\n",
       "          -0.1415]]),\n",
       " tensor([[ 0.6182,  0.3371, -0.1618,  1.0169, -1.0795,  1.6167, -0.5516,\n",
       "           0.6980, -0.0933, -0.5419,  0.2996, -0.0603, -0.4414, -0.1722,\n",
       "          -0.5112,  0.0124, -0.6213, -0.0935, -0.3727,  0.3478, -0.1321,\n",
       "          -0.1732, -0.4849, -0.1012, -0.0930, -0.1584, -1.0306, -0.5488,\n",
       "           1.9668]]),\n",
       " tensor([[-0.1722, -0.1694, -0.1711,  0.0963, -0.3374,  1.9800,  1.8545,\n",
       "          -0.0931, -0.1721,  2.0351, -0.1826, -0.0973,  0.1392, -0.0929,\n",
       "           1.5697, -0.2403, -0.1684,  1.5089, -0.1199, -0.1595, -0.1457,\n",
       "           1.3293, -0.0932, -1.6744, -0.3268, -0.1234, -0.1717,  1.3661,\n",
       "          -0.1609]]),\n",
       " tensor([[-0.1725,  0.7029, -0.1726, -0.0930,  1.4895, -0.0930,  2.6535,\n",
       "          -0.5435, -0.3954, -0.0940,  3.3670, -0.1726, -0.0971, -0.1730,\n",
       "          -0.1718, -0.0977, -0.6592,  2.3156, -0.1726, -0.1973,  1.8147,\n",
       "          -0.4558, -0.0930, -0.7421,  1.7567,  0.2003,  0.6559, -0.0941,\n",
       "          -0.2299]]),\n",
       " tensor([[-0.0929,  2.2447,  2.6179,  0.0445, -0.4763, -0.2660, -0.5003,\n",
       "          -0.4563,  0.1190, -0.0932, -0.1511,  1.6853,  0.9388,  0.1409,\n",
       "          -0.0929, -0.1410, -0.0976,  0.7590,  0.8247, -0.1035,  1.9460,\n",
       "          -0.0931, -0.1079, -0.5489, -0.1657, -0.1719, -0.0979, -0.3689,\n",
       "          -0.0342]]),\n",
       " tensor([[ 0.1283, -2.5001,  3.3838, -0.1416, -0.6922,  0.7966, -0.3108,\n",
       "          -0.0923, -0.2138,  3.3015, -1.4421, -0.1702, -0.3491, -0.1577,\n",
       "          -0.1699, -0.0922, -0.2560, -0.0460,  3.3558, -0.5810,  1.6552,\n",
       "          -0.0924, -0.1735,  2.6006,  2.3045, -0.1683, -0.0936, -0.3663,\n",
       "           2.4690]]),\n",
       " tensor([[-0.1115, -0.5433,  1.0707, -0.2989, -0.0849,  0.4879,  0.7647,\n",
       "          -0.0545, -0.2250,  0.2688, -1.7726, -0.1713, -0.5580, -0.1703,\n",
       "          -0.0199,  1.3353, -1.4840, -0.5116, -0.2320,  0.0793, -0.2937,\n",
       "           3.2024, -0.4372,  2.2708,  0.8018, -0.0915, -0.0915, -0.0915,\n",
       "          -0.4310]]),\n",
       " tensor([[ 2.4701,  0.8237, -0.0912,  2.7918,  0.9288, -0.1702, -0.6397,\n",
       "          -0.1581, -0.1196,  0.5771,  1.8760,  2.6520,  0.0187, -0.1699,\n",
       "           2.0776, -0.5328,  1.4696, -0.4821,  1.3993, -1.7199, -0.0918,\n",
       "           2.6237,  2.6783,  1.5187,  2.4197, -1.0925, -0.1240,  2.4791,\n",
       "          -0.0914]]),\n",
       " tensor([[-0.0008, -0.3327,  2.1465, -0.0986, -0.0915, -0.2400, -0.1917,\n",
       "          -0.1199, -0.0915, -0.0943,  1.2929, -0.0915, -0.1674, -0.1215,\n",
       "          -0.1598, -0.5810,  0.5650,  2.9133,  2.5961, -0.1072, -0.1703,\n",
       "          -0.5989, -0.0915,  2.2668, -0.0915, -0.1764, -0.3977,  0.0521,\n",
       "           0.0010]]),\n",
       " tensor([[-0.0980,  0.3231, -0.0921, -0.0916, -0.1117,  0.0111, -0.1699,\n",
       "           2.4209,  2.9227, -0.0947, -0.0916, -0.1056,  1.1199,  0.3056,\n",
       "          -0.0955, -0.0916, -1.1800, -0.1225,  1.7503, -0.1441,  1.5914,\n",
       "          -0.5134,  1.3166,  1.1122, -0.1686, -0.5321, -0.1654, -0.1128,\n",
       "          -0.3556]]),\n",
       " tensor([[-0.0917,  2.6978,  2.4199,  0.6935, -0.6236, -0.0917,  0.9286,\n",
       "           2.1292,  0.3381, -0.0961, -0.1304, -0.0917,  1.1456,  1.2265,\n",
       "           2.0928, -0.0938, -0.2274, -0.0917, -0.1674, -0.1354,  0.0234,\n",
       "          -0.1704,  0.0592, -0.0917, -0.6085, -0.2221, -0.7105,  1.3705,\n",
       "          -0.1684]]),\n",
       " tensor([[-0.0918, -0.1956,  2.7994, -0.4578, -0.1705, -0.1703, -0.1599,\n",
       "          -0.0918,  0.0950, -0.0921,  0.1470,  2.7287, -0.1886,  0.0111,\n",
       "           2.6268, -0.0923,  2.5236, -0.5467, -0.1033, -0.4766, -2.5864,\n",
       "          -0.0918, -0.1716, -1.4733, -0.1689, -0.4855, -0.0918, -0.0967,\n",
       "           0.4481]]),\n",
       " tensor([[-0.1156,  1.1347,  0.6052, -0.0700,  0.9965, -0.0936,  1.3266,\n",
       "           2.0323, -0.2410, -0.0918, -0.1248, -0.2201, -0.1705, -0.0440,\n",
       "           0.7308, -0.0942, -0.1707, -0.5469, -0.0918, -0.5404, -0.1352,\n",
       "          -0.5459, -0.0918,  2.6718,  2.5991,  0.8950, -0.0920,  2.0596,\n",
       "           0.5009]]),\n",
       " tensor([[ 0.3920,  0.1713, -0.3521,  0.3449, -0.4249, -0.0920, -0.5408,\n",
       "           2.8048, -0.0379, -0.0994, -0.0923, -0.1706, -0.0981,  0.3363,\n",
       "          -0.3764, -0.2036, -0.3867, -2.0093, -0.0922, -1.8115, -0.0227,\n",
       "           1.6019, -0.1706, -0.0918, -0.0932, -0.4021, -0.0915,  1.3453,\n",
       "          -0.0931]]),\n",
       " tensor([[-0.4859,  1.8907, -0.1540, -0.1649, -0.1661,  2.8969, -0.0938,\n",
       "           2.7182,  1.0129, -0.1450, -0.1676, -0.1698, -0.2946, -0.1705,\n",
       "           1.0841, -0.1697, -0.1706,  2.5977,  0.4343, -0.0918, -0.0994,\n",
       "          -1.9037, -0.1486,  0.3459,  1.6420, -0.0609, -0.5684, -0.1585,\n",
       "          -0.1007]]),\n",
       " tensor([[-0.1362,  2.5793, -0.0926, -0.1687, -0.1704, -0.1667,  0.3264,\n",
       "          -0.0966, -0.1576,  1.0808, -0.6748, -0.7045, -0.3272,  0.3339,\n",
       "          -2.0528, -0.1134, -0.8234, -0.0925, -0.1697, -0.5564, -0.4684,\n",
       "          -0.1701,  1.3654,  0.9087,  1.4078, -0.1705, -0.1195,  1.0176,\n",
       "          -2.2413]]),\n",
       " tensor([[-0.0918, -0.0944, -0.0925, -0.1632, -0.1705, -1.6155, -0.0924,\n",
       "          -0.1706, -0.5449, -1.5959, -0.1737, -0.0893, -0.1705, -0.0614,\n",
       "          -0.1653, -1.3345, -0.0946, -0.1991, -0.1706, -0.0918, -0.6975,\n",
       "          -0.0922, -0.1705, -0.1831, -0.4578, -0.1734,  2.5108,  1.3676,\n",
       "           1.0385]]),\n",
       " tensor([[-0.5828, -0.5449, -0.1809, -0.1698, -0.0204,  0.0053, -0.1534,\n",
       "          -0.0918, -0.1255, -0.1704,  0.6036,  2.4284,  1.4963, -0.1797,\n",
       "           2.8037,  0.4104, -0.0918, -0.1015, -0.5434, -0.0918,  2.2186,\n",
       "          -0.0919, -0.1687, -0.1625, -0.5329,  2.6018, -0.0918, -0.1700,\n",
       "           2.1909]]),\n",
       " tensor([[-0.1476, -0.5078,  1.2978, -0.1236, -0.1248, -0.0923, -0.4089,\n",
       "           0.8810, -0.0926, -0.1688, -0.2340,  3.3474, -0.1779, -0.1015,\n",
       "           0.8001, -0.0919, -0.1208, -0.1594, -0.1713, -0.1070,  2.8256,\n",
       "           1.4316, -0.4202, -0.1685, -0.1934, -0.0947, -0.2603, -0.1138,\n",
       "          -0.0918]]),\n",
       " tensor([[-1.0418, -0.1008, -0.1654, -0.1871, -0.1958, -0.4059, -0.1223,\n",
       "          -0.1613, -0.0724,  2.5413,  2.4389,  0.2977, -0.1706, -0.0954,\n",
       "          -0.2474, -0.1702, -0.2345, -0.0041, -0.0353, -0.1699,  2.1139,\n",
       "          -0.0050,  1.2457,  2.5148, -0.1575, -0.5257,  2.2627, -0.5142,\n",
       "          -0.4325]]),\n",
       " tensor([[-0.1705, -0.4261, -0.1696,  3.0755, -0.1434, -0.0918,  2.4430,\n",
       "          -0.0952,  2.5320,  0.6202, -0.0918, -0.2678, -0.1033, -0.4510,\n",
       "          -0.6635, -0.1739,  0.1795, -0.1705, -0.1697, -0.0919, -0.0917,\n",
       "          -0.1349,  1.9498,  1.7818,  2.1643,  1.9958, -0.5285,  2.9734,\n",
       "          -0.1121]]),\n",
       " tensor([[-0.4702, -0.5466, -0.5445,  0.0152, -0.0918, -0.3798,  2.9134,\n",
       "           2.6384,  0.9964, -0.0920, -0.1689, -0.1092,  1.9048,  0.5881,\n",
       "          -0.0947,  0.1409, -0.0924, -0.0703, -0.1046, -0.1692, -0.0919,\n",
       "          -0.1790, -0.6799,  2.2139, -0.5594, -0.3540, -0.1775, -0.1702,\n",
       "           1.8706]]),\n",
       " tensor([[-0.2457,  2.1029,  2.5278, -0.1031, -0.5984, -0.0918, -0.0972,\n",
       "           1.5119, -0.5473, -0.5444, -0.1699, -0.1632, -0.0963, -0.1716,\n",
       "          -0.1111, -1.2472, -0.5874,  0.8061, -0.1689, -0.2508, -0.5427,\n",
       "          -0.6183, -0.1443,  3.0332, -0.0919,  2.7444, -0.4223, -1.1352,\n",
       "           0.5600]]),\n",
       " tensor([[-1.7620,  0.5277, -0.4563,  0.1788, -0.3215,  2.7959,  0.2732,\n",
       "          -0.1486,  2.3122, -0.1705, -0.2228, -0.0918,  2.6791, -0.1703,\n",
       "           2.7200, -0.1220, -0.0243,  0.0946, -1.0850, -0.0918, -0.1712,\n",
       "          -0.1622,  0.5794, -0.1216, -0.1586, -0.0918, -1.3994,  3.0098,\n",
       "          -0.0919]]),\n",
       " tensor([[ 1.7217, -0.0524, -0.0920, -0.1125,  2.5236, -0.1728, -0.0919,\n",
       "           0.4457, -0.1402, -0.1706,  0.0398, -1.3997, -0.0961, -0.2391,\n",
       "          -0.0918,  2.4477, -0.5458, -0.0920, -0.1588, -0.0594, -0.1392,\n",
       "          -0.6340, -0.1688, -0.3991, -0.0654,  2.2767, -0.1226, -0.1273,\n",
       "          -0.1703]]),\n",
       " tensor([[-0.5853,  0.0609, -0.0935, -0.1201,  0.2698, -0.0922, -0.1700,\n",
       "          -0.1796, -0.5453,  0.4292, -0.0918, -0.6175, -0.2540, -0.0900,\n",
       "          -0.0922, -0.0880,  1.0951, -0.2905, -0.1701, -0.4134, -0.0952,\n",
       "          -1.3604, -0.1613, -0.1005, -0.9801,  3.0798,  0.8315, -0.1677,\n",
       "          -0.1624]]),\n",
       " tensor([[ 2.6186, -0.1705,  2.7185, -0.3872, -0.0970, -0.1699, -0.0918,\n",
       "          -0.1449, -0.5678, -1.2924,  2.4708,  1.9203, -0.1433, -0.1707,\n",
       "          -0.5439, -0.0936,  1.6976, -0.2592,  2.5981, -0.4875, -0.1618,\n",
       "           2.9198,  0.6159, -0.5449,  1.0349, -0.1705, -0.4620, -0.6353,\n",
       "          -0.1206]]),\n",
       " tensor([[ 2.3442, -1.5078, -0.0918,  0.3733, -0.1705, -0.0918,  2.5936,\n",
       "          -0.1705, -0.0918,  1.9856, -1.0139, -0.1362, -0.1177, -0.0935,\n",
       "          -0.1666, -0.5987, -0.1471, -0.4945, -0.1153, -0.5468, -0.0918,\n",
       "           2.5090, -0.5467,  0.0290, -0.5654, -0.1710,  1.5838, -0.1700,\n",
       "          -0.1701]]),\n",
       " tensor([[-0.1570, -0.0918, -0.1748, -0.1680, -0.5453, -0.5725, -0.0918,\n",
       "          -0.5446,  1.5441,  1.5168,  0.7933,  2.3570,  1.4728, -0.0918,\n",
       "          -0.1478,  3.0932, -0.5737,  0.8360, -0.5194, -0.0918, -0.0638,\n",
       "          -0.5438, -0.1701,  2.0740, -0.1705, -0.1373, -1.3484, -0.1076,\n",
       "          -0.0956]]),\n",
       " tensor([[-0.7410,  2.1702, -0.8756, -0.1705, -0.1082, -0.1754,  0.8498,\n",
       "           0.0647,  2.5339, -0.2821, -0.1699,  2.7275, -0.0449, -0.1062,\n",
       "          -0.1986,  2.7048, -0.1705,  1.8033,  1.9160,  0.8858,  0.3385,\n",
       "          -0.3255, -0.0924, -0.0403, -0.1220, -0.1845,  2.9737, -0.1270,\n",
       "          -1.4340]]),\n",
       " tensor([[-0.1705, -0.3402,  2.0140, -0.0942, -1.2867, -0.0918, -0.0918,\n",
       "          -0.0885, -1.3613,  3.1439,  2.8243,  0.0554,  3.6594, -0.0918,\n",
       "           0.1154, -0.0918, -0.1440,  0.0231,  0.6393, -0.1057, -0.1576,\n",
       "          -0.1142,  0.0220, -0.1704,  0.8870, -0.1704,  2.1403, -0.4731,\n",
       "           0.2001]]),\n",
       " tensor([[-0.0918, -0.0908, -0.1381, -0.1679, -0.1145, -0.0918, -0.5268,\n",
       "          -0.2819, -0.1704,  0.3879, -0.6175, -0.1684, -1.2102,  3.2704,\n",
       "          -0.5339, -0.1548,  1.1730,  2.5092, -0.1671, -0.1243, -0.0956,\n",
       "          -0.5197, -0.4094,  3.2972, -0.5518, -0.1704,  1.7872, -0.0936,\n",
       "           0.4250]]),\n",
       " tensor([[-0.4608, -0.0924, -0.3017, -0.0918, -0.0918,  0.1869, -0.0922,\n",
       "          -1.7577, -0.1688,  1.4611, -0.4947,  0.1942, -0.5074, -0.8359,\n",
       "          -1.7705, -0.1379, -0.1017, -0.5451, -0.3672, -0.0920,  2.3963,\n",
       "          -0.5452,  0.3867, -0.1759, -0.5407, -0.0929, -0.0918,  1.5912,\n",
       "          -0.0919]]),\n",
       " tensor([[-0.0919, -0.3583,  0.5977, -0.0190, -0.1167, -0.0932, -0.4623,\n",
       "           0.4548, -0.1296,  0.2406,  1.9275, -0.1331, -0.5435,  0.9124,\n",
       "           1.4108, -0.1210, -0.5452,  0.2186,  2.2607, -0.1400, -1.2794,\n",
       "          -0.2329, -0.1615, -0.0919, -0.1704, -0.0931,  0.6284, -0.8099,\n",
       "           0.5651]]),\n",
       " tensor([[-0.0945,  2.2254,  3.1316,  3.3585,  3.6509, -0.1746, -0.1501,\n",
       "          -0.1003, -0.1504, -2.0441, -0.0918,  2.0730, -0.5404,  1.3671,\n",
       "          -0.0919, -0.0059, -1.6138, -0.1705, -0.0296, -0.1532,  2.2679,\n",
       "          -1.3515, -0.2659,  1.4906, -0.2574, -0.0918,  0.8771, -0.2328,\n",
       "          -0.5795]]),\n",
       " tensor([[-0.1699, -0.1019, -0.5895, -0.5307, -0.3325,  1.0937, -0.0918,\n",
       "           2.6336,  0.7956, -0.0922,  0.5255,  0.7816, -0.1743,  2.1354,\n",
       "          -0.1063,  2.4699,  0.1396, -0.1698, -0.1700, -0.1703, -0.0967,\n",
       "          -0.5092, -0.1265, -0.4923,  2.5806, -0.5075,  0.2792, -0.0978,\n",
       "          -0.0919]]),\n",
       " tensor([[-0.1716,  2.4301,  1.0836,  0.1061, -0.0929, -0.1560, -0.1246,\n",
       "           2.9901,  0.1258, -0.1705, -0.0925, -0.1705, -0.1700, -0.5468,\n",
       "          -0.1705, -0.0967, -0.1648, -0.0890, -0.3662, -0.1264, -0.1560,\n",
       "          -0.1706,  2.4393,  1.0252, -1.6430, -0.4165,  1.2403, -0.0928,\n",
       "          -0.5420]]),\n",
       " tensor([[-0.1526,  2.9881, -0.0918, -0.1825, -0.5139, -0.1649, -0.0918,\n",
       "          -0.0947,  2.5914, -0.5617, -0.1088, -0.9443, -0.0918,  3.0932,\n",
       "          -0.1705,  3.0402,  2.1723,  0.3589, -0.1649, -0.1704, -0.1452,\n",
       "          -0.1002, -0.9599,  0.4775,  0.1094,  1.9850,  2.8949, -0.4040,\n",
       "           3.2983]]),\n",
       " tensor([[-0.8626, -0.0919, -0.1199, -0.4603,  2.8013, -0.1643, -0.0918,\n",
       "          -0.2675, -0.1705,  0.6864,  1.2989, -0.0953, -0.1705, -1.7219,\n",
       "           0.0699, -0.1705,  2.3978, -0.3866,  2.1074, -0.1435, -0.1909,\n",
       "          -1.2715, -0.0947,  1.9341,  0.5221, -0.0925, -0.1015, -0.1675,\n",
       "           0.2362]]),\n",
       " tensor([[-0.1706, -0.4766, -0.1701, -0.1112, -0.1264, -0.0567,  2.3998,\n",
       "           1.8868, -0.5321, -0.0922, -0.0964,  1.5127, -0.3406, -1.3206,\n",
       "           1.9381,  0.1121, -0.0918,  1.2903, -0.0919, -1.2241, -0.1077,\n",
       "          -0.0918,  0.1304,  0.0149, -0.0937, -0.5277,  0.8776,  0.1100,\n",
       "          -0.0926]]),\n",
       " tensor([[-0.1699,  0.6752,  0.6047, -0.0911, -0.0911, -0.1697,  0.5030,\n",
       "          -0.1001,  0.0827, -2.4220, -0.1692,  2.4472,  2.8489,  1.9312,\n",
       "          -0.1674, -0.0860,  0.5370,  2.4055, -0.4071, -0.5596, -0.1698,\n",
       "           2.9011, -0.1696, -0.1608, -0.1709, -0.1689, -0.5084, -1.0154,\n",
       "           0.1181]]),\n",
       " tensor([[-0.1433, -0.0918, -2.3730,  0.7620, -0.1489,  1.0201,  0.2525,\n",
       "          -0.5447, -0.1696,  1.1414, -0.6489, -0.1957,  0.2572,  2.3499,\n",
       "          -0.1676, -0.1643, -1.3120, -1.2984,  0.0098, -0.0316,  0.5395,\n",
       "          -1.9054, -2.2428,  2.4726, -0.1705, -0.4679,  0.5086,  2.1162,\n",
       "          -0.5072]]),\n",
       " tensor([[ 0.3300,  2.6933,  2.0080, -0.6409, -0.0905, -0.1687, -0.0905,\n",
       "          -0.5230,  2.8324, -0.0342, -0.7784, -0.1020, -0.0156,  2.4911,\n",
       "           1.4486, -0.2324, -0.1077, -0.0931, -0.0906, -0.0956, -0.0908,\n",
       "           3.2847,  3.0844,  0.1726, -0.1687, -0.1694, -0.0867, -0.0162,\n",
       "          -0.6234]]),\n",
       " tensor([[ 2.1957, -0.1158, -0.0910, -0.1743,  0.2794, -0.0905, -0.1403,\n",
       "          -0.1612, -0.5569, -0.7537,  2.3362, -0.1803, -0.0947, -0.0930,\n",
       "           3.0546, -0.1707, -0.6539, -0.1695,  2.6172,  0.2436, -0.5614,\n",
       "          -0.1346,  1.3236, -0.3131, -0.0905, -0.1663,  0.2141, -0.0958,\n",
       "           1.0986]]),\n",
       " tensor([[ 1.0204,  0.3265, -0.0906, -0.1692,  1.2843, -0.1829,  1.2987,\n",
       "          -0.1048,  0.9747,  2.4491, -0.0965, -0.1205,  0.6274, -0.2965,\n",
       "          -0.0915, -0.1605, -0.3612, -0.5449, -0.0910, -0.5321, -0.1486,\n",
       "          -0.1389, -0.0904, -0.6065, -0.0904,  2.1452, -0.4956, -0.1066,\n",
       "          -0.1684]]),\n",
       " tensor([[-0.1693, -0.0916, -0.1695, -0.1612, -1.5725, -0.0906,  1.1549,\n",
       "          -0.1668, -0.1677, -0.0904,  1.5522, -0.0904,  0.4188,  0.3747,\n",
       "          -1.8433, -0.2947, -0.5783, -0.1679, -0.5087,  2.6920,  0.9224,\n",
       "           2.6411,  0.8447, -0.5380,  2.4908, -0.0912, -0.4959, -0.0904,\n",
       "          -0.1670]]),\n",
       " tensor([[ 0.4139, -0.4800, -0.0904, -0.1693, -0.1588, -0.0904, -0.0933,\n",
       "          -0.1682,  0.1765, -0.1019, -0.5310,  0.0350, -0.0904, -0.0904,\n",
       "          -0.1170, -0.1678, -0.1739,  0.0987, -0.4138,  3.4178, -0.1227,\n",
       "           1.8220, -0.1693,  2.0176, -1.1767,  0.1566, -0.0943,  1.1082,\n",
       "           2.6009]]),\n",
       " tensor([[-0.2106, -0.0881,  0.6824, -0.0911, -0.0928, -1.2875, -0.1357,\n",
       "          -0.5458,  2.2551, -0.2180, -0.4002, -1.4411, -0.6406, -0.0976,\n",
       "          -0.1654,  0.0007, -0.1676, -0.1043,  1.0207, -0.1694,  0.9244,\n",
       "          -0.2089, -0.7687,  2.1891, -0.0910,  0.5062, -0.1957,  2.9475,\n",
       "           0.4614]]),\n",
       " tensor([[ 0.2619,  0.8153, -0.0904,  0.4380,  1.8400, -0.1987, -0.1150,\n",
       "          -0.7164, -0.5454, -0.7582, -0.0972, -0.0903,  1.8061,  3.8199,\n",
       "           0.7685, -0.0905,  1.8678, -0.4258, -0.1609, -0.1404, -0.0904,\n",
       "          -0.0903, -0.8716, -0.0904,  2.7161, -0.0947, -0.0922,  2.5576,\n",
       "          -0.1690]]),\n",
       " tensor([[-0.0993,  0.4737,  2.9434, -0.0903, -0.6822,  1.2016, -0.0980,\n",
       "          -0.1337, -0.8684, -0.6662,  3.1017, -0.2010,  2.5198, -0.0913,\n",
       "           0.4848,  2.5720, -0.0201, -0.2724, -0.1693, -0.0849, -0.1073,\n",
       "          -0.1698, -0.1615, -0.6304,  0.4874, -1.2537, -0.4870,  0.2467,\n",
       "          -0.5431]]),\n",
       " tensor([[ 2.6016,  3.1314, -0.1703,  2.4623, -0.1135, -0.0931, -0.1687,\n",
       "           0.0669, -0.1688, -0.0896,  2.3415, -0.4445, -0.0957,  3.3266,\n",
       "          -0.3192, -0.0904, -0.0911, -0.1281,  2.0908, -0.2657, -1.0832,\n",
       "          -0.3500, -0.1567, -0.5429, -0.0903,  0.0716, -0.3084,  0.4583,\n",
       "          -0.1721]]),\n",
       " tensor([[-0.1692, -0.1516, -0.1134, -0.5647,  0.3078, -0.7505, -0.1717,\n",
       "           3.8065, -1.3926,  3.0711, -0.3139,  0.6776,  2.2773, -0.0916,\n",
       "           2.4048,  3.2871, -1.6314, -0.2472,  3.1034,  2.3343, -0.1693,\n",
       "          -0.0904,  3.0209, -0.1441, -0.0639, -0.1692, -0.1692, -0.0912,\n",
       "          -0.1688]]),\n",
       " tensor([[ 0.5475,  3.6313, -0.0903, -0.1244, -0.4340, -0.0904, -0.2407,\n",
       "           1.9765,  3.0215,  0.0178, -0.1693,  3.3730,  2.5410, -0.1008,\n",
       "          -0.1130,  0.1337, -0.0913, -0.5262,  2.7136, -0.0997, -0.1693,\n",
       "          -0.5284, -0.1288, -0.1693, -0.0978, -0.4872, -0.5454, -0.6261,\n",
       "          -0.0903]]),\n",
       " tensor([[ 0.0103,  1.2050, -0.0958, -0.1418, -0.1320, -0.1692,  2.2850,\n",
       "          -0.0916, -0.5152, -0.1693,  2.4514, -0.1109, -0.0505,  0.5948,\n",
       "          -0.0903,  0.5187, -0.0903, -0.1002, -0.0904,  3.4594, -1.2406,\n",
       "           2.3974,  2.0143,  0.2848, -0.1350,  1.9129, -0.0979, -0.1710,\n",
       "          -0.0927]]),\n",
       " tensor([[-0.1581, -0.1576, -0.0994,  3.0964, -0.0790, -0.1571, -0.1572,\n",
       "          -0.1591,  2.4828, -0.1577, -0.1057, -0.1576, -0.5331, -0.5091,\n",
       "          -0.8519,  2.1516, -0.1577, -0.3381,  0.1327, -0.1519, -0.0790,\n",
       "          -0.1690, -0.1574,  0.8681, -1.2215, -0.2719, -0.1599,  0.8306,\n",
       "          -0.0795]]),\n",
       " tensor([[ 0.7160, -0.5018, -0.1080,  2.4584,  2.9700,  1.5810, -0.0928,\n",
       "          -0.0735, -0.1519, -0.0789, -0.0858,  2.5839,  1.4026, -0.1601,\n",
       "           1.0498, -0.0812, -0.1185, -0.0736, -0.0152, -0.1240, -0.8158,\n",
       "          -0.5982, -0.0655, -0.1457, -0.2994, -0.5230, -0.1989,  2.5443,\n",
       "          -0.0736]]),\n",
       " tensor([[-0.0265, -0.0602, -0.0705, -0.1490, -0.0769, -0.0737,  1.1278,\n",
       "          -0.1439,  2.4718, -0.1724, -0.0728,  0.5890, -0.1327,  2.5982,\n",
       "          -0.5186,  2.5617, -0.0713, -0.0706, -0.5238, -0.1435,  0.7723,\n",
       "          -0.1487, -0.0871,  2.5951, -0.0706,  1.5371, -0.1323, -0.4887,\n",
       "          -0.1466]]),\n",
       " tensor([[ 1.5259, -0.1223, -0.0692, -0.0691,  0.2700, -1.1414, -1.5103,\n",
       "          -0.1458, -0.2245, -0.3947,  0.5500, -0.0692, -0.0697, -0.1185,\n",
       "          -0.1471,  2.4970, -0.4519,  0.5944,  1.8783, -0.1873, -0.1473,\n",
       "          -0.2999, -0.1474, -0.0695, -0.1075,  0.4958,  2.0982, -0.6055,\n",
       "          -0.0691]]),\n",
       " tensor([[ 2.4941, -0.0683, -0.1451, -0.5068,  0.3842, -0.0769, -0.1416,\n",
       "          -0.1408, -0.2327,  2.2873,  2.4720,  0.8424,  0.7290, -0.4184,\n",
       "           0.0171, -1.0291, -0.1471,  2.6854,  1.0151,  2.7486, -0.1444,\n",
       "           1.9813, -0.0684, -0.7191,  0.9096, -0.2148,  0.5593, -0.0684,\n",
       "          -0.0354]]),\n",
       " tensor([[ 3.4765,  2.3523, -0.3387,  3.5168, -0.0636, -0.0674,  2.1731,\n",
       "           1.4845,  2.8605, -0.0636,  0.0347, -0.0231, -0.0735, -0.0636,\n",
       "           0.3627,  1.2633, -0.1160,  0.1026, -0.0636, -0.0636, -0.4460,\n",
       "          -0.1447,  0.8528, -0.0772, -1.0805,  0.5223, -0.0636,  0.2736,\n",
       "          -0.1767]]),\n",
       " tensor([[-0.0620, -0.0619,  0.2083, -0.1103, -0.0622, -0.0628, -0.1390,\n",
       "          -0.1333,  2.6187, -0.0611, -0.3918, -0.5485, -0.1389, -1.0413,\n",
       "          -0.0614, -0.5450, -0.0611, -0.2764,  1.2110, -1.7912, -0.4745,\n",
       "          -0.1391,  0.7960, -0.1391, -0.7491, -0.1373, -0.0612, -0.0622,\n",
       "           0.8633]]),\n",
       " tensor([[-0.5098, -0.0586, -0.1330, -0.1269,  2.5170,  1.1749, -0.7944,\n",
       "          -0.0553, -0.0555, -0.0825, -0.0875, -1.4316, -0.0023, -0.1326,\n",
       "           1.3694,  0.4044, -0.4720,  3.4984, -0.1301,  0.3711, -0.5117,\n",
       "           2.4435, -0.4362,  1.0687,  0.9885, -0.2296, -0.1011, -0.0883,\n",
       "          -0.1081]]),\n",
       " tensor([[-0.0036, -0.1319, -0.1225,  0.8893, -0.1807,  2.5042,  2.8438,\n",
       "          -0.5056, -0.0552, -0.0946, -0.8772, -0.1307, -0.1823, -0.9048,\n",
       "          -0.0543, -0.0542, -0.0537, -0.1535, -0.1313, -0.4947, -0.0518,\n",
       "          -0.1311, -0.1119, -0.1311, -0.4870, -0.0549, -0.0804, -1.6853,\n",
       "          -0.0006]]),\n",
       " tensor([[-0.0789,  3.0792, -0.4588, -0.0747, -0.3547,  1.4535, -0.1664,\n",
       "           1.0215, -0.0533, -0.2259, -0.3345, -0.0857,  2.3916,  0.1563,\n",
       "           1.7802, -0.0790, -0.6895, -0.0852, -0.0533, -0.7200, -0.1307,\n",
       "           2.1586, -0.4888, -1.3146,  1.0165, -0.1289, -0.1333,  0.0436,\n",
       "           2.7406]]),\n",
       " tensor([[-0.5875, -0.5322, -0.0559,  1.8988, -1.1002,  1.0086, -0.1272,\n",
       "          -0.0601, -0.5098, -0.0550,  3.1031, -0.1324, -0.0770, -0.0957,\n",
       "          -0.0572,  0.5259, -0.0530, -0.5758, -0.1295,  2.0758,  0.0331,\n",
       "           0.9444,  2.2473, -1.8384,  0.1504, -0.1303, -0.7730,  3.3154,\n",
       "          -0.0530]]),\n",
       " tensor([[-2.4052, -0.1212,  1.5096,  0.7894, -0.2126,  0.9857, -0.2597,\n",
       "          -0.1294, -0.1664, -0.0742, -0.3641, -0.0528, -0.3250, -0.0560,\n",
       "           0.7168, -0.0592, -0.2269,  1.6983, -0.1215, -0.1629,  0.0238,\n",
       "          -0.1774, -0.0557, -0.0528, -0.0560,  0.0213,  0.2043,  2.6317,\n",
       "           1.6465]]),\n",
       " tensor([[ 2.3711, -0.3585, -0.0578,  2.5505, -0.1248, -0.3369,  1.7943,\n",
       "          -0.0533,  0.8237, -1.6263, -0.1907, -0.0538, -0.0530, -0.3305,\n",
       "          -0.0529, -0.1452,  1.6975, -0.0527, -0.0527, -0.0563, -0.4774,\n",
       "          -0.0297, -0.0529, -0.4530, -0.0500, -0.1264, -0.0527,  1.1181,\n",
       "          -0.0527]]),\n",
       " tensor([[ 1.5790, -0.0527, -0.0532, -0.3300,  0.3388, -0.4939, -0.0529,\n",
       "          -0.5005, -0.0526, -0.0668, -0.0851,  0.8443, -0.1293, -0.0526,\n",
       "          -0.1235,  2.0050, -0.3774, -0.9159,  1.1353, -0.5895, -0.0527,\n",
       "          -0.1250, -0.1316, -1.0697, -0.0534, -0.5006, -0.1157, -0.1292,\n",
       "          -0.0527]]),\n",
       " tensor([[-0.2058, -0.0721, -0.1261, -0.0562, -0.3830, -2.3741, -0.0536,\n",
       "           1.7214, -0.2742,  0.8976, -0.0698,  0.4368, -0.2485, -0.0867,\n",
       "           0.2650, -0.1291,  2.1181, -0.1160, -0.1292, -1.8132,  2.3868,\n",
       "          -0.4841, -0.9072,  1.9223,  0.5983, -0.1963,  0.2703,  0.1511,\n",
       "          -0.0682]]),\n",
       " tensor([[-0.5326,  0.3851,  1.1389, -0.2040, -0.0579, -0.0625, -0.0580,\n",
       "          -0.4896,  0.4070, -0.1345,  1.6439, -0.0974,  0.0105, -0.0694,\n",
       "           0.4687,  0.0680, -0.0897,  2.4586, -0.1328, -0.0741, -0.1119,\n",
       "           2.2926,  3.0753, -0.0581, -0.0580, -0.0579,  2.0962,  3.0117,\n",
       "           0.0475]]),\n",
       " tensor([[-0.0326, -0.5135,  3.8263, -0.0605, -0.0677, -1.9505, -0.0562,\n",
       "           0.7171, -0.0605, -0.4386, -0.0662, -0.2432, -0.1472, -0.0687,\n",
       "          -0.1054, -0.1206,  2.2229,  2.2872, -0.0721, -0.1009, -0.1321,\n",
       "           2.7602,  0.3617, -0.5183,  0.0875,  2.3940, -0.1704, -0.5130,\n",
       "          -0.6336]]),\n",
       " tensor([[-0.3697, -0.0660, -0.2790, -0.1345, -0.1306, -0.0816, -0.1322,\n",
       "          -0.1384, -0.0784,  3.0233, -0.0752, -0.1343, -0.0619, -0.1379,\n",
       "          -0.0703, -0.0064, -0.1023,  2.8405, -0.0618, -0.5152, -0.0949,\n",
       "          -0.0618, -0.0857, -0.0618, -0.0829, -0.0652, -0.1115, -0.2358,\n",
       "          -0.1001]]),\n",
       " tensor([[ 0.1602,  0.2818, -0.6979,  2.0657,  1.5596, -0.1392,  2.3263,\n",
       "          -0.0624,  3.1528, -0.1392,  0.4032, -0.1767, -0.1383, -0.1094,\n",
       "          -0.1874,  0.2236, -0.2152, -0.1430, -0.1393, -0.0036, -0.4314,\n",
       "          -0.5103, -0.4791,  0.0184, -0.0636,  3.0842,  0.0410, -0.1386,\n",
       "          -0.1258]]),\n",
       " tensor([[ 2.0847, -0.1080, -0.0633, -0.0632,  1.6447,  2.8828, -0.1374,\n",
       "           3.3902, -0.0627, -0.0634, -0.5853, -0.0622, -0.5138, -0.4229,\n",
       "          -0.5079, -0.1396, -0.0628, -0.0672, -0.1396,  1.1099,  3.2669,\n",
       "          -0.1348,  0.6564, -0.1310, -0.1344, -0.0992,  1.6585, -0.1355,\n",
       "          -0.0628]]),\n",
       " tensor([[ 0.2005,  1.7699,  0.0906, -0.1447, -0.3566,  3.0362,  0.2383,\n",
       "          -1.2904, -0.0714, -0.0687,  2.2844,  2.5737, -0.5166, -0.1712,\n",
       "          -0.0629,  0.0927,  0.7867, -0.1501,  1.2593,  0.1410, -0.0629,\n",
       "          -0.1218, -0.1397, -0.1305, -0.0629, -0.0616,  1.5089,  0.6262,\n",
       "          -0.9796]]),\n",
       " tensor([[ 0.4598, -0.3642, -0.5178, -1.0224, -0.0636, -0.0757, -0.0635,\n",
       "          -0.1886, -0.1404,  0.2757, -0.1206, -0.0886,  1.7978,  0.6502,\n",
       "          -0.1395, -0.1195, -0.1404,  0.0192, -0.0695, -0.0635,  2.9169,\n",
       "           3.3166, -0.0635,  0.1264,  1.6042, -0.2157, -0.0637, -0.1443,\n",
       "           2.9633]]),\n",
       " tensor([[-2.2253, -0.2319,  0.2579, -0.0645,  2.9372,  0.1235,  0.2433,\n",
       "           1.9796, -0.6117, -0.0638,  3.4228, -0.5629, -0.7296, -0.1393,\n",
       "          -0.0461, -0.1500,  0.9412, -0.6371,  2.8082,  3.0258, -0.1407,\n",
       "          -0.2681, -0.1451,  2.0301, -0.1372,  0.3384, -0.0641, -0.0652,\n",
       "          -1.6678]]),\n",
       " tensor([[ 2.2750,  2.3130, -0.4834,  2.3165, -0.1555, -0.1411,  0.8658,\n",
       "          -0.0919,  2.4021, -0.0779,  0.1099, -0.1400,  3.3378, -0.1409,\n",
       "          -0.1383, -0.2990,  2.1872, -0.0637, -0.0775,  1.2419, -0.0666,\n",
       "          -0.0934, -0.0347,  0.1252, -0.1375, -0.1409, -0.0640, -0.0743,\n",
       "          -0.1086]]),\n",
       " tensor([[ 2.7437, -0.2836,  0.8921, -0.1408,  2.6235, -0.1371, -0.0952,\n",
       "          -0.1581, -0.3366,  2.9505,  2.2796, -0.0643, -0.1353, -0.4109,\n",
       "          -0.3630, -0.0665, -0.3779, -0.1014,  0.0944, -0.5575, -0.2752,\n",
       "          -1.0418, -0.1404, -0.1440, -0.0861, -0.1372, -0.3355,  1.2389,\n",
       "           2.8117]]),\n",
       " tensor([[ 3.1344, -0.0641, -0.1358,  3.4809, -0.0351, -0.0641, -0.1531,\n",
       "          -0.0642, -0.2742,  0.0906, -0.1353, -0.2877,  1.3875,  0.1709,\n",
       "          -0.3162, -0.6697, -0.0947, -0.0393,  0.2284, -0.1104, -0.1180,\n",
       "          -0.1180,  0.5834,  0.1625, -0.6379, -0.3391,  0.2563, -0.0641,\n",
       "          -0.0641]]),\n",
       " tensor([[-0.0690, -0.1440,  0.4555, -0.0265, -0.0672, -0.0677, -0.4355,\n",
       "           2.4741,  2.7638, -0.5504, -0.0875, -1.4550,  2.3159,  1.9901,\n",
       "           0.4279, -0.0671,  1.4978,  0.3998,  0.1058, -0.5370, -0.1442,\n",
       "          -0.0697, -0.1207, -0.6285,  0.2491,  2.7659, -0.1439, -0.0671,\n",
       "          -1.5796]]),\n",
       " tensor([[ 0.4593, -0.0700,  0.0552, -0.1455, -0.1157,  0.3473,  2.1121,\n",
       "           2.0541,  3.0680, -0.6549,  0.1356,  2.1042,  2.6512, -0.0775,\n",
       "          -0.0688, -0.0820,  1.0816, -0.1238, -0.4379, -0.1455, -0.0686,\n",
       "          -0.0687, -0.1304, -0.4906, -0.1515, -0.0767, -0.1919, -0.1503,\n",
       "          -0.0687]]),\n",
       " tensor([[ 0.4117, -0.6101,  1.0311, -0.1432, -0.4329,  1.6877, -0.0711,\n",
       "           3.2926, -0.0695, -0.2727, -0.1423, -0.5374, -0.0568, -0.1113,\n",
       "          -0.0695,  2.0131, -0.1026, -0.1462, -0.1464, -0.2541, -0.4876,\n",
       "          -0.1444, -0.0694, -0.4167, -0.1757, -0.0694, -1.4645,  2.8056,\n",
       "          -0.3909]]),\n",
       " tensor([[-0.1151,  0.3028, -0.3358,  2.5804, -0.6239,  2.7567, -1.6673,\n",
       "          -0.0741,  0.4431,  0.3653,  3.4875,  0.0336,  2.4304,  0.2956,\n",
       "          -0.0741, -0.1472,  2.1319,  3.5304, -0.1307,  0.3969, -0.0780,\n",
       "          -0.1510, -0.1068, -0.0761,  0.0458, -0.1412, -0.2598,  1.4788,\n",
       "          -0.1533]]),\n",
       " tensor([[-0.0818, -0.0767,  2.7836, -0.1534, -0.0613, -1.2333, -0.2966,\n",
       "           1.8327,  3.6375, -0.0967, -0.6470, -0.2219, -0.0847, -0.1027,\n",
       "           0.0782, -0.2025, -0.0764, -0.4297, -0.3641, -0.0251, -0.0764,\n",
       "          -0.2120, -0.1828, -0.0764, -0.0895, -0.5343, -0.5179,  2.2811,\n",
       "           2.6604]]),\n",
       " tensor([[ 2.7251,  2.8985, -0.0763, -0.1442,  1.6439,  0.7946, -0.4434,\n",
       "          -0.1541, -0.4056, -0.1249, -0.1547, -0.1497, -0.1501, -0.1624,\n",
       "          -0.0777, -0.0778,  0.9792, -0.1539, -0.2438, -0.6586, -0.1546,\n",
       "          -0.0775, -0.1546, -0.1197, -0.0776,  0.8554,  0.6176,  0.1241,\n",
       "          -0.1538]]),\n",
       " tensor([[ 0.1233,  0.4945,  1.2849,  3.4715, -0.0938,  1.9729, -0.0805,\n",
       "          -0.0781,  0.8020, -0.4573, -0.1383, -0.1622, -0.2350, -0.0807,\n",
       "          -0.1545, -0.0834, -0.0938,  1.3291, -0.0804, -0.0909,  0.9968,\n",
       "           0.4028, -0.4729,  0.4749, -0.2861,  2.8625, -2.2336, -0.3544,\n",
       "          -0.0767]])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can train and test model on the generated data\n",
    "synthentic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert array of tensor to tensors\n",
    "temp = torch.Tensor(99)\n",
    "synthentic_data = torch.cat(synthentic_data, out=temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.215778</td>\n",
       "      <td>0.474415</td>\n",
       "      <td>-0.211338</td>\n",
       "      <td>-0.215579</td>\n",
       "      <td>0.877716</td>\n",
       "      <td>-0.248438</td>\n",
       "      <td>-0.134396</td>\n",
       "      <td>-0.132359</td>\n",
       "      <td>-0.134139</td>\n",
       "      <td>2.270247</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157917</td>\n",
       "      <td>-0.033881</td>\n",
       "      <td>-0.208994</td>\n",
       "      <td>-0.134985</td>\n",
       "      <td>-0.110472</td>\n",
       "      <td>1.217278</td>\n",
       "      <td>-0.564559</td>\n",
       "      <td>-0.761483</td>\n",
       "      <td>3.851270</td>\n",
       "      <td>-0.499591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.215474</td>\n",
       "      <td>-0.207443</td>\n",
       "      <td>-0.608031</td>\n",
       "      <td>-0.133760</td>\n",
       "      <td>-0.126034</td>\n",
       "      <td>-0.133760</td>\n",
       "      <td>2.818491</td>\n",
       "      <td>-0.625777</td>\n",
       "      <td>-0.220521</td>\n",
       "      <td>3.437765</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092828</td>\n",
       "      <td>-0.215576</td>\n",
       "      <td>0.133350</td>\n",
       "      <td>-1.164985</td>\n",
       "      <td>3.035639</td>\n",
       "      <td>0.499088</td>\n",
       "      <td>-0.134386</td>\n",
       "      <td>0.523439</td>\n",
       "      <td>1.994684</td>\n",
       "      <td>-0.133760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.219625</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.168345</td>\n",
       "      <td>2.000838</td>\n",
       "      <td>-0.165377</td>\n",
       "      <td>-0.215360</td>\n",
       "      <td>-0.578755</td>\n",
       "      <td>-1.887420</td>\n",
       "      <td>-0.216038</td>\n",
       "      <td>2.540466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522390</td>\n",
       "      <td>-0.133762</td>\n",
       "      <td>-0.133764</td>\n",
       "      <td>-0.215110</td>\n",
       "      <td>-0.159208</td>\n",
       "      <td>-0.133848</td>\n",
       "      <td>2.671087</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>1.263887</td>\n",
       "      <td>-0.505945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.139862</td>\n",
       "      <td>-0.207352</td>\n",
       "      <td>-0.133790</td>\n",
       "      <td>0.154414</td>\n",
       "      <td>-0.212979</td>\n",
       "      <td>-1.354460</td>\n",
       "      <td>-0.140674</td>\n",
       "      <td>-0.165964</td>\n",
       "      <td>-0.215215</td>\n",
       "      <td>-0.215544</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320588</td>\n",
       "      <td>-0.496594</td>\n",
       "      <td>-0.188337</td>\n",
       "      <td>-0.214638</td>\n",
       "      <td>-0.214322</td>\n",
       "      <td>0.497751</td>\n",
       "      <td>-0.214454</td>\n",
       "      <td>2.015528</td>\n",
       "      <td>3.031071</td>\n",
       "      <td>-0.590596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.133760</td>\n",
       "      <td>3.044000</td>\n",
       "      <td>-1.730282</td>\n",
       "      <td>2.760940</td>\n",
       "      <td>-0.201644</td>\n",
       "      <td>-0.241702</td>\n",
       "      <td>-0.410702</td>\n",
       "      <td>-0.213721</td>\n",
       "      <td>-0.229968</td>\n",
       "      <td>-0.270861</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133809</td>\n",
       "      <td>-0.215568</td>\n",
       "      <td>-0.215200</td>\n",
       "      <td>-0.203412</td>\n",
       "      <td>2.953381</td>\n",
       "      <td>-0.133938</td>\n",
       "      <td>-0.177451</td>\n",
       "      <td>0.313484</td>\n",
       "      <td>-0.259314</td>\n",
       "      <td>-0.164502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.215778  0.474415 -0.211338 -0.215579  0.877716 -0.248438 -0.134396   \n",
       "1 -0.215474 -0.207443 -0.608031 -0.133760 -0.126034 -0.133760  2.818491   \n",
       "2 -0.219625  0.000146 -0.168345  2.000838 -0.165377 -0.215360 -0.578755   \n",
       "3 -0.139862 -0.207352 -0.133790  0.154414 -0.212979 -1.354460 -0.140674   \n",
       "4 -0.133760  3.044000 -1.730282  2.760940 -0.201644 -0.241702 -0.410702   \n",
       "\n",
       "         7         8         9     ...           19        20        21  \\\n",
       "0 -0.132359 -0.134139  2.270247    ...    -0.157917 -0.033881 -0.208994   \n",
       "1 -0.625777 -0.220521  3.437765    ...    -0.092828 -0.215576  0.133350   \n",
       "2 -1.887420 -0.216038  2.540466    ...     0.522390 -0.133762 -0.133764   \n",
       "3 -0.165964 -0.215215 -0.215544    ...    -0.320588 -0.496594 -0.188337   \n",
       "4 -0.213721 -0.229968 -0.270861    ...    -0.133809 -0.215568 -0.215200   \n",
       "\n",
       "         22        23        24        25        26        27        28  \n",
       "0 -0.134985 -0.110472  1.217278 -0.564559 -0.761483  3.851270 -0.499591  \n",
       "1 -1.164985  3.035639  0.499088 -0.134386  0.523439  1.994684 -0.133760  \n",
       "2 -0.215110 -0.159208 -0.133848  2.671087 -0.000193  1.263887 -0.505945  \n",
       "3 -0.214638 -0.214322  0.497751 -0.214454  2.015528  3.031071 -0.590596  \n",
       "4 -0.203412  2.953381 -0.133938 -0.177451  0.313484 -0.259314 -0.164502  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tensor to PD data frame\n",
    "synthentic_data_df = pd.DataFrame(data=synthentic_data.data.numpy());\n",
    "synthentic_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns of the synthentic dataset\n",
    "cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "        'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount']\n",
    "synthentic_data_df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.215778</td>\n",
       "      <td>0.474415</td>\n",
       "      <td>-0.211338</td>\n",
       "      <td>-0.215579</td>\n",
       "      <td>0.877716</td>\n",
       "      <td>-0.248438</td>\n",
       "      <td>-0.134396</td>\n",
       "      <td>-0.132359</td>\n",
       "      <td>-0.134139</td>\n",
       "      <td>2.270247</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157917</td>\n",
       "      <td>-0.033881</td>\n",
       "      <td>-0.208994</td>\n",
       "      <td>-0.134985</td>\n",
       "      <td>-0.110472</td>\n",
       "      <td>1.217278</td>\n",
       "      <td>-0.564559</td>\n",
       "      <td>-0.761483</td>\n",
       "      <td>3.851270</td>\n",
       "      <td>-0.499591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.215474</td>\n",
       "      <td>-0.207443</td>\n",
       "      <td>-0.608031</td>\n",
       "      <td>-0.133760</td>\n",
       "      <td>-0.126034</td>\n",
       "      <td>-0.133760</td>\n",
       "      <td>2.818491</td>\n",
       "      <td>-0.625777</td>\n",
       "      <td>-0.220521</td>\n",
       "      <td>3.437765</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092828</td>\n",
       "      <td>-0.215576</td>\n",
       "      <td>0.133350</td>\n",
       "      <td>-1.164985</td>\n",
       "      <td>3.035639</td>\n",
       "      <td>0.499088</td>\n",
       "      <td>-0.134386</td>\n",
       "      <td>0.523439</td>\n",
       "      <td>1.994684</td>\n",
       "      <td>-0.133760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.219625</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.168345</td>\n",
       "      <td>2.000838</td>\n",
       "      <td>-0.165377</td>\n",
       "      <td>-0.215360</td>\n",
       "      <td>-0.578755</td>\n",
       "      <td>-1.887420</td>\n",
       "      <td>-0.216038</td>\n",
       "      <td>2.540466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522390</td>\n",
       "      <td>-0.133762</td>\n",
       "      <td>-0.133764</td>\n",
       "      <td>-0.215110</td>\n",
       "      <td>-0.159208</td>\n",
       "      <td>-0.133848</td>\n",
       "      <td>2.671087</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>1.263887</td>\n",
       "      <td>-0.505945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.139862</td>\n",
       "      <td>-0.207352</td>\n",
       "      <td>-0.133790</td>\n",
       "      <td>0.154414</td>\n",
       "      <td>-0.212979</td>\n",
       "      <td>-1.354460</td>\n",
       "      <td>-0.140674</td>\n",
       "      <td>-0.165964</td>\n",
       "      <td>-0.215215</td>\n",
       "      <td>-0.215544</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320588</td>\n",
       "      <td>-0.496594</td>\n",
       "      <td>-0.188337</td>\n",
       "      <td>-0.214638</td>\n",
       "      <td>-0.214322</td>\n",
       "      <td>0.497751</td>\n",
       "      <td>-0.214454</td>\n",
       "      <td>2.015528</td>\n",
       "      <td>3.031071</td>\n",
       "      <td>-0.590596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.133760</td>\n",
       "      <td>3.044000</td>\n",
       "      <td>-1.730282</td>\n",
       "      <td>2.760940</td>\n",
       "      <td>-0.201644</td>\n",
       "      <td>-0.241702</td>\n",
       "      <td>-0.410702</td>\n",
       "      <td>-0.213721</td>\n",
       "      <td>-0.229968</td>\n",
       "      <td>-0.270861</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133809</td>\n",
       "      <td>-0.215568</td>\n",
       "      <td>-0.215200</td>\n",
       "      <td>-0.203412</td>\n",
       "      <td>2.953381</td>\n",
       "      <td>-0.133938</td>\n",
       "      <td>-0.177451</td>\n",
       "      <td>0.313484</td>\n",
       "      <td>-0.259314</td>\n",
       "      <td>-0.164502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -0.215778  0.474415 -0.211338 -0.215579  0.877716 -0.248438 -0.134396   \n",
       "1 -0.215474 -0.207443 -0.608031 -0.133760 -0.126034 -0.133760  2.818491   \n",
       "2 -0.219625  0.000146 -0.168345  2.000838 -0.165377 -0.215360 -0.578755   \n",
       "3 -0.139862 -0.207352 -0.133790  0.154414 -0.212979 -1.354460 -0.140674   \n",
       "4 -0.133760  3.044000 -1.730282  2.760940 -0.201644 -0.241702 -0.410702   \n",
       "\n",
       "         V8        V9       V10     ...           V20       V21       V22  \\\n",
       "0 -0.132359 -0.134139  2.270247     ...     -0.157917 -0.033881 -0.208994   \n",
       "1 -0.625777 -0.220521  3.437765     ...     -0.092828 -0.215576  0.133350   \n",
       "2 -1.887420 -0.216038  2.540466     ...      0.522390 -0.133762 -0.133764   \n",
       "3 -0.165964 -0.215215 -0.215544     ...     -0.320588 -0.496594 -0.188337   \n",
       "4 -0.213721 -0.229968 -0.270861     ...     -0.133809 -0.215568 -0.215200   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  normAmount  \n",
       "0 -0.134985 -0.110472  1.217278 -0.564559 -0.761483  3.851270   -0.499591  \n",
       "1 -1.164985  3.035639  0.499088 -0.134386  0.523439  1.994684   -0.133760  \n",
       "2 -0.215110 -0.159208 -0.133848  2.671087 -0.000193  1.263887   -0.505945  \n",
       "3 -0.214638 -0.214322  0.497751 -0.214454  2.015528  3.031071   -0.590596  \n",
       "4 -0.203412  2.953381 -0.133938 -0.177451  0.313484 -0.259314   -0.164502  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tensor to PD data frame\n",
    "#synthentic_data_df = pd.DataFrame(data=synthentic_data.data.numpy());\n",
    "synthentic_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 1 to Class column since they're all synthetic generated fraud data\n",
    "synthentic_data_df['Class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "data = data.drop(['Time','Amount'],axis=1)\n",
    "\n",
    "# Rearrange columns to the right order\n",
    "cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "        'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount', 'Class']\n",
    "data = data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (284807, 29)\n",
      "Shape of y: (284807, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(data.loc[:, data.columns != 'Class'])\n",
    "y = np.array(data.loc[:, data.columns == 'Class'])\n",
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape\n",
      "(199364, 29)\n",
      "xtest shape\n",
      "(85443, 29)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print('xtrain shape')\n",
    "print(X_train.shape)\n",
    "print('xtest shape')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the synthentic data\n",
    "X_synthentic = np.array(synthentic_data_df.loc[:, synthentic_data_df.columns != 'Class'])\n",
    "y_synthentic = np.array(synthentic_data_df.loc[:, synthentic_data_df.columns == 'Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_synthentic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add newly generated fraud data to the training data set\n",
    "new_X = np.concatenate((X_train, X_synthentic), axis=0)\n",
    "new_y = np.concatenate((y_train, y_synthentic), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(new_X, new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85291,     5],\n",
       "       [   34,   113]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/output/generator.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-ac5a9421c4e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"generator.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"discriminator.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/output/generator.pt'"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(generator, OUTPUT_PATH + \"generator.pt\")\n",
    "torch.save(discriminator, OUTPUT_PATH + \"discriminator.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
