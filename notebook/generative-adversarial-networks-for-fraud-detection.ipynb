{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import recall_score\n",
    "import torch\n",
    "#from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quadro P5000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if Cuda is running\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomize the data, just to be sure not to get any pathological ordering effects that might harm the performane of Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124011</th>\n",
       "      <td>77147.0</td>\n",
       "      <td>-1.437314</td>\n",
       "      <td>-0.672735</td>\n",
       "      <td>0.925002</td>\n",
       "      <td>-0.728117</td>\n",
       "      <td>-0.498919</td>\n",
       "      <td>-1.088232</td>\n",
       "      <td>0.098312</td>\n",
       "      <td>0.294362</td>\n",
       "      <td>-1.763211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032172</td>\n",
       "      <td>-0.254662</td>\n",
       "      <td>0.569343</td>\n",
       "      <td>0.599137</td>\n",
       "      <td>-0.909906</td>\n",
       "      <td>0.070153</td>\n",
       "      <td>0.017838</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>149.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182926</th>\n",
       "      <td>125586.0</td>\n",
       "      <td>-1.188026</td>\n",
       "      <td>0.648003</td>\n",
       "      <td>0.195708</td>\n",
       "      <td>-2.300317</td>\n",
       "      <td>-1.876858</td>\n",
       "      <td>0.483442</td>\n",
       "      <td>-0.257970</td>\n",
       "      <td>-4.314653</td>\n",
       "      <td>1.394378</td>\n",
       "      <td>...</td>\n",
       "      <td>3.773234</td>\n",
       "      <td>-1.129681</td>\n",
       "      <td>-1.190132</td>\n",
       "      <td>0.066215</td>\n",
       "      <td>0.806729</td>\n",
       "      <td>0.669240</td>\n",
       "      <td>0.580496</td>\n",
       "      <td>0.090244</td>\n",
       "      <td>392.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>2364.0</td>\n",
       "      <td>1.571567</td>\n",
       "      <td>-0.852696</td>\n",
       "      <td>-0.411170</td>\n",
       "      <td>-1.735533</td>\n",
       "      <td>-0.574247</td>\n",
       "      <td>-0.298426</td>\n",
       "      <td>-0.606201</td>\n",
       "      <td>-0.165091</td>\n",
       "      <td>-2.384797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.546954</td>\n",
       "      <td>-1.269700</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>-0.896239</td>\n",
       "      <td>0.433003</td>\n",
       "      <td>-0.425964</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>-0.003500</td>\n",
       "      <td>19.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10848</th>\n",
       "      <td>18552.0</td>\n",
       "      <td>-0.265284</td>\n",
       "      <td>0.081207</td>\n",
       "      <td>1.595559</td>\n",
       "      <td>-1.671107</td>\n",
       "      <td>-0.359368</td>\n",
       "      <td>-0.158713</td>\n",
       "      <td>-0.188054</td>\n",
       "      <td>0.271057</td>\n",
       "      <td>2.710338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102560</td>\n",
       "      <td>0.673973</td>\n",
       "      <td>0.126516</td>\n",
       "      <td>-0.065758</td>\n",
       "      <td>-1.159539</td>\n",
       "      <td>-1.066871</td>\n",
       "      <td>0.283226</td>\n",
       "      <td>0.230268</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250966</th>\n",
       "      <td>155148.0</td>\n",
       "      <td>-1.071928</td>\n",
       "      <td>0.594211</td>\n",
       "      <td>-0.352817</td>\n",
       "      <td>-0.819417</td>\n",
       "      <td>1.439513</td>\n",
       "      <td>0.755137</td>\n",
       "      <td>0.594540</td>\n",
       "      <td>0.448423</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085680</td>\n",
       "      <td>0.088365</td>\n",
       "      <td>0.361190</td>\n",
       "      <td>-0.336159</td>\n",
       "      <td>-1.006584</td>\n",
       "      <td>0.176926</td>\n",
       "      <td>-0.098098</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>14.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "124011   77147.0 -1.437314 -0.672735  0.925002 -0.728117 -0.498919 -1.088232   \n",
       "182926  125586.0 -1.188026  0.648003  0.195708 -2.300317 -1.876858  0.483442   \n",
       "2807      2364.0  1.571567 -0.852696 -0.411170 -1.735533 -0.574247 -0.298426   \n",
       "10848    18552.0 -0.265284  0.081207  1.595559 -1.671107 -0.359368 -0.158713   \n",
       "250966  155148.0 -1.071928  0.594211 -0.352817 -0.819417  1.439513  0.755137   \n",
       "\n",
       "              V7        V8        V9  ...         V21       V22       V23  \\\n",
       "124011  0.098312  0.294362 -1.763211  ...   -0.032172 -0.254662  0.569343   \n",
       "182926 -0.257970 -4.314653  1.394378  ...    3.773234 -1.129681 -1.190132   \n",
       "2807   -0.606201 -0.165091 -2.384797  ...   -0.546954 -1.269700  0.009546   \n",
       "10848  -0.188054  0.271057  2.710338  ...    0.102560  0.673973  0.126516   \n",
       "250966  0.594540  0.448423  0.063034  ...   -0.085680  0.088365  0.361190   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "124011  0.599137 -0.909906  0.070153  0.017838  0.001861  149.20      0  \n",
       "182926  0.066215  0.806729  0.669240  0.580496  0.090244  392.00      0  \n",
       "2807   -0.896239  0.433003 -0.425964  0.000977 -0.003500   19.75      0  \n",
       "10848  -0.065758 -1.159539 -1.066871  0.283226  0.230268   11.85      0  \n",
       "250966 -0.336159 -1.006584  0.176926 -0.098098  0.001136   14.75      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.reindex(np.random.permutation(data.index))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.174225e-15</td>\n",
       "      <td>3.429687e-16</td>\n",
       "      <td>-1.386421e-15</td>\n",
       "      <td>2.073779e-15</td>\n",
       "      <td>9.939598e-16</td>\n",
       "      <td>1.493625e-15</td>\n",
       "      <td>-5.931037e-16</td>\n",
       "      <td>1.318317e-16</td>\n",
       "      <td>-2.414318e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.416845e-16</td>\n",
       "      <td>-3.515296e-16</td>\n",
       "      <td>2.727492e-16</td>\n",
       "      <td>4.482012e-15</td>\n",
       "      <td>5.203181e-16</td>\n",
       "      <td>1.689590e-15</td>\n",
       "      <td>-3.712632e-16</td>\n",
       "      <td>-1.159267e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.174225e-15  3.429687e-16 -1.386421e-15  2.073779e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.939598e-16  1.493625e-15 -5.931037e-16  1.318317e-16 -2.414318e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...        1.416845e-16 -3.515296e-16  2.727492e-16  4.482012e-15   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.203181e-16  1.689590e-15 -3.712632e-16 -1.159267e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Frequency')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = pd.value_counts(data['Class'], sort = True).sort_index()\n",
    "classes.plot(kind = 'bar')\n",
    "plt.title(\"Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Class')['Class'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Data is hihgly imbalance. 284315 Normal transaction vs 492 Fraud transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run with Normalising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns != 'Class']\n",
    "y = data.loc[:, data.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape\n",
      "(199364, 30)\n",
      "xtest shape\n",
      "(85443, 30)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "print('xtrain shape')\n",
    "print(X_train.shape)\n",
    "print('xtest shape')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the training data and test data\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit classifier to a model\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85287,     4],\n",
       "       [   45,   107]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     85291\n",
      "          1       0.96      0.70      0.81       152\n",
      "\n",
      "avg / total       1.00      1.00      1.00     85443\n",
      "\n",
      "Accuracy : 0.999427\n",
      "Area under the curve : 0.851950\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print('Accuracy : %f' % (metrics.accuracy_score(y_test, y_pred)))\n",
    "print('Area under the curve : %f' % (metrics.roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run with Over Sampling data using SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.349671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.349231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.127897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.053373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.221892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10     ...           V21       V22       V23  \\\n",
       "0  0.098698  0.363787  0.090794     ...     -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425 -0.166974     ...     -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654  0.207643     ...      0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024 -0.054952     ...     -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739  0.753074     ...     -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  normAmount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0   -0.349671  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   -0.349231  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0   -0.127897  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0   -0.053373  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0   -0.221892  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data2['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "data2 = data2.drop(['Time','Amount'],axis=1)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (284807, 30)\n",
      "Shape of y: (284807, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(data.ix[:, data.columns != 'Class'])\n",
    "y = np.array(data.ix[:, data.columns == 'Class'])\n",
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape\n",
      "(199364, 30)\n",
      "xtest shape\n",
      "(85443, 30)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print('xtrain shape')\n",
    "print(X_train.shape)\n",
    "print('xtest shape')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over Sampling data using SMOTE\n",
    "smote = SMOTE(random_state=2)\n",
    "X_train_resample, y_train_resample = smote.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train_resample, y_train_resample.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85274,    17],\n",
       "       [   26,   126]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a Confision Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     85291\n",
      "          1       0.88      0.83      0.85       152\n",
      "\n",
      "avg / total       1.00      1.00      1.00     85443\n",
      "\n",
      "Accuracy : 0.999497\n",
      "Area under the curve : 0.914374\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print('Accuracy : %f' % (metrics.accuracy_score(y_test, y_pred)))\n",
    "print('Area under the curve : %f' % (metrics.roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run with DCGANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Custom DataLoader\n",
    "class FraudDataset(Dataset):\n",
    "    \n",
    "    # Initialize the data\n",
    "    def __init__(self):\n",
    "        data = pd.read_csv(\"creditcard.csv\")\n",
    "        data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "        data = data.drop(['Time','Amount'],axis=1)\n",
    "        \n",
    "        # Rearrange columns to the right order\n",
    "        cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "        'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount', 'Class']\n",
    "        data = data[cols]\n",
    "        \n",
    "        fraud_data = data.loc[data['Class']==1]\n",
    "        self.len = fraud_data.shape[0]\n",
    "        \n",
    "        self.fraud_data = torch.FloatTensor(np.array(fraud_data))\n",
    "        \n",
    "        #self.X = np.array(data.loc[:, data.columns != 'Class'])\n",
    "        #self.y = np.array(data.loc[:, data.columns == 'Class'])\n",
    "        \n",
    "        #self.X = torch.FloatTensor(self.X)\n",
    "        #self.y = torch.FloatTensor(self.y)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.fraud_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FraudDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=5,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator's paragrams\n",
    "g_input_size = 30     # Random noise dimension\n",
    "g_hidden_size = 50   # Generator complexity\n",
    "g_output_size = 1   \n",
    "g_learning_rate = 0.0002\n",
    "\n",
    "#Discriminator's paragrams\n",
    "d_input_size = 30   # Minibatch size\n",
    "d_hidden_size = 50  # Discriminator complexity\n",
    "d_output_size = 1   # Single dimension for 'real' vs. 'fake'\n",
    "d_learning_rate = 0.0002\n",
    "\n",
    "minibatch_size = d_input_size\n",
    "\n",
    "num_epochs = 50\n",
    "print_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ELU (Exponential Linear Unit) function tends to converge cost to zero faster and produce more accurate results\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.sigmoid(self.map2(x))\n",
    "        return self.map3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.map2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.map3(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "discriminator = Discriminator(input_size=d_input_size, hidden_size=d_hidden_size, output_size=d_output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these 2 lines to run on GPU\n",
    "#generator.cuda()\n",
    "#discriminator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(disc):\n",
    "    h=0.1\n",
    "    x_min, x_max = data[:, 0].min() - .5, data[:, 0].max() + .5\n",
    "    y_min, y_max = data[:, 1].min() - .5, data[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    x_test = np.c_[xx.ravel(), yy.ravel()]\n",
    "    y_hat_test = disc.forward_with_sigmoid(Variable(torch.from_numpy(x_test).float()))\n",
    "\n",
    "    plt.pcolormesh(xx, yy, y_hat_test.data.numpy().reshape(xx.shape), cmap=plt.cm.Paired)\n",
    "    plt.colorbar()\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y*20, alpha=0.1, cmap=plt.cm.flag, s=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Binary Cross Entropy loss\n",
    "BCE_loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizers\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=d_learning_rate/2, betas=(beta_1, beta_2))\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=g_learning_rate, betas=(beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Discriminator Loss: 1.216, Generator Loss: 0.626\n",
      "Epoch 11 - Discriminator Loss: 0.333, Generator Loss: 1.792\n",
      "Epoch 21 - Discriminator Loss: 0.242, Generator Loss: 2.543\n",
      "Epoch 31 - Discriminator Loss: 0.531, Generator Loss: 2.453\n",
      "Epoch 41 - Discriminator Loss: 0.310, Generator Loss: 3.593\n"
     ]
    }
   ],
   "source": [
    "# Training DCGANs\n",
    "for epoch in range(num_epochs):\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    synthentic_data = []\n",
    "    for i, fraud_data in enumerate(train_loader):\n",
    "        # Updating the weights of the Discriminator\n",
    "        discriminator.zero_grad() # Initialize gradients of the Discriminator to 0\n",
    "        \n",
    "        mini_batch = fraud_data.size()[0]\n",
    "        \n",
    "        # Wrap data in PyTorch Variable\n",
    "        d_real_data = Variable(fraud_data[0])\n",
    "        y_real = Variable(torch.ones(1))\n",
    "        y_fake = Variable(torch.zeros(1))\n",
    "\n",
    "        # Training the Discriminator with real data\n",
    "        d_real_result = discriminator(d_real_data) # Forward propagate this real data into the neural network\n",
    "        d_real_loss = BCE_loss(d_real_result, y_real) # Compute the loss between the prediction and actual\n",
    "        d_real_loss.backward()\n",
    "    \n",
    "        # Inject fake data to the generator\n",
    "        d_gen_input = Variable(torch.randn(minibatch_size, g_input_size))\n",
    "        d_fake_data = generator(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "        \n",
    "        # Train the Discriminator with a fake data generated by the Generator\n",
    "        d_fake_result = discriminator(d_fake_data.t())\n",
    "        d_fake_loss = BCE_loss(d_fake_result, y_fake)  # zeros = fake\n",
    "        d_fake_loss.backward()\n",
    "        \n",
    "        # Combine discriminator loss from real data and fake data\n",
    "        d_train_loss = d_real_loss + d_fake_loss\n",
    "        \n",
    "        #d_train_loss.backward()\n",
    "        d_optimizer.step()     # Apply SGD to update the weight\n",
    "        d_losses.append(d_train_loss.data[0])\n",
    "        \n",
    "        # Update the weight of the Generator \n",
    "        generator.zero_grad()\n",
    "        gen_input = Variable(torch.randn(minibatch_size, g_input_size))  \n",
    "        g_fake_data = generator(gen_input)\n",
    "        \n",
    "        dg_fake_result = discriminator(g_fake_data.t())\n",
    "        g_loss = BCE_loss(dg_fake_result, y_real)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        g_losses.append(g_loss.data[0])\n",
    "        \n",
    "        synthentic_data.append(d_fake_data.t())\n",
    "        \n",
    "    if epoch % print_interval == 0:       \n",
    "        print('Epoch {} - Discriminator Loss: {:.3f}, Generator Loss: {:.3f}'.format((epoch + 1), \n",
    "                          torch.mean(torch.FloatTensor(d_losses)), torch.mean(torch.FloatTensor(g_losses))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a20459b38>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(d_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a20474198>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzsvWmUJNd1Jva9WLMys6p6qeoFa4PYN4ILRJMELRIiRYISJdNjyTocS2NxzhGPR4ul0TLmnLFnPBqNRY10RqPl2BKloUb2yB7tI5E2CZEiKRAkRRIUABKNHWhia/RS3bXmFqt/vLgRL168iIzIzKqM6o7vHBxUV+USGfnixve++917WRiGaNCgQYMG+wfavA+gQYMGDRpUQxO4GzRo0GCfoQncDRo0aLDP0ATuBg0aNNhnaAJ3gwYNGuwzNIG7QYMGDfYZmsDdoEGDBvsMTeBu0KBBg32GJnA3aNCgwT6DsRsvurKyEp44cWI3XrpBgwYNLkl8/etfXwvDcLXMY3clcJ84cQIPPfTQbrx0gwYNGlySYIy9UPaxjVTSoEGDBvsMpQI3Y+wAY+xPGGNPMsaeYIy9ZbcPrEGDBg0aqFFWKvk1AJ8Kw/D7GGMWgPYuHlODBg0aNCjA2MDNGFsC8O0AfhgAwjB0ADi7e1gNGjRo0CAPZaSS1wA4D+D3GGMPM8Z+lzHWkR/EGPsQY+whxthD58+fn/mBNmjQoEEDjjKB2wDwBgD/RxiGrwfQA/Bh+UFhGH40DMO7wzC8e3W1lKOlQYMGDRpMgDKB+2UAL4dh+JXo338CHsgbNGjQoMEcMDZwh2F4BsBLjLGbo1+9E8Dju3pUc8bXX1jH46e35n0YDRo0aKBEWVfJTwD4g8hR8jyAD+7eIc0fP//xkzjctfGxH/62eR9KgwYNGmRQKnCHYfgIgLt3+Vhqg4Hrozfy5n0YDRo0aKBEUzmpgOMFGHnBvA+jQYMGDZRoArcCrh9i6PrzPowGDRo0UGJXmkztd4y8AFbDuBs0aFBTNIxbAdcPMGoYd4MGDWqKhnEr4HgBdI3N+zAaNGjQQImGcSvgNIy7wSWEf//gKfyzP//mvA+jwQzRMG4JfhDCD0IMG427wSWCLz+3hmfP7cz7MBrMEA3jluD6PGD7QRj/3ODyxCsbA/zcHz+Kkbe/d199x4fTEJFLCk3gluAIwbrxcl/e+OKza/jjr7+Mb631530oU6Hn+HD8cN6H0WCGaAK3BJGZzNvL/dgrm7j/5Jm5HsPlDKqe7Tn7u4p24HjN7vESQ6NxSxAX+LwD98cePIWvnLqI99x+bK7Hcbmi7/Dvf+Dsb6mkN2qkkksNDeOWIC7weUslo6b0fq6IGfc+71szcP1dYdwff/Q0njqzPfPXbTAeTeCWUCepxPEDOPs8MbafQQF7sM+tob2RBy8IEQSz1bn/+V88hv/4ty/M9DUblEMTuCXUKTnp+kHqeBrsLXqRRNIb7d/A7QdhvI5nvZYcL2gkmDmhCdwS6sS4XT+A27gB5oZ+lJTs7+PkpHjssw7crh82xGJOaAK3BDFQjtw5M24vjAuCGuw9iGn393FyUkysujNkx2EYwg2aHeG80ARuCenk5Pw1bgDNdnROuBTsgOJNZ5a7Nz8IEYbN2pwXmsAtwfGThT6cN+NuAvdc0bsE7IDiTWeW64huAo0/fD5oArcExxOkkjkzbi+6OJrt6HxA+vB+Tk6KN51ZriM34K/VBO75oAncEpxUAU5NGHdzccwFJJXs5+RkTwzcs2TcXrMbnCeawC3BrZGrpNG454uyycmnzmzXVk4ZCDedWbJjL6DdYJM4nweawC2hbj5uoAnc84AfhHHhTRHj3uy7eN9vfAF/9NBLe3VolSDKPLMM3E7DuOeKJnBLqFOvEkoAzfvi+MQ3TuOej3wW3mUk2YjBuohxnzy9CdcPsdF39+KwKqPv7pJU4jca9zzRBG4JdepV4u5SxVtVPHeuh1c2BpfVcAkxWBcF7sdObwJIu5HqhMEuFeDEUslltCbqhCZwS6Bg3bWNuTPuumjc5K65nBg3JSZbplbYZOqxV7YAzNYjPUuIUsks1xG91jjGff/JM/ilTz05s/dtwNEEbgm0EBdbxkSuEj8I8RePvDKThj51cZXsVq+LOoNY9uqiXZh4jBl3TZmn2CBrljcXYtzjAvdnHj+LP66p/r+f0QRuCY4XwNAYFix9Ih/3l55bw0/+p0fw8EsbUx2HH4Sg2D/voEDnoa6scjewE7Hs1a6NnuMhDLOffWfk4dRaD8D8ZbU8iLuFWerR9FrjPrcfhHGQbzA7NIFbgusHMHUNtqFPxLgv9hwA0yc2xYts3gkg6tkyy14XdQclJ1cXbQShOkA98eoWKJ7P+zvKw8DxoTH+8274uMd9bjcI40KyBrNDE7glOF4Ay9DQMrWJGDe5C6a9kEVZYv6M+/JzEJA2vLpoA1AnKB97hcskXduY+3eUh57jYXnBBDDryslyyUnPD+AF9Tw3+xmlRpcxxr4FYBuAD8ALw/Du3TyoecLxw4hxaxN1B0wC93QsQ2S38w4KdAO7nDTuXiyVtOJ/H+pYqcecPL2Fla6NQx1z7t9RHvqOjwNtC+t9d1cYdxByOUQnWi/Baxj3rqDKzMl7wzBc27UjqQkcL4BtaGiZeix7VMHmYDaMO9Vedt5SScy4L58LsOekGbdqCs5jr2zijiuXsLYzqu1upO/4MeOebeVkWsrTNV39OD+AF4QIwxCMqYN7g+qolVTyY3/wd3PPQDt+AFNnaBn6ZIx7wIP99IG7Rozbvfykkn7EuFe6nGXLlsCh6+OZczu444plWLpW290IZ9yRVDJLO6BILApelxKTTU/52aJs4A4B/BVj7OuMsQ+pHsAY+xBj7CHG2EPnz5+f6GD+5unzcx8+6kYat21qGE6gcW/OSCpx6pScJFdJTeWA3cCO48EyNCxFbFXWuJ86sw0/CHHHlUswda22rpKB42GpNXvGLa6FotclmaRxlswWZQP3PWEYvgHAewH8GGPs2+UHhGH40TAM7w7D8O7V1dWJDsbU2dyDlONHyUlDn8gZsjEzqaQ+jJtuIu5ldPH1Rz46lo6OxdVEOXCTf/v2K5ZhGdrc120eeo6Pjq3DMrSZNoQSpZKi9ek17V93BaUCdxiGp6P/nwPw5wDetBsHY+qzXVyTgOyA3FVSfbGRxj1tlaEr9AWfd+C+HO2APcdDxzbQtrl2KzeaeuyVLSwvmLjq4AKXSmp6bgaOjwXTmPkxitdpUVCmnWcjlcwWYwM3Y6zDGFuknwG8G8Bju3Ewpj5/5jLyAli6BtuckHFHUsm0N6CUHbAG5wS4vFhTb+ShYxloWxS402vh5OlN3H7FEhhjtWXcYRii53hoW/rMj9EruSP04wrLJnDPEmUY91EADzLGHgXwVQD/bxiGn9qNg6nDBeDGUgln3KqKuTyEYYjNKDk5NeOukVRyOdoB+46Ptq2jHUklcgXik69u444rlwHwdTvv70gFvn6Btq3D1NmudAcEitcFPa7xcs8WY+2AYRg+D+CuPTiWemjcXgCrzRl3GPJFaRtqq5OMvuPPbBZf2QtjL3BZ2gELGPeLF/tw/AA3H10EEEl8NQzcdLNpm/rMd7NuSirJXxeUlGy83LNFreyA/AKY7xfseEFcgANUG19GiUlgeqmkVoz7crQDRkk9U9dg6VoqcJO/nzzes078zQp0zG3biI5xlxh3UXIyZtz1Oz/7GbUL3PMODrFUYnKmVaXsfVNopi9LJWEY4iOffBLPntsp9VpOTZKTYRgKTaYun8C9EzFugEsNYnKSAjdVUvLE33xaAJ88vRk3xJJBRUNtS595crJsL52Ecacfc25riF//62cqSZENEtQqcFs1CNzUq4QYd5UiHCq+AbKLeXPg4rf+5jncf/JMqdei52tsvgHTq1GXwr0EadwAlxpUjDsO3DNms1XwA7/9t/jpP3xE+TeSSjqWsQvJyXLEIs/H/ZknzuHffvppvLIxmNkxXU6oVeA2jdkmUCYB9SqZlnHLuh9d2HnsSAZdZB1rvg2MRqlCi8uHHfVG3A4IcKlhHOOex7nxgxA7Iw9/9fhZfOm5bDcK6iO+YOmR1XaWdsByORhKSsoad9m2sA3UqFfgrgXj9mEb02vc8uegC3tnWDFw28Zck5MjwRJ5uUzA8fwAIy+IpZKOpacmyVzsOWhbenxztwwNfhDuuVdZXGO/8IknMu9P/VbaFneVuDPMH3klfdzxwIVAvh74v+c9ZWq/onaBe95JHtcPea+S6KKssrCo+GZ5wcwGbq8a46bz0LH1ubKSNOO+PAK3GPAAzljFKTjrPQcH20mnQFPnl9Fe74zou3nd1Qfw+Ktb+NO/ezn1d9oltC0DlqHPKTmpLsChgN4w7slQq8BdC407k5yswLj7Lixdw9KCkSuVbJdl3MLsy0kCQhiG+LXPPIMzm8PKzxUhfv5531T3ChTwujYxbgM9USrpOzjcTQK3Fe3O9npnROviv379lXj9NQfwy/c/lfKb9x0xOTlbGdLxg7iVa3HlpNqRRLu3SRq5NahZ4J63j5u2u2k7YBXG7WC5bSolHydm3K7qqRmIUskk5+T05hC/+pmnSydD8+Bcjoyb/M8pjTstlYiM29J5ANtrxk3fh21o+F/edxvOb4/wsQdPxX+nY96t5CTtSMpUTsoaN5GASYaVNKhZ4LYMba79MGhhi4y7ksbdd7G8YCp3DvTvqsnJ9oTJSdrabw/L3SjyIF5Yl0/gpoAnukrSyUlxqMK8GbdlaHjDNQdxy7FFPPpyMuuUWtPuRnLS9ZMcQN5OLAzD3LauMeNupJKJUKvAPW+NmxaRFTWZ4r+rpnEfWDBh6CwjlVRNTooa9yQXHO0Uykozebg8Ne5EGwYiH7eUnFQF7r0mHY5ANADg2HILZ7YSaazv+jB13kvF0mdLilw/jO2SeetCtABmpJLob01ycjLULnDPMziIjJvK3Ksy7gNjpZLyjNuKqvYmYdxx4C75fnkQNch5V7XuFShIk8bdtnT0XR9hGGLo+ug7fipwx8nJOTFuev9jSy2c2RzFf+edAfk6Nmdc3Zli3DnrU2TZMuNu7IDToVaBe95NppwZMO7lBUsZuOnfVZKTxJYmOSd0w5mecV+GUgkxbirAsQz4QYiRF2C9n/ZwA3y9AHuvccuM++hSCxd6yRi13siLdw3jqjvDMMQLF3ql39v1+Yg/XcvPS6WqKzNSSeMqmQa1CtzzTk7Se/Pk5CSM28Hyghl9DrWrZOQFpS5w1w9gGtrE01Wo3Hlnao17/0klL1zoTbUFTzTuxMcN8GTfhR0euFPJyTlr3LaeSCVhCJzb5qy77ybVn5wA5DPuzz11Dvf+yudxumQloxcVqhV1HRQTknINABXmjBqpZCLULHDzxTWv/gVisqeqq8T1A/Si+X6mrmUWqhj05PmFymMRps1PJZXMiHF3LH1fVE4GQYj3/fqD+NVPPz3xa1AisiMwbvp9rRg3SSVGIpUAiC2g/ZEXOz9MnRXeWF5eHyAIgbWdUe5jUu/tBzB0Vpj0FDVuueTdbRj3VKhd4AbmV1o9EjRDTeMyRdmFRcU3FLhlPVG8qMvo3LHGHfXBqHozG8wqcEc7jm5rMlviXmPH8bA98vDJx85MTADo+xGTkwBn3HK5OyAkJ/f4/MQ5GT2RSgDgbJSg7Du+IJXohdWd6z2+fsUK0SJ4QTRwpIBYiD24ZTtg4uNuGPckqFXgtvT5XAAE0RdL/y/LuGnyDUklRYy7TDDlI9QYLF1DGFZvi0kXRNlkaO7rCIVA+yFwb0U30Bcv9vFMyU6MMvqOj5apxQUm4tzJosA9L8YtukoAgXE7fsK4jeJiGWqQJo9oy4PrkVSSn4NJSSVyyXtTOTkVahW4zTkVMhDkC6Flli83p8k3PHArXCXCIi7jrabZl+aEbI60+a0Z+bi7rWwZfx2xKfSL+fTjZyd6jZ7Q0hXgPmiASw/rPQeM8e+ZMK+Sdzk5ebBtwjI0gXEnn8Ma43wh4tFzykuDRpw8V5MKcb3kMu4mcE+EegXuOW05CbQA6UK0Da30Vi6RSiwYWnYxuxWlEidiNJPqpwOBcQdTND8iqWTRNvZFyfvWgJ9bS9emC9x2Ergp+PUcHxeiqkli48D8kpNi3QEAMMZwdMmOvdwDx49vOuO85huRdt8va1eNpJKi6T9+UMC4/b31cX/m8bN4+y9/7pLxjdcrcM/JD0twfP6liox7WNIOSIzlwIIJy8i6Y8TPVFbjNg1t4m04LdAw5O6CSTHyeE+KlqnviynvdAN9561H8MhLGzi3Xb1XS0+QGACBcUfJyYNtM/X4eSUnXYlxA+Tl5p9Z/Bzjrq31qozbC8cmJ4vGm+21j/sbr2zihQt9nN8ul3ytO2oVuK05JyepwIQkm5aplW6CEwfunAIcMeiV1bitaCsKVF/goo1xmrL3UdTmVnUzqiNIGvpv3nAVAOCvnzhX+TX6jsS4peTk4Y6devy87YB03QA8QXlWYNxtSSrJa+1KN7yyjNsLuJRXNChZZNnZkve97VWyGe0oyrpm6o5aBW5zzslJJ5OcrMC4o4W/2DLVUskkjFtodlX1nAwElj2Ns8TxeKHFvKtay4KSk9923SFcdXABn5lALtkZ+anA3ZaSkwc7asa95yXvXg7j3hrC9QM4fiAkJ+nmol7PZHMsy7hpNqtVUHuRsgPm+rj35pzRjoJ8+PsdNQvc9UhO0g2kCuPeGrhYahnQNQZTKZWEsHQNGivXr8Txk6w9/3e1czKaUeAeeXzKvepmVEdsDVwwxjX5d916FA8+u1baKUHoj7y46AZI+nL3Rx4u9tyUowQQg+J8S94B7iwZukEsl9CxJx0Ms99hEIQJ4y55rrwgHFvZmxq2MGcfN92YLvQaxj1zzMsPS5A1w0qMu+/gQFscZSUF7miWZdc2yjHumNFMl5wEppVKAtgml0rmOYmnLDYHLpZaJjSN4d23HcXIC/CFZ7JjvYog+p8BxN/DTqRxy4F73ho3ER4g8XI/v8bL12OppODa2h56IMt7WR937HoqSE6KLDs7SIH/ba+ShSRlrjWMe/aYv8ad1gxbpla65H1jwBtMAYChaQjC9GIlX/ZiyyyvcRtsquQkySzj3q+oUIU0blU1aB2xNfSwtMCD1bdddwiLtoEHKwbunuPFujahbes4uzmEH4SpcndA2CmWXLdD10/NJ50Uo2joB2NJ4CYv9/PnuYedPkfRzo3YKFCOcYdhCNcPYVDgzvnchd0B58W4m8A9e8zfDpguIW4ZeunkCfXi5s/PFju40UW22DJKDVNw/QCGpk2c+Bq6AVYXeRKtiOH/9B8+gn+cMyUc4BqkbehxO4K6gzf6ir4HXcPqkp0KTGUg2wEB3pP75XXex0OcfgNwG16VLo4f+eST+P7f/lKlY1LB8YK4TwmByt5PRYybugMW6fDirNQyGjcFZEqe5zWvKqqcTFwle5WcjDTuRiqZPeZtB5R9sXYFxi0GDFUFqBNtLUtLJbLGPYFUQoG7SCo5eXoL//mR0/jqqYvKv48iiWfWjfh3C1vC9wBELVlLJtwAfp5dP0xp3ACfgkOBW2bcAArdFTK+euoinjvfm3q4sOMFMckgHFni3/kpSSqhx40KGPeCqZdyldC6NiIJKb8AJ79XyV7OnHT9IG5v3DDuXQBtOeflF5Z7P9iGXqkAJ5FKiHEni9XxeMFCt2WUTE5yqcSeQio53LHBxiRDqYXpL37yCaVkEtsBI/fAvBqAlQVp3IS2ZVRKTvalIQrJ6+g4G3nCZY0bKN+S2PECPHNuG34Q4sKU1jTqZyPCNnQc6lh4/nwUuO3xjJvY6JUHF0oxbrFQrWin4Re5SvawAGdDkKUaO+AuoA4at6ExaBr5uHUMSwTMIAh5cnKBX9DEbjxJKiHGXWa4AT1+cqmEF190bQNbBYG77/g43LHw8Isb+NRj2fmUI8EOGEq6/QsXenjDv/o0XrrYr3Rsu4mt4XSMm3ZDXVkqsfQ4gacK3EXtTUU8c247Xt9nt6YLIpTwlnF0qYVXovasbblyUnFtEeO+4sBCqZtcQnDUDir5cUCWcYttjncbVBXatQ1c6DWMe+aYu487cnIQqPPZuJLxHcdDECKlrQLpYOv6YaJxl0lOTukqGboBWqaGRdsoTE72Rh7e//orceORLv7N/U9lzn2scSsu/OfXerjYc/CtCg34dxubAxdLQuDuWEalwB1PRpeSk2LvkmkY98nTW/HP4pixSeBECW8Zx5aSAiE67mRNZs8FMdIrllulXCVpqSR/tB6xal1jWakk7g64+9c6ebivX+3gYs+ZqgVEXVA6cDPGdMbYw4yxT+zWwczLD0ugBCKBBgaPOx7aai63KXBnpRJylVTWuCdtMuX5aJk6FltmbjLU8wOMvABLLRP/03234NRaD3/4tZdSjxl5PmxT7ScnGWlQITDuJkaej6EbpBj3glVOtyVQr/SOJJVQ2bttaHHCT4Sla0r9WMbjp7dAbU6mDtxeAMvIHgs5SwAoepVkg9ZG38FSy8Biq5ys5AlSiWnk7zRod9Yyso4ksXJyt+U3YtzXH+nCFzzr+xlVGPdPAnhitw4EqEEBjhS4yw5TiBtMSYxbXKwjj6QSE33HL0xMhWHIjyVq6wpMkJyM5g0utvIZN/Uw6dg63nnrEdx19QH8P199MfUYkkosPeuUoW1uGTlpL0ANppZaYoMovVKvFmKcbUvNuA93rJT9jlDkZxbx+OktvPaqA9A1hrOb0zLuMFcqISSMmx+z6ubCraxWlA/wxzJSunlT2+FcqSRylbRMPTtIIfpbMEHL4qqgHcUNR7oALg1nSanAzRi7CsB3A/jd3TyYMv24f/Ozz+BH/s+HduX9HS9MJXuIcY9zloi9uAHu4wZkqSQqwImCSpFcQgtZ1LiraIFhGEaFMzq6RYE7DlIGGGO4YbWLdUkDjCsnFd8N3dDq0nGNbqBLKcZtpCa0j0Mvnn4jadyRdHJQIZMA/CY/blcUBCEef3ULd165jCOL9gwYt5+xAwKJJZAxxLNTbT2ayK6yA/ZdHGybsed7MOb79KTkZBBmk4/i41qmrmTcdK3t9vohDf+GVR64L4UinLKM+98B+CcAclcmY+xDjLGHGGMPnT9/fqKDKaNxf+PlTfzt8xcmev1xkBl32YHB1IQ+rpyMfNyeJJVYOtecAWC7wMst+sntCeQjCvItU4ukEnXg7kkjuhZb2cTpyE0KcOTPFDPumgRuajCV1ri5BltWaurnBe6Igav0baAc437xYh87Iw+3X7GUagY1KbgdMMv+j0ZSyYKpx7sDepxqHW30HSxHjBtI1kUexNmsqtyH/LiWqaVYdRiG8IIwJjG7naBc77swdYZrDrcBXBqWwLGBmzH2PgDnwjD8etHjwjD8aBiGd4dhePfq6upEB1NmdFnf8bE99HZFV6XJ6oSyA4OJPdNCVN2AKPEZM+4C3dX1BA1xTFc3FejcLJjcVZLn4xYZN8AlBrl/t+MHkcadvfDryrhljRtA6QRlMihY8nFH5ygvcJfxcT/+Kk9M3n7FMu+bPaVU4vphxg4IJIxbLtsH8gtwRMY9bodCa8AQpTzFDSHWuE1dmobDf6b3Gxe4eyMP//dXXsQP/PaXJ2oaRu0oVro8aXspSCXG+IfgHgDfyxj7LgAtAEuMsf8YhuEPzvpgymjcxAbObQ9x7eGO8jHbQxd/9NDL+OBbT8TWvjKYlHFT4KKklVoqCWFGvUqAYqkknmyiM+ga/y+vq5vyeKLjbZk6llr5dsCYcUdBqtsy4v7dXduAH/DSZtvQlTIW3dDKFintNqgzoOjjJuY8cPxUQM8DJSfbOYxbVXwD8MA9bgj0ydOb0DWGG492cWyphS89N93OMc8OmATu5OZjaAyMqXez6z0HBxbM0oybgrAlMG7VNUsBesHUlVWUpL+raiWCIMTfvbiOv3z0NP78716Jd4I3HV3Eu247Wnh8Mjb6Lg4smDjYtsDYpSGVjA3cYRj+UwD/FAAYY+8A8LO7EbSj14dZ0CYSSC6sc9uj3MD9B195ER/55JN46/WHcevxpdLvn7UDlmPcAzfZEgJqqcSJ2Dwx7iIvt7gVBVCpnBpIM+7FlgHHC6JCmjSLjItNoiC1GAW87aGLrm3E7ylKJSLzpxvaOE10r7ClYNwUvMYFI0Jv5IExXuIuQkxOqmDq4wdLP356Czce6aJl6ji63ML20EPf8TLFPmXBiUbWVXIgGmEmBm5+bWWdL34QYmvo4UDbSs3WLEJsB9SYMmktP65l6qkdJiUmicSI15frB/iV+5/CXzxyGme2hrAMDd9953H84Juvxc/+8aOV2xcAiIZf8KlFh9rW1IVPdcBkK2YXMa7vM21li/RBKiSpGlAcqRKNAvE4KYD+3jLSDX3kXiW2kWjcRYw7E7grlFPz40luJCLDt7vpi1yWBVK7geUkMNuGprRqJoy7JoE7OqfUZApI5IKy0trOyEfHMjI7tXHJScsY3xLg5OktvO3GFQAJKz67NcJ1KxMGbk/t42aM4dhSK+OMsXUtI7klI/fM+DOO2zmIOZiiJmh+EEJjfCedrqLkPycad/LdPHVmG7/9wPN482sO4cPvvQXvvPVITCgOts1UFWRZbPRdXBvp24e71uWhcYsIw/DzYRi+b7cOBqBChiKNO2LcOVVnr24O8MhLGwCqBxR560muknFMauj5sAwtvthJKlH1KqFFWKhxS82uygQF+XgAwI583Hnvl2Xc/P8UAOPeLYYueNNVGnc9pJLNgYuWqaV2FjHjLunl3hm5mc6A4uvkMW57DOE4vz3Cue0Rbr9iGUASuKfRuZ2IDKjw2quWcX3koiCYhpaR3MjjfLAS4xakkgJDgeuHMDQNhnRu6GciCuL1RQ6o//GdN+L9r78yXr90jBcnqHwkxg0Ahzv2ZaNx7ynGNTOKGXfOLMH7hbLtqtlqufdDWR/30PHREi4gy1AU4MjJySKN20u6r/H/a8oG+EXHAyRSCaBu7Soz7uSxnNVQVRv3cWcv0Nq5SqQ+JYAwBKHkMfak6TeEqw+2oTHgulW1PDfOVUKJydsi6Y6cH9M4S6j/jQq/8YEUn5NDAAAgAElEQVTXZ/zmps4yjHtdKB4re5NLKieLk5OeH0RzKZmyxasqcMeuHoV8dKBt4YlXtzK/L0IYhlzjjqYWHe5aePx0tdeoI2pV8g5EQwhyLgDHC+IFcj6HcX/q5JkkqbhXjNsNYvcCkCeV8GKJtqmDsYoat8S4T6318PMffzy3UEJMTnZjFp3dYsoNlWR2HkslpppZ1dFVIicg47FjJb3cOyMvlrNE3Hh0EY/+i3fjlmPqnMk4Oevk6U0AwG1X8OfHjHvawJ3DuFVFQqqy/E2ysi6Y8Q2rrMZtlkhOGhqDrmlKqaQTa9zJ+9HaU908D3XM+EZTFgPXh+MHMeNe6dqXRKOp2gXuouSkWI57TjGt+cLOCF89dRHvupVnnSdh3HKvEqAE447KywlysQpVQpo6l1O6VnG/EnVyMjmGTz9+Bh/74qncXcfA4c9fMPWYgarer+f48fgpIGFA25JUwgtwsqOv6qdxp/uUAALjLpmc3FH04iYstvJdKeMkvsdPb+GqgwvxjaVjG1i0jamlEjOHcaugSk6SZnywbZVO5MbdATUtLgBSfXYvCPiwBS19TXtBPuOmXaDc5AvgjHvg+pXWGwV6qmo+3LGwNfTmVp09K9QwcOdfAGLLSdUW8zNPnEUQAu9/3ZUApte47bhysvh1Bo4fJyaBbK+SRBPkv++OGabgqBi3sNCotDtvSxsnS02tUCrpj9KOhkVJxhGTk7QlFm1ddXOVqBl3VR93fuAuwjip5Pz2CFceWEj97siSPbFU4gch/EBd8p4H1W42DmxtE7ahQdfY2N2JF+dgWDHj9jnjNnQmFaNFyUkK3ML6SeyY2TwDseYqzhKqBKbiuMOXiJe7loE7T+OmZkGHOpaScX/ysTO45lAbr7vmAIDqSTOZwSQ+7nHJyQAtQSqR9WCZQY9rNBUH+kgr57uQZOGT7JFXyp6SSuy0bi2i5/ipQpOOZXAZR6Fxq6SSUe183F6qTwmA0tt/Qp5UMg4kZ+U1TNoZeRkWeWy5NbFUIs9HrXKMIjb7Dhjj3nfGGNqWXq1ysiA5yQcKa9A1TZr4npZKUoy7QOM+GDVxW++Vl0uSHUWicQPVqyf/4X/4Gv6vL3+r0nN2E/UL3AU9HyjYXbfSwebATTHhraGLLz67hvvuOCZo09UZt5ilt3QNjI3XyoduOjkpSyXyRVbUPwRIqtvyGTdfjHnBn6xvosad5yoRC01IxtkeSVKJKbR1FaUSr/4at23w73AWUkkRaN3k7hYVr3t0qTVxoyl5WlMZqBpCrUcj98gR1SnR20WUSuj9VeTG8wPoGiUnhXxP9LOqcrI38rBg6tAVhXNkxazEuMk10yGNm/+/qs79xWfX8MhLm5Wes5uoXeC2ChrSE2s6ERXenBdY9+efOg/XD3ngjrXp6oxbZDCM8Qk045OTaY1blkpk6WM845YDt57SJsmul6eTi71KbEOHZWi5rhK5tFvsJpjycStK3kc10riDIMS2QuNmjFXqyT25VJLfCwTg/vCutBs4ttTCue3RRP2h48HWFRi3qfBx83L3xOLYtiswboMJDiqFHTAIYegMhqalS94lqSSdnFS7eoDJpBKyO9J0qsOdSCqpwLiHro+RF2Dglm8PvNuoXeAuKsAh/es1kSXrnJCc+8ZLG2iZGu666gAMnWt1VRk374GdvtO3TL1UAY7Yo9mUfNzy9PiiVquAQuPW1Yw7z5kycHxoLHm/vLJ3VdUe3w1EUolQOakseY8Z9/ylEnmYhYgFSy/FuEeeD9cPY62/CopGgwF87aqkEi8IJ5rKIo/ZKwPTUCUnndQ5K3OTSyon+SAF8XciPD+AqWlc406VvPOfae3JjFvlowcEqaSCs2QjTk6Sxh1JJRU0brreygyZ2CvUMnA7ucnJRCoB0qOfnjq7jRuOdOMtVssoP+gXEJI9ulRtVuJ1aNoMQYv6i3hScpI6tHXtca6SpMCBjsFNMe5IKsnTuKMdAFnC8joEcs+yzLhNwQ4oSCVFGvceTeouAg2zkH3cQNSTuwTjzmswVQZFQ0A8P8DA9TO67dGlyb3ckzBuVXJyo5/MSgV4Mne8j5uaoLGk62BO5aSuMRhaOjnpCNIh39Em303f8ZT6NpAkGDcq3OjW+y46lp5yTlmGVolxU3VpXQaGADUN3PmsJS2VnBMW/NNnt3HT0cX43y1Tr8S4xe2fiDKvM5CkEiBta0zYEZWW57daVR2LPM9wrKtEsifmdQhUMm7bUNgBxSRUVuOuw4JWtXQlLFhGKbZEN8JJpJKigRfkhpJvktNUT8q7slLHaLDMjUWsKuTHWI5xmzqL+5/Q77KP4ztYQ+fJSUrcekLgtw0tNb5MlcRNjp+3cLhYUSo5IHw+xhhWOlalRlMUuMv2u9kL1C5wWwXDR2m7e9WhBRgai50lm30XZ7dGqcBdhimLyEv2lGPcisCtJRl8J042inZAL1fbHNerJGbcuVJJkJJu8qSZnqNi3MlugJKyKY1bOA6SkEbe/Ke/J0MUshd9x9JL6ZN5g4LLoGiocy/ndWnE2CTOkokZd8ZVkh2uPLY7YBDGbR2KBn2Qj9uIdsFUhEOyiaFpsCVi1Bv5Sisg4UDFfiXrfQcHO+mb+eFutbL3hnGXQJHGvUPmfMvA6qIdSyVPn9sGANwsMe4qSbO8C4FPei9+nZEbZAO3kSRkYnZkJJozkH8Hd1SuEoG9EyPK07iH0ZxIQt6AYtnHzR9rZnqV2Iae6dxIU3aKXAWzxtNnt3HPRz6rlBZoF5KncZdh3PR9yEnEMihk3CP16x7uWNBYOalEvsmLckNZyF5z1w+wPfLSjLuEq0RsbmUpdmIE0ccNJG1e3QLG3XOKk8OHOlZFV4mbacVbtdFUw7hLoKgAhwcaHZrGcGTRjpOTT5/lgfumY0ngtkq4QUTkJXtahl44idoPeFWkqHHzzyFIJRKbj7vw5QReWeO2dD2+4ETmnOsqcdMFQV3bzEglYRii76pdJVQcNJJ2Cobgx+We5WRA8l6wkSde3cIrGwN88+WsLUvVi5vQsYxSxzeVVFJQiJJXxm3oGlYXxw9UOHl6E3f9/F/hGy9vxL+LW+5WTE6KpEjsDEgo6yohUjHOx81dJenAHTNuXYukyHRyslvQ5vZA28qM1yvCZjRPU8Thjl2ptSudpyoj8HYbtQzceZaqnuPHDPHIUivuEPj0mW10bQNXCNOtZ8W4bVMrZNzyEAWCoYksmQpqEh83kB9448o0PRk7Ra+1JUyozpVKXD/VO0U1kmzo8sArDwxYtA0MXT7qi/fw1pLxV4LWTvIRlRLvRYKSWPOLF/uZv5F8tNzOBu4y239gNlKJKoAVve6xpfFFOB/55JPYHnr41oXkc8e7sopSiXhj2ehnAze5SoqkL88P44AdD/rI8XEbmhbLKp50PRgayyQn85p8EQ62q/Ur4Rp+ek2sdC2s9ZzS8l4cuN3dn0hfFrUL3EVDV7mligekNOPewY1Hu6nGOmX81yJkXZmw1DLx8vogN0GZlJenA7clSCWqykkgX+pw/QCMIXbI2NEFF4ZhqllUXuCWXS6qkWTy9BuCeFMZuVJBkvDd0Pmgi34vLIG0E3hpPRu4NwcuNAYlW2vbeinGnadFl4FZQipRuSXGzZ780rNr+MIzawCSymFgMjugXDmZeJzTPm4/CAuvHddPz7rM6y/EKydZpq5BHDYs5pDCMIykknyN+2C7vFTiByFn3Auyxm3B8YJCg4AICtzjzsteonaBm7eezE9Oxox7sYX1vouR53NHyZHF1GNbpl6pO+Aoh3F/4E3X4Pz2CH/wty8qnzcQ+oJkPkcc5NIMWu4JIsOJGA3diBI2F6a03LznD5y0r5xGkomsU543SUim4HjxpPjkMwmBO7rYliN/7F5IJfR5X1Ix7oGLxZapHFXXtoxKjHsaqUT2SfPX5edG5Q8/ttzKlUrCMMQv3f9UPOdSdHtMnpxMbt5yOTiAUj253SCMaxXoddWVk2HEyPljM8lJncE2kuTkwPURhsXn/2DbwvbQKzX8eWvgIgyhlEoAdaM6FTaFXW5dEpQ1DNwFTaZGfsyGji7xk//UmW1c6DkpfRvggXQWGvfbblzB225YwW9+7lmlpS6ZNpOVSrJ2QGLcxcMU5L7gomOBFtHx5VY+4/b8VMBVDVOQJ7wT6PxuDd1YKiGI3w3tNGLGvQdSCe1QVFKJqtyd0LZ0DN0g1VpUhThwT+DjLirA6RXcEA5F3epUDqP7T57Boy9t4GfefROAdNn+pMlJqlcAkgpEKk4Byg2eoN7yBFW7WICXthu6FicnXUkqMTUtdZ2WuXGSQyTPWTJw/HgnkZS7p9fFnVfxYRZfO3Ux931EiPJkXRKUtQzcec16eo4XW4WORIGbtpE3HU1P+7CN2WjcAPBP7rsZF3sOfucLpzJ/y5NKTKHNp6pXCZDPuMknSxCDAkklVx5YyA3cIzdIJSdVHQLlXtyEJaG3yUjq3WLqidZOF1usce9B2XvCuAeZ9bE19JRWQCAJRuO6GFKfDKOC/EAosgMmASl7Q8jr+e75AX75/qdww5EufuDuq2FoLMWCJ+lVYkrl6Rv9bF6gTFMuNxqQQFDZDAHOsE0tkUri5KQvMe6I/JQpgIqLcHLkkp//xEl8568+gAs7I2zEydc0477xSBfHl1v4m6fP576PiIZxlwBdAJ6CgfRGSVXVkUWeiHwwCtyiFRDgjLuK7lpU0PDaqw7gu+88jt/9wvOp/ihAgcatKMCponGLxyFW5dHd/4oDC/lSietjwUqer+oQGF8kMuMWgjzXuCWpRJp6k2jcexC4o/M1cP1MAUUx46ZgVMyWivpkjEORHXBn5EXWt2xAyuv5/pVTF/Hc+R5+6l03wtD54F8xmE7UHVCaVnOh58DUWaqjoqon90UpkecGYWZ95rV11TVNkEqy14NtJsnJop0J4VDcr0TNuF+40Mf57RE+/GffTI1lE8EYwztuXsWDz6yVklw2B258nntN4FZDVehBEEu0iXF//YV1LC+YWF20U48VtbMyGHch/My7b8LIC/Cbn30m9Xu6OahcJXKvEjlw52rcXvrCEIPC1tCFHtkhncj5IWMo2QFF3ZqQx7gTWSWSSkz1lpg+93J0UUySnNzoO5VsWeIOQ5ZLVGPLCHFP7jF2rh0h+V0VZoGrpKhxVR7jJjZ8Y5S7aVtGWiqZpDugZFm8sDPC4Y6dSurHjDs6V2c2h3jz//bX+NxT5+LHuNLINFk7jx8X7RxNTd2fnnzcw5hxj08OE1HImz15sedgwdTx6cfP4nce4DtkOTkJAG+/aRXbIw8Pv7iR+ZuMzYGL45FjrWyXyd1GDQN3wQUgJCcPd2xojLOHm48uZkY1VWbcYy6E16x28d13HsdfPno69fthXnJSkEocyZeta7zvsUozByKNW3JzAPzipp7TJH/IhSVhGCrtgEA6cCfbUjlwC4w7I5Uknyl2lUwhlfzcn3wD9/7K50tvWbeHXmz5fFlylpRj3OOlkt1i3HnBKI9xj7z0umrbujI5WdUOCCTX1oWeEzddIsiM+8kzW3D8AC+vD+LH8IpI0VWizif55OOmIRx+kpzUNRZ130wIVpJ3KdK4i6WStR0H33vXFXjr9Yfx5ecv8OdIjBsA3nrDCgyN4fPCDSkPqcBdEy93bQO3rBWGYYi+UKKtawwr0TSLm46l9W0gYdxlfZdxcDWyrgTClQcXsD30Uq8pDi1IfQ5N0atEuMjalpE7wFbWuG2BzdF4rm7OSDIqjGmZJTXunOTkdjTeSdzeG1qiccc+birAmSBwr+2MsDX08MHf+yp+62+eG/td7Yw83BIN233xgsS4FS1dCWXHlxUF2HFING61xJf3unmMm84vnf+2lbY0TsK4ZcvihZ1RPBGG0JFkpVNrPQDpteP4WalE3auEfNykcfPHUEUlACk5OV7jLpJKgiDEet/B6qKNX/n+u7DUMqAxtZtnqWXiDdceHEsaRp6PoRvg+DKfXlR26PRuo3aBO6+EduRxV4B4N6buajdJ+jbAF0QQ5je2l5FcCPmLpmsb8CQvJ11MslQiWueSwQjillRP+XJFZDRuUSqJJAHa0m9LI9CSC14seU/kD0Lc+Ehi3C1Th6VrSsbNvemSj3thcqmkP/Lx7Tet4rvuPI6PfPJJ/Iu/PFn4+J2hh5WuhSOLdsrLTRdXHuOmm30Zxj1x4B7DuPNYZB7jlndybTNtaUx04nyikTlGSc5Z23Gw0pEYt02uEv7+34oCt+gycT2JWOSMbaNhwaqSd1rTnGDx5/ZLaNwLlg7b0JRe7s2BCz8Icbhr4YoDC/iNv/8G/Og7blBaRAHgHTev4uTprVR7aNVrAhAYdyOVKBFnvqWFoCpiOBLp2urAXW0KTrL1zL8QOgqr1DBufZqVSsReJWJBDUDeYvWxyYxGdCxsDT0sL5iJpVBi3HElp8Ba2tFEEdFC1R95YCwr8QBJT+6RZCtM2wHTjHsSqWRnxAPxb3zg9fiuO4/h45IMpXp81zZxzaF2SuOmhHFe4F4wyyYnp5BKCkve85OeuYxb2snJRUSjSE5TTXPPQ0IAeKe+C71RRiqRGffzUeAW8wteICfPcwpw/DBqMpW+YYhSi21wi6LrB6V99Adzyt6pcRT53t9+0yp+9j03577O229aBQA88PRa7mPIDHA8mhfaJCdzkKdxE1tqCwHpSAHjTphMOSZYphKtq/BDj/LsgIKsQLMsU0mggub+quQPIDDuBSN3JFnM1ASJQ9MYVrt2quCAz5s0lBf+YlRpOXLTxyEWFdH7LLb4nMpJAjf1XmaM4aaji1jvuzGjlxEEvKqu2zJw9aE2XrqYaK5fjfy4r49mjcqoxLgnaDAFJKXfecnJvDmW+Yw7vXPiZftpqaSKTAIkMqATNSobukFGKqGdY8y4L2QDtxsF5Ph1c+yAXhCkGLcvMG4K5rYw17VsP/SDHUsplZDTaEX6THm47fgSVhftQp07ZtxRrBk0yUk18jRuVb+Hd99+FP/t3VfFd1gRZSe0E8oke0ieEBdxkVQSl7x7YaYZUNvO7xEtlxSLbG5rSFJJXuCOXC7S4j+6nC6t7jte5jHJ5zSEysm0ZCP7uFumjlZFzzyh5yQtPCmA5PVa7kdVdYs2D9yvbg7i7+zBZ9dwqGPh1mNLyucuxAm3Mq6SyQI3kPa5iyia6pLHuOU+MQumkdG4q1gBgUQGdP0g7o53WLp2tChx3nc8jDwfr0RJSXFnJ+dgVMnJIAgRhEg3mfITHzc9P/78ro+e48E2tLE+et6vJLtOyGmiigcqMMbw9ptW8YVn1nKLsyhwryzaMDTWMO485GncSTItubDuvfkI/s333aV8HVtwYpSBU4Jx0xZODLhDz48GospSSTo5Kd8QChl3nsbtR66SBTMpm5cCd14J/rGldBc61bxJArWBlSsnRWZFgdrSNSxElYl5uO/fPYDf/cLzmc/oeEG8NSetNa/dJgWObsvANYfaCELg9AYvxPnSsxfwltccztUy6T2K2JLnBxi6Qe70lTKQmziJx15V45b7xHTs9HqRq2vLQLTarkWygoqdkoz30sU+KJ7J+npqJ6ZITtJAYNUkeOoaCKSv07I5hrx+JWQtleWfIrzj5lVsDlx88xX1IGAK3MsLZiZBPE/ULnDnSSVVx0q1JmTchVJJzHKTbdrQDVIT3gmGNEhBTiK1C6ayyBo3Le6+42Hg+lhqGblecJVUAmS70Kmm3xB4T243Sk7KBTiRxu35sAwNmsbQMrRcV4nnB3jyzDaeObuT+r0sfcWMO8efS+e8axu4+iDXG1+82Mdz53s4szXEPTesKJ8HZLf/KtDfJpVKAD7UWeWG6jmTuErSwzkWVFJJVcYt5Epixq0IcpQ4P7XG8wgH2um+OFy7lpKT0ucmBksSkvg7kZjQ+qLAXSbHcLCjHqZAUskhhf0vDySzqvrfAMlIPB64jbFj3fYKtQvcceY7LzlZcitblXFz6xLLZW2AGLiTC0j2TBPk7oAyI+/axYxb1atkbZsvzKXo7s9YAeOWjunIUgvbQy9+T9W8ScJiJJU4so9b2EWMhBtWUQtdYiw70meViy1oe7uWU5CzLTLuw20AvEvgl57jiaV7bjisfB7At/8Lpl5oWaTjm7QAB+DVsjLjHrg+goLGSSRFqVwlYuDuWAYcL4hzAI6fJQPjILqTEnaaz7hPrfGb7R1XLKfWmaPYEcrT48XWrXF3QDFwk8Yt7Dh2Rn4qh5WHg20LG30n09/lYs/BgbZZqWXBavT55YpowmbU1G2pZXAv/X6xAzLGWoyxrzLGHmWMnWSM/cvdPKB4Oycz7hz7Wh5E7awMyjCYRCoRXCWuryxlFhN5jp997bad7yrJ9CqhwB1dbEstE4wxdK3sSLJRAeMGkgHLxYzbiDP0+Rp34jjhgVt9g6QtrcxUZOlrpTtGKhEC/dHFFixdw4sX+3jwmTVcdXAB1xxqK59HGDcEtyoxUEEeMScftwq0drIFOOnWvLEXPXocX6/VbjKiHZAmy8saN5DIeKfW+jjUsXDFgZaUnBzfZCrpKZ/txy0ydnHH0S/YmYg40LYQhEi1OAa4q0T1eYqwvGDC0FguYdgcuOjaRtJ2YB8x7hGA7wjD8C4ArwNwH2Pszbt1QKqhtEByoRf16hWRtwXNg8wiVFAF7pGbnX4DJNNiwjBU6pEdi0+1URYuyN3XdArcxLj5cdDsShF5ycl4vmGkc6vmTRK6LSNTAAKkveliz29epaq+CVH2X5Z0ZOlrqcUvoLxZgLHGbRvQNIarDi7gW2s9fPn5C7jn+pWxtrhxPbnpBjht4M6T+PKlEvXOUCYE9H3SZ3D8cILkZBK413ZG6NpGxg0FJInzU2s7OHG4ja5tpmQmLxoCTJBHogFqqST2cQeJKyXeGbt+eakksqDKzpK1HUe5gyiCFhXy5TPupCK3bY0fpLxXGPvNhxwkUJrRf7s2BiJvkkjVXsl5SZ88yGXmKohVhYQiqYS/bhhp3BLjJr+sQnd1/DCVzDQVjJuOR2aRecnJozHjHkbvW6xxE+TugJ5Q8k6svkgqIb+tfIOhZBcdg6YxHOrkzwLclpjr1YfaeODpNWwPPdxzY76+TZALWGTQecyz7ZWBKoCNG4dm6RoYy+4M5WEYcp9sx/MrjS0DhGZlHte485J4xLi/tdbHdStddG09HsQRhmE0ICEtock7ZJJF+CAFqeTdD+L+JaIdsKyrh8re5QTlxZ5TmXEDwMqiVci4qSK3IzX6midKffOMMZ0x9giAcwA+HYbhVxSP+RBj7CHG2EPnz5frPaFCro975ENj6UBShDg5WbIARxx8mwc90kplqUSWJYBE8nH9AK6CHRHbVQWTjMatS4E7Wkhqxp0jlUgTxbmPO98OSJB7lXhBiCAIMXQTq+BCwUBlSiLJx9lXdCc83LUzXf8IFADJTXPNoXZ8k3rr9fn6NkHu9SFjZlJJLuFQn2veryNrpxt62eSkeJyOFxQWiymPL3YnhYWyQtsysLbj4MzWENettOOEbd/1hQZRQvJc0Yo5bt2qaYKPOyuViFJRr4LGDSBThMNL+KsH7tWujfM5gXtr4GI52uGWHcixFygVBcMw9MMwfB2AqwC8iTF2h+IxHw3D8O4wDO9eXV2d+IAo4MkLmaY/l60US7ZgZZOT5baeHTv95clJJIJYLaZKJBW1Gs3rx61i3LLGTcFM3gV0bQMdS08Yt+Nl5k0SxN4OcuUkwK1e4g2rZebLEBdzNG5VQ6GVrpUvlUiB9epD3Flyy7HFUgUXcltUGTKjnwQqxl2m452qd/xQaqkbWxpJ457ADigWcl0okBU6th67e65b6cbnfEeYPCNLJWGIlBeaZBHRxx13BwxEV4lgBxwz4Z2gkko8P8B638WhTjWpBOCWSEr8y0hLJfvUDhiG4QaAzwO4b1eOBtkOZgSxF3cZVLcD+qWy9HzbKLpKAmXgNgWpROUq6Ug9IUTIj9eizPwFWeO28zVu1c6EinC4th4W+rgJso+bPhNPnvHn2wWdGGk7m5FKFN0JDxdIJb2Rh5aZeIIpGVlkAxQxTp+cZt4kwVYw7p4z/nVV05pGnp+SSmTG7XrVNW5xkMLajhMnhGWIEtqJlXaq2MsVmDRBNUTCi10lSUGNusmUyLirSSVih0AK4nmfqQirizbWdkbKKURy4N43dkDG2Cpj7ED08wKAdwF4crcOKGZ1GcbtZzrZFSEv6fPUmW3l5JiyjLvbMrAjZLNHrq9MTiY9iANleTJdHPLWiyczw0ygJ5nCiOQaIArcCleJWHEn4tgSn2+Y14ubUKRxA/y7GbpJcc6Cmd/7fKPHzxUP9sljVN0JD3ft3P7c21GfEsJtx5dh6Rrec/sx5eNltAsKnoAZSSWK0u/t4WSMWx5i0c4kJ8cn01XHR6+93nfi2YsyxBv6icMdKXBHUokkoQFIWQLjAC9WTgp2QDk5uTlwEYTZbpUqLNoGDI2lPP+0U8v7TEVY6drwosHCMlKB2zYm6oK5GyizSo8D+H3GmA4e6P8oDMNP7NYBiUxVRNXObYl2llxIazsjvPfXHkDXNvBDb7kW//1bT8STdMr2fuhIhTN5Uomo1asSn52c5CR9bvnxlqGh7/hYWjDjoNxtqZOTeaXsx5Za+Mqpi4m1Ms/HnWLcglQiJI7Fz10klYgJpJ2hB7ub3mm0TTFwW+g5Ph92LH2GnaGXOq5rDrfx2L98T2nWOY5x74x8WLpWmcWKKJJKim4IKsY9lAhBNjlZvQCH98AGzu8M4y56KpCEdnTJRsdOir16AuO2FHZVkXH7QnIyU/IeJK4UWl8UhMtc44wxHGibKank4k61cncRNITl/M4oZvMAP8cD108Ct6nHZoNp1sksUMZV8o0wDF8fhuFrwzC8IwzDn9/NA8rzcfdLJi4IeiQviCxvbWeEIASOLy/gf//8c3jbL30u7ttDMhkAACAASURBVEincn6oIMsTA9fP9CkBZKkky6DbOcnJvHaddFMRx0wt2gZ2nPSg2bxkKZBIJcTS8xi3ePFYCmbFJ+8kycmWqcVdEmWIgVu84fUVfSkoWabSuVWOgyoXj+zB/f0vfQu/8InHhdd3p6qapONRBW7GULh21Rq3OjlJuwbHD0on6gmMMVi6FltCczXu6L2uW+nwfwtuKlECIcgj0YBEFuGjyyhwB/H/Y407WkMUuMvKoceXF1LDNNZ61GBqAldJdB7WJEugWO4OJDe0Oujc871tKGBKLSAJPaeaxg3QBZG8Dm1b/+f33YrP/sw7cMuxRfyvf3kSW0NXWSSjQrclJyfVPm5ZKslo3DlTWeT5lAQ6NnFYQLdlIAzTzd2HbpDLuI8u8i0hLfh8xq2WSkSNW0yeLZh63JpTxnrfjRmX2DtcFYhpm6vSuXeG0zWA6lh61KiKB54//vpL+MOvvRT/u6iStCwsIzvCa2fko5vThZGg1rglO6DU4XCS7oAA/w5Pb/DALffiJlCAosCdTFvyktmsUkUtkJY3k0nufNKNobFUP25DqpykgqCyUtUtxxbxxKvb8b+LKkHHQWTcIihwLwkaN1CPSe+1C9yaxr9kZXKy4oXLmWAS1GhUWNc2cN1KB//6/XfiYt/Bb3722fJSiaArh2GYsW0RRKmE3xQkV0mcnMxOsBGfT0gYd3Yi947kK89jYmQJfP48b9VZhnG3zCzjdv0glTyjz6/S/zb6Dq6gXsYpxp3NWdDWXcW4t6douQoACxa/yQ1dXjb+9NkdbI88vBqxz52KyW8VVF3yyqxb2Qfv+QG8IEzJVLSLEgN3VY0b4DcXsoRWZdxictLURNcTf7wyORkdo6GzlMZNO0rGGCxDSxh3yZvnLceXsLYzigtnLvYcaEw9X3Ic8sreM4zbSn8HMj75zVfxa595Rvm3WaN2gRtIN+wnFFX65cE29JQdcDv2AvMv4s6rlvH9b7wKv/fFUzi9OSjHuAWpZORlx4TFn0FylWSSk6Z6EcQady7jToKAuumV+kYCJEU4z0c9KPICla6x+OJNjS4TvOli8iyvhW4Yhljvu7F1T7xJqVxCtGVVMu6ROx3jthOp4fm1XixpPH2Ws7ZpGT0QuUqkJO1OQUtX8XnizpBkJ/GmSf1WRKlkEp3V1JMgmadx0/Vx3QofCSgmJz2Fj1s14DuRSvjfTGF4ttgdEOCff70i4771GG8O9eSZLQC8avJQxyrsNZSHpQUDlq5lGPeWFLjlIRMyPvvkOfynr71Y+f0nQU0Dd7ZZT38CRmRLjHtrmDSMIfzse26GbejY6LulGEzHMjCKStXppqAM3IJUIpewA5yJ2IaW1bhzJvHQRSpOeVHNkhzmaO5AwrifI8ZdEFCI3aqkkqHLdxEtoQAHyHrmt4Ye/CDE1Qe5dW9bCNx9J5uzSBj37KWSBeFG+cSrW/Hvnz3Hb2I0pGEaqKWS8cdtS66cZGxZ+vx0oiIiPwjhB9XtgECyrhhTD9EFgDdeexC/+PfuxDtu5vUYtqHB1Bl2cqQSVbVzEuD5++k6S3UHFDVy29Dj77zsd3wzBe5ILrmwM5ooMQlw1r/StTJe7qqMe6NgYPWsUcvALfd84NNP/NyCkTy0JMa9IzFuADiy2MKPf8cN8fuOQ1fQ+4bSJG4RtLA9Sk4qXrtjGwpXSXmphOxxaZeLWnMHOKNlLBkAW3QjpHMk9yrh78fPY+Iq4b+XpRLy2V4dea5TjFtRbNG2DCyYesYSGIYhD4BTBFZx0vvjr27B1BkOts2EcU8xtoygGqRQRirJMO6c6teFqIhINXy6LGgdHWpbqVF6InSN4QNvuib+vhljvPBs5CXNo1JSSVLYQyDGTQHa0JKbmtzrpGVqcZAsa0A43LVxZNHGExHj5uXu1fVtwupitnoyLzmZx7g3+248ym+3Uc/ALflhKSCU7cVNsE0txWS2hzxRJge2D95zAq+9ahk3K0agyRCn4ORNvwFEB4af67nl46hKatyq5GSOVJKXnDR1LdVQp4hxE5tPdweMkozRDZDYeCunwx3Zta6K+meLWnxv5Ckv0sPdbBHOKCoYmoZxiy6eJ1/dxg1HFnHLsSU8HfUJ7408dKfUuC1dj9kwoQzjbpl6Shunn+U5pm3TiCbTjO8dnwdaV1VLw6lmQOnjVlh4xcpJ/r4scZUEQcpNJO7qqnzHtx5fShh3L7/3Shnw6smSycmcvu4bAycenr3bqGXgNqUtp6o8ugxkxr0deYHlDL9t6PiLH7sHP/Ltrxn7muIUHHmgqwhyUtC2SpUw7Fgqxq3WuM2YcQt2QIVUMiiwAwJJe1cg7aGWQReQPOmEvx9f0PS56UYhV0+SbhkH7pTGrR6ge7hrx9YuAj1vcRrGTQnUSCq59fgibjraxbPndjijH04vlYiViYRSUomhpZpM0Q1QbhdM/VbioR8TMG5ah1XZKeV2kspJBeP20x0ExcfpGpdK4gIzTdS4hdL+Ctf4LccX8ey5nbjb4SQNpgh5jLtj6fG1JxdBydi43Bm32PcZEMqjqyYnFa4SUSYRUbYHiphhH7rZJBKBLioK3Kpy+radZdxV7IDisRB486f880QJynGz/ZZaJixdSyV76ALdiaUSLfV/WSohD/ehjo2OpaeOs59j71zpWBmpZBbl6HSuXrrYx7ntEW47voQbji5iZ+ThlY1BlPyelnHz8yCy57KuEvE5eeuqLUsle824RVeJNEgB4NPjCfLjTF2DK+xGUow7+pyWoZXKMxFuPbYExw/w1JltbA+9iayAhJWujYs9J7Vb2pQ067xqZ4DLeRsDF8uXe+B2pcUP5NvX8iAPsd2egXNgUQiWdOctsgPSsasWZEdRzRcnJ+UCHArcKTtgJNuUTE4CwLFlO3pu8Xno2kZml0CfYSuWStLOkzyp5GDbjDVSQl4LA5VUUqZsfBxoV/D1F9YB8G32TUe4a+LRlzaj15/Ox03ny/HSpKOMxu34QRw0RjnJSar+nIZx03dYdhI6gb4/VWWvKjkp9uMGOPMmmyOAjKsEqC6F3nKcS5s0BWkaqWR10YYfhKmCMbGlK1CcnBy6vF5jr6SS6aLYLsGSkjx0oqpeuLZU2LAtlU1PAnGYQmAlJd8yZKkkT+OW+wCrsvYA4t7Loh3QNnRYhpYaCyaXSss4GpX4j0sCfedtRzMBJ9G4SSqJXCWWOnBv9Lm3dqllotsyYleJPChYxKGOjQu9EcIwjHdB8RSZKb47eq+vv5gEbgodD0e/E3uhTAK5JbHjcffNuDVH68fxePHUKLYDyoGb2wHz8iBlYMVSSUXG3TLw0npfSDoWJyddKUDrUQGOardAn7Pqjuc1K12YOsODz16Y6DOJiKsnd0bxzzLjtg0NGlMnJzcGPOBf9lKJ66sYd7U7coZxj7xcqaQsRE9r3pgwQJRKvNS/RXRsBeMeq3Gnj39RKAhyI0ZTxLiPRpbAcdbKd912FP/8e25THgO9n9jWFcgGbj4DkHtrxaEPpOurvs+VrgXXD2NWL77f4hSBlW4uz5/v4eiSjUMdCwc7Fla6Nh5+aQNAdSlOhiUx7rhPyZh1S7M76fwlGrdaKpkF464qK3SjQbmOl71pqO2A5D5JpBIv6k0PpAM/fc6qxMwyNFy/2sXXTl2c6DOJiKsnhQTllhS4GWPKXTKQ9J2fpABoEtQ3cHszSE5mGLebSu5NArHhTl7vayBZmKTPq/RIVcc6sauaCJXGDaSHKeT5f0VQcrJKp0WCKWncca+SOPCkk5NiskbsZFjU6jT2cgs7kVkwbvEmcevxpfjnG4908c1XSCqZ3scNJN9h2alNlJOgtZqX9G5bBgaOHzPuqZKTVTXuFv/+vKCgAEcllUR/o8rJeMBCylXCP2dVYgbw75Kuw0l93EASuMUdsMy4gShBrHCVkMRyeWvchpycnCxw22ZW456VVLIzLE5OmhLjVmrctpGxFo1NTkqMWwyIRcdDOFaScauQtQPKrpJscpKKPDpCxak8KFhE3K9EcJbMasgB3TzFwH3T0W7MIqf3caeTk+MGBRPo+0oYd35ysuckrHey5CT/Dqs2Y+IDRPx4l5kapKDQ9pNeJeTj5mPvxJFmhFjjnuD833IssfCuTOHjpvMhMm5l4LYM5aT3zZhxX8Z2QEtnklQymY+7FY2ECsMwLuKYViqxDB4Adhwvt1ACSC6qXpGrxNIxcP1UJjvvojzUsbC8YGYu5o6daMdlGDe5SiZhN7EdUC7AMdS9Stb7bhy4u8LkoKLvU8m4Z5CcBJJdRopxC979WTFuIh1lCUec3PXSUom8rtpRvxUqxZ5KKqkY5CgpvzmIiIiiotZRSCW0c+TDs4PUSDMCraNJzj99l4bGUvmfquCDk7V4dN56z0Hf8VNtXoFsl0nCRvSdNBp3Kjk5mavENnWEIV9QVCo8rVcX4FpoWamEvmSlxi2NowIEpiIF7g/ecwKf+Im3ZWyLiynGPT5wL7X4Ap2E3dAFSslJYkqaxtuFqnzcB1VSSYFLKEkSCX28Ry50ReFUVZCX+7bjSbC+aYaBmxLIrsy4xyYnI6Yenb+i5CSQBImJugNOKJXQeiFJwFS0dRXlzdg9oqWlkljjnhXjjr7LQx2rtKVXBV72nhSnferkGQDA229Kj2HMG4G3ETuoLmNXidxkatIm97agvZJ1b1qpBEj0vqIxYdS0nhi3UuOmxkdCkUZeP+62ZaB9KHvs3ZaB3nkeICjYFSUnGWP4oTdfi7uuPlD8IRXIJCeF92mZmloq6SRSCTHtokEOtPDF6SbUp2SaCxPg0oxtaDhxuBP/7sbIEkjHOA1MiXGXlUpkO2VecpIIAm3LJ2HcLVOHbWiVb1J086E2BuL6VHX09IIgugZEO2AYu1JSw4bNyeyAAO/sd7hjTZWYjF8rGmEGAJ/4xmlct9LB7VcspR7TtozUyDTCxsCBZWhTk4uyqG3gFvUyPti2+peaJH38uCx8WqkE4Ex5Z+QXjgljjMHUtUTjLmDcPUdk3Go7YB5EJvvRB57DUsvAt113qPA5/+y7byv8ex50jUFjicYtLlK5NenA8THygnjruNgyogEMfnxOVIHSMjQsL5gpqWS74vSjPHRsAzcfW0wlxshZsrYzmvqmLtviykol8pi9oRtkip+AZL2Q9WwSO+APvflavOm6Q5VvguRxJ2++3OdELpoT50oCPBnp+kGmohJIblyT3DgZY7j3liMTnQsZK10bL13s4/z2CF9+7gJ+/N4bMuepbel4ZUOtcR8QplPtNmoZuC0jq3FPkkwjt8PIDWJ72UwYd2RtKxoTBvBGPJSBzvNxA+nmS47C51p4LJE/+ivPX8DnnjqPD7/3ll3tUGbqWiztpPpFS4E7rpqk5KTQ50E1KFjE4a6VKnuXx5ZNig/fd0umLzrAE5TrfafyRBkZWVdJVH8wZu2qGLfcpwQQpJJ+WqqqghMrHZxY6Yx/oATyuK/3HVh6lqzIHT3lqU9m1B1QlXynG9ekO55f+f67JnqejNVFGw+/uI5PPvYqghB4311XZB5Dzh4Ze1nuDtQ0cKt83JN4bFsC4yaWOK0dEOALbL3vFI4JAzhrpoScKhDTQhU1M9IJyzKIrmXA8QL86//vCRxbauGH33qi7MeYCFY0LEBj6e3ygpmeNkRSxwFKTkY7nZ2hpxwULOKwVPbec2bDuN9y/WHl799wzUG8vD6Ymi3JrpKEcY/xcUuMm0+/yT4no3Hv4dzDTsy4HWWi3TJ0yQ4YpFi5rvFh1+rKyYhxTyCVzBIrXRsXeg7+/OFXcPPRxVT+g6BqDAfsbYMpoNbJybSPu2piEkhr3JRQm4VUImrcRZoWl0oijVvB9FSjkFyfB8W8lpuqYwGAb7y8iZ96142FiclZgCSclqmnAl3L1FJJ1g2h3B1Id1Uk7TevydXhjp0qe59FA6gi/MQ7b8Bf/vg9U7+OXPLeG3lomcU9Yfjz0oybJDgZdA3EGvcM5IGyoOKnjZ6r/DyWztKjy4J061Yz0sBVjHua5OQssbpoIwyBh1/cwPfcdVz5GGr0JWOjv3d9SoCaBm5L8nFfFNwJVaBi3LNgbl2Le5IHBdNmgEgqKSh5jxn3KK1xV9Hr6PNcv9rB973xqtLPmxTJdG6pJD9PKumQHTBi3CMPfccvbHJ1uGtlfNyz+N7yYBt6vDOYBknJOycdZY/bljXunHF4CzHjjjTuOTDu7ZGnXJ9yD31PGpZA3QFdX+HjNiernJw1VgWnzftem5VJAN5a1/ESWyNhc+DuWdUkUNPATVIJDXI9uzWMC0eqQM24ZyOV9EZe4ZgwgF9Y5NFWbWtVjNtRjDkrwkpU8fVz77l5LLObBehilD/3gqmnJr1T5v1AXICT6Pm9MQHtyoMLuNhzcC6ajTgrjXu3kZS8R+6ZksMZ5JYBeTu5WK7o7T3jFnc8Kqkkk5wMwtSu0Yh20Sof9zTJyVmCqifvvHI5Nw8Qj8DL9OXZW427loHb0hnCkH/5I8/H2o6DY0sLlV9HvCC2hx4Ym6xiUEbX1tFzfAyc4oZOqtaXIuIZdqO0VFKFSX37jav4+I+/Dffdod7azRoUnGTG3TI1DIUt5MVeuiAh7h0eMe4il9B33HIEAHD/42cBlOtpXQeoCnDKrDc6l4mrRJ07aZtUBMPPrSqA7hZsQ4/fT7WWW6aeStrJU25MncELAqWPm9bGXgY+FY4v8xjzvYqkJCGepJSaOuVj4Poz2bWVRS0Dt9hl7dwWT1JRO9IqELeg1NJ1kmGiMoh9XOw5hZ5p0fKkdJXEU1nSyckqF6SuMdx51XLpx08LOjaZcbdMPdX7fL3vYNE24s8tdlUcF9BuPrqI61Y6uP+xM/CDEH3Hn7pz316Azg0Fp7Lj1rhLI2nnmpecJKlkZ+TByrGh7ibo5qlan12pfYM85UbXGPwcH/dbr1/B7/yDu3HnlXu3jlW44sAC/vQfvQU/fM+J3MckrV0TsiUPFd4L1DtweyHORNvlY8sTMG4jzbjlPh+TgoLQ2s6oUCpJ9SxWJnQ0GBpLLYKqGvdeg45NHtawIGncG0LxDZBuzsWTzcVFQvfdcQxffv4CXl7vA5i+c99eQByksDV0cXpjWGqnwBjjcycFxq1KTlrR0F7xvfYSnThwq/M14qAM2cfNBymofdy6xvCdtx3d8xuRCm+89lDh9afqyb3X5e5AXQO3sOU8sxkF7qXqGneSnAyi6Tez2W7Txbg19Io1bkXrSxGMMW4vEphKVY17rxEH7oxUkt4q8z4lwtCHiGFvD71SwwXuu/0Y/CDEf374NIDZ5CZ2G4zx0v+vnbqI9/zqA3h5vY/33nGs1HNbph4z7qLcCe3w9tIKSOgWBO7FlhS4gzAlh8RNpqboJV4HxFNwhM+6sccNpoCaBm4r3nIKgXuq5KQ/k86ABJFFFQVuYhVF9j7ek3v/MG66qcifm4+JS5JT630npflpGkPH4j1e8saWiXjtVcu48sAC/uTvXgIw/ZCDvYKpM3z5+Qvo2gb+7EfvwffffXWp54mT3vn4OfUaoBvePG7utO7llsMA3xHtSLkaMQFpRIMUVD7u/YS2IjmZJOKbAhwAUeDeGmLB1CcqnEkx7pGL1Rn0MwDS2e+i5CSxonFbr3TJexgPna0j6NhaEuNbMHU4XoAgCKFpDOt9B9evdlOPoe10b1ScnAQ4e33P7cfwsS+eAjBdL+69xN//L65B2zLwo/denxn2WwQ+d7JY4wYSnXsea4S+A9V67tpmKnD7QbbkHUh0fDGo7yfEUskoK5U0GrcYuDeHOL7cmkj/yjLu2ZxYkXGXSU4WsaOObWRdJTVm3Hkad+zgiYLPRi9rj6KBs2UYNwDcJ8gM+8FVAvA+MP/4O2+qFLSBNOPOK8ABksAxX41blZzkN24qPvJ8SSqJfh646iZq+wWxE0zYJce9uOukcTPGrmaMfY4x9gRj7CRj7Cd3+6DEqdFntoZxD+mqENuNztIL3CkpldDnKNIjZcZ9fnsU9/eoI+gzyYxbnIIzdH1sj7xMi0ua1lOGcQPAG689GLd53Q8a9zQQGXdeAQ6QaKxWxRvDLLBYoHGLyWcAcIM0ASESQ9W1dSYnRVhQJicd6NF4vr1CmbPnAfiZMAxvBfBmAD/GGJusvVxJUHm4yLgnhW1oceXkbjDuQh93CamEz7Dji90PQjy/1sMNR7q5j5838uyA4hScjz/KE4pvvPZg6jEdy8B634XjqwcFy9A1hvfcfpQ/d58w7klBjJuqC/N64MSMu2bJybgXzShZy6kCHC3Z/QL7V+NOGHe6vcNedgYESmjcYRi+CuDV6OdtxtgTAK4E8PhuHZTYrOfs1jAecDsJbFPH1sArNW27LMpKJWa0cIv0yLZtoL/GF8HL6304XpDRhuuEIlcJwBnVx774Ldx8dBFvlZo6dVsGXrjQA1A+EH/o21+DxZaJ4xPuuvYLWiZP3I7ieZM5yUli3HMIfOOkEiBp+ev6YUrHpueQ88jcpxp3y+See1Eq2RjsbZ8SoKLGzRg7AeD1AL6i+NuHGGMPMcYeOn/+/FQHRcHh1c0BvCCcinG3TA3no05zs+gMSK9JZELWekXQ5yhm3Em3sefO7wAArj9Sve3mXsHMc5VEDPHzT53HE69u4R++7USGgXRtI/4uynaCu/ZwBx9+7y0zKZyqM2xDj2Qm9fQbwsIcGffimOQkkLRv8PwgFeD1KFAPXB8aw779PhljaJvpRlPUi3svUfrbZ4x1AfwpgJ8Kw3BL/nsYhh8Nw/DuMAzvXl1dzb5ABdDCeHl9AAATa9wAZ4Zr0TiiWTkTGGMx+xjXqwQoTiS1LSPOUD93jrPR/cC4ZUZI//6dB57H4Y6F/+p1V2ae27WNuKpQNSj4coZtckkvb/oNoQ7JSZUjhAqkaKgH93ELGncUxIeuvyc9dXYTbcnCuzFw9rTcHSgZuBljJnjQ/oMwDP9sdw8pWbQvXuBVc5MU3xBaph6zvMUZeoEpUVNGKiliRx2bM+4wDPHsuR2sdK09XwRVYMXdAbOVkwBwZmuI/+7N1ypvaKI8Mu/ey3VDK2bcxXNDk+Tk/DRuVYtiYuOkcXtBIFVOJoHb3KdsmyDPndyoI+NmfL/77wE8EYbhv939Q0pY3UtRufO0yUlq6j9LZ0LCuMc3mSr2cRsIQq7nP3d+B6+pMdsGihh3wgR/8M3XKJ8rnv9J+qtfykgYN0klxYx7Hq6MwuSknU5OyiXvepycDPY9415sGamZqJt73IsbKMe47wHwQwC+gzH2SPTfd+3mQdHd+cWLfegam2oQaMvU49aqs3KVAKgklRR5VsV2p8+d36m1TAIknynPx/09d12BI4vqG63IsveLL3uv0DJ0jNwgTk7m5U7m6ipplZdKXD8tlZiCHXC/ergJb7zmIL72rYsYuj5cP8D2yNvTcnegnKvkQQB7eqbpjn56Y4BjS63S02BUELXCWTJueq1yUkn+Y4h5vrw+wHrfrbUVEMh3lVy30sEH3nQ1/tHbb8h9ble4cU4y/PlSBm8ZkDDufI3bKPz7boIcLSqXFP0tsQMGysrJgePv26pJwjtuOYLf//IL+Mqpi/EU+L1uSVtL2kNsIggxlRUQSDPiWXUHBJKFWkYqKbJuEQv9xssbAPgkmzrDyvFxW4aGX/x7ry18blcI1rPoi34poWXocP0QA5cHvnyNe37JydhVogi81IsmJZVITaYAXly0Xz3chLe85jBapobPPXkOVx7gXUv3OnDX8tYnamjT6NtA+gKYZb8LkkqKSpvLFOCQu+IbL28CqLejBEiY0ySMT2wU1TDuNKipFA1JGFeAMw+Nu6itK8Cvr9zKScHHXeful2XQMnW89foVfO6pc3GDqb3sUwLUNnAnd+RprIBAEmDalj6V5CKD2ONCgTuCWMY4HzcAfPOVTbRMLb6D1xV5Pu4yEHtq5w0KvlxBLQOo70V+cnJ+rpLFloGDbTOXTHVtA9s5lZO6oHHvd8YNAPfevIoXLvTxyEt8p7zXTrBa7ld3g3HPutcFsfcygxSKe5Xw13n67DZuObZU+8KEPKmkDBZbiby0350FswYlIzcHxVLJPAtwTF3DA//k3lxHUNc2sDPk1lbXD1O2P7qmh+7+17gB4B03HwFwEn/+8CsAsOd2wNoH7lkx7lk6SgA+UPT2K5bGdAcsUTkZsdAgBK6veWISyE9OlgFttRt9Owti2DTBPe/80nqZV5OmouuIpBI/7rmdbTI1dIN97yoBgKsPtXHjkS5Onua1iE1yEnxbpWsMfhDGAzwnhb1LjPu+O46PHdCbjJkq6FUiBLG6JyaBhPFNYuej5zT6dhakaccad15y0pyfq2QcOpaBCzv9eFiCqskUgEtmt3XvLUfwzLkdMDZ7YjgOtT2DFPSmqZoEEiaz1ycWKDdIQdR9656YBID33H4Mv/WDb8DVh9qVn0tMu2HcWcTJyX5x4D7QMWEZWtzutk6gtr0UuE1FP24gPW9yP+Pem48A4G61WebPyqC2V5AZ9dE+sjTdAiXXxzz6ORPLKNIjW4YOxoAwRO093AAPKON2GnnQND5js2hQ8OUKkXGbOssNBEstE3/902+fOvezG1iMBmV40VxJVXdA/nNt+WIl3H3iILq2secyCVDjwG3pGg51rImSYCJixj2HSj1arEULVdOibmOuj+tW6i+VTIuubVzyvbUngWgHHDc9Z5Ldzl6gEyUnqZGYoegOKP9+P8PUNbz/9VfEw4L3ErW9gswocE+LeTJus4SrBOBe7kPd6W9S+wGHOvVuojUv0DrdGLiFRV11RrdlwAvCuHOePCw4+Xl/fj4VfuH9d87lfesbuA020WR3GfPUuM3YVVLMMBZtA9cerieLmjV+9Qde1/QpUaAlMO5ZDbXea9D3uh4xUCNHHlF1F2xQDbW9gn7kv3zNTIpR5sq4Y1dJMcP4V++/Yya7i/2AW48vzfsQaglaoNXoDwAACCBJREFUp44X7F/GHQVuqiY0FAU4/Pf78/PVCbUN3P/gLSdm8jpzZdzkKhkjldxzw8peHE6DGsMWgvV+lcwocJOlMdUdMMdh0mAyXPK3vt2qnCwDs0QBToMGQDpY19GjXQYJ4+aB21R0B+S/35+fr0645M/grceX8L13XYG7pYnjewFqf7lfL8QGewdxjexbxt0ijZtLJWl5pGHcs0RtpZJZoWsb+PUPvH4u733toQ7uvXkVr7v6wFzev8H+gaVrsZ9/vwbujsy4FSXv8u8bTIZLPnDPEwuWjt/74JvmfRgN9gEYY2gZOgauv2+Tk4sZjTsvOdkw7mmxP1dIgwaXIChBOa4Ap66QpRLRPcIYiwP2pdKrZJ5ozmCDBjUBlb3vV8a9YOrQWCKVyFq2EVcSN4x7WuzPFdKgwSWI/c64GWPo2EYilUiSCLlJGh/39GjOYIMGNUHCuPdn4Aa4GYCkEjkJqRPjbionp0YTuBs0qAmIce9XqQTggZsYt9zhMB4s0jDuqdGcwQYNagJi3PtVKgG4JTDkzQEzWjb9u/FxT48mcDdoUBNcCoxbrFCWtWy9cZXMDM0ZbNCgJrAvEY2bIEslpHmbjY97ajSBu0GDmuBSYNzikIxMcrJh3DNDcwYbNKgJLgWNW2TcGR+31vi4Z4UmcDdoUBO0LgHGnQrcOVJJ4+OeHs0ZbNCgJog17v3MuMXkZK5U0jDuaTE2cDPGPsYYO8cYe2wvDqhBg8sVxLTtSyQ5mWXcjVQyK5Rh3P8BwH27fBwNGlz2sGONe/9uhIsCt9EMFpkZxp7BMAwfAHBxD46lQYPLGonGfWkw7kzlJBXgNBr31JjZGWSMfYgx9hBj7KHz58/P6mUbNLhsQEx7PycnyQ5o6gyMNa6S3cLMVkgYhh8Nw/DuMAzvXl1dndXLNmhw2eB11xzEm04cwuqiPe9DmRhUOSmzbSBJVjY+7unRTMBp0KAmeN3VB/BH/8Nb5n0YU4GkElUjqXiQQlM5OTWaW1+DBg1mBpJKVJa//7+9+wuRqgzjOP79ueuu6VpqkplKKiyWSKVI2B8irAu10i66SIK8ELoJsgjC6MrLIPoHIoRWFmGRSYkXQZjQTVlqYZaW9tctS6O06EbNp4vzDmw2u+02c/bMOfP7wDBz3p3ded7zzD6cec67e2pH2j452bihLAfcArwPzJHUJ2l1/mGZWRnVWiX12iGjvY67af6zVRIRK0ciEDMrv+7OUXSMUt12SK3v7f/H3TjvQTNrGkn0dHcO2irxEXfjXLjNrKl6ujvrHlX7QgrN41UlZtZUPd2dnKtdBqefWqukyycnG+Y9aGZN1TOms/7JSa/jbhrvQTNrqsk9Xf+4hFmN13E3j1slZtZU65bP48xf5/417nXczePCbWZNdelFY+qO33HVVHq6O+r+ObwNjwu3mY2I3inj6Z0yvugwKsGfWczMSsaF28ysZFy4zcxKxoXbzKxkXLjNzErGhdvMrGRcuM3MSsaF28ysZBR1/otXwz9UOgF89z+/fTLwSxPDKQvPu7143u1lKPO+PCKGdKX1XAp3IyTtiYiFRccx0jzv9uJ5t5dmz9utEjOzknHhNjMrmVYs3M8VHUBBPO/24nm3l6bOu+V63GZmNrhWPOI2M7NBtEzhlrRE0heSjkhaW3Q8eZE0Q9IuSQclfSZpTRqfJOkdSYfT/cSiY82DpA5JH0vakbZnSdqd5v2apK6iY8yDpAmStko6lHJ/XTvkXNJD6X1+QNIWSWOqmHNJz0s6LulAv7G6+VXm2VTr9ktaMNzXa4nCLakDWA8sBeYCKyXNLTaq3JwFHo6IK4FFwP1prmuBnRHRC+xM21W0BjjYb/tx4Kk079+A1YVElb9ngLcj4grgarJ9UOmcS5oGPAAsjIh5QAdwN9XM+YvAkvPGBsrvUqA33e4DNgz3xVqicAPXAkci4uuIOA28CqwoOKZcRMSxiNiXHv9B9gs8jWy+m9PTNgN3FhNhfiRNB24DNqZtAYuBrekpVZ33hcBNwCaAiDgdESdpg5yTXWXrAkmdwFjgGBXMeUS8B/x63vBA+V0BvBSZD4AJkqYO5/VapXBPA4722+5LY5UmaSYwH9gNTImIY5AVd+CS4iLLzdPAI0DtSrIXAycj4mzarmreZwMngBdSm2ijpHFUPOcR8QPwBPA9WcE+BeylPXIOA+e34XrXKoW73tVDK73cRVIP8AbwYET8XnQ8eZN0O3A8Ivb2H67z1CrmvRNYAGyIiPnAn1SsLVJP6umuAGYBlwHjyNoE56tizgfT8Pu+VQp3HzCj3/Z04MeCYsmdpNFkRfuViNiWhn+ufVxK98eLii8nNwDLJX1L1gpbTHYEPiF9jIbq5r0P6IuI3Wl7K1khr3rObwW+iYgTEXEG2AZcT3vkHAbOb8P1rlUK90dAbzrb3EV2AmN7wTHlIvV1NwEHI+LJfl/aDqxKj1cBb410bHmKiEcjYnpEzCTL77sRcQ+wC7grPa1y8waIiJ+Ao5LmpKFbgM+peM7JWiSLJI1N7/vavCuf82Sg/G4H7k2rSxYBp2otlSGLiJa4AcuAL4GvgMeKjifHed5I9rFoP/BJui0j6/fuBA6n+0lFx5rjPrgZ2JEezwY+BI4ArwPdRceX05yvAfakvL8JTGyHnAPrgEPAAeBloLuKOQe2kPXxz5AdUa8eKL9krZL1qdZ9SrbqZliv57+cNDMrmVZplZiZ2RC5cJuZlYwLt5lZybhwm5mVjAu3mVnJuHCbmZWMC7eZWcm4cJuZlczfRnDZ1ZNFW6MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.6005,  0.4651,  2.1631, -0.5658, -1.6601, -0.0394, -2.0175,\n",
       "           0.4058,  0.9407,  0.2962,  1.2229,  1.5611,  0.4376,  1.1903,\n",
       "           1.8944,  0.7561,  0.6074,  2.2054,  0.2962, -1.6331,  1.2484,\n",
       "           0.7519, -0.1184,  0.9450, -0.5241, -2.5909, -0.1450,  3.3329,\n",
       "          -1.8571, -2.5995]]),\n",
       " tensor([[ 1.4493, -0.9631,  0.1222,  3.0944, -1.0900,  0.1468, -0.8380,\n",
       "          -2.3489,  2.9746, -1.5429, -2.5887, -2.1620,  0.7208,  2.9958,\n",
       "          -1.4551,  1.3203,  2.1740, -1.1238,  2.2019, -2.0090,  2.8515,\n",
       "          -2.3903,  1.0435,  1.9974, -1.3971, -2.8270,  1.1575,  1.1323,\n",
       "          -0.6941, -0.6987]]),\n",
       " tensor([[ 2.9367,  0.5309,  0.1568,  2.5909,  0.3661,  0.4127,  0.3895,\n",
       "          -0.1740, -1.4023,  0.2388, -0.7181, -0.4153,  1.7530,  1.0625,\n",
       "          -1.9211,  0.6427, -1.7324,  0.1593, -2.4833,  0.1843, -0.8949,\n",
       "           2.2678, -2.2927, -0.4199,  1.4474,  2.2511,  0.7532,  0.5215,\n",
       "          -0.2236, -1.6148]]),\n",
       " tensor([[-1.2973, -0.5273, -1.3147, -1.9111, -0.5761, -1.1771,  1.8994,\n",
       "          -1.1292, -2.1082,  1.4912, -0.4935,  2.7488,  0.8530, -0.0445,\n",
       "          -1.8006, -2.0963, -0.3149, -1.5980,  1.1783, -2.1199,  0.0052,\n",
       "           1.4588,  2.7908,  1.1736, -2.4381,  0.5851,  2.2982, -0.2783,\n",
       "           2.3365,  0.9967]]),\n",
       " tensor([[ 1.8977,  2.0972,  3.0018, -0.1003, -0.9147, -0.8423, -1.9167,\n",
       "          -1.9359, -0.3465, -1.6182,  2.4259, -2.3541,  1.0703,  1.4702,\n",
       "          -1.5292,  1.1793,  0.5153,  0.1542,  1.2714, -0.8643, -1.1877,\n",
       "           2.2698, -0.5614,  0.1921, -0.1986,  0.5388, -1.4069, -1.5851,\n",
       "           1.8689,  0.6732]]),\n",
       " tensor([[ 0.8415,  0.6861,  1.5012,  1.3929,  0.0818,  1.2228,  1.8461,\n",
       "          -1.6451, -3.1724,  0.9435,  0.8535, -0.3689,  1.6424, -0.9406,\n",
       "          -1.7124,  3.3763,  1.3564,  2.5637,  2.6646,  0.5219, -1.4244,\n",
       "          -0.4727,  2.5895,  2.2078,  2.0395, -0.7844,  0.2987,  2.3182,\n",
       "           2.9683,  2.3671]]),\n",
       " tensor([[ 0.6740,  0.7645, -1.8564,  0.9812, -1.3591, -1.8763,  1.2961,\n",
       "          -1.5124,  0.6806,  1.9582,  0.9902,  1.8656, -2.3683, -1.2023,\n",
       "           0.7689, -1.8518,  0.5070, -1.4838, -1.1598, -2.2236, -0.4990,\n",
       "          -2.6003, -1.6576, -1.8634,  1.8346, -1.0343,  1.8149, -2.7645,\n",
       "           0.8657,  1.7779]]),\n",
       " tensor([[-1.1162, -0.3783, -1.4093,  2.4035,  0.9956, -2.1060, -1.4686,\n",
       "          -1.7460, -2.7342, -1.6782,  1.2318,  2.5590,  2.4685,  1.2138,\n",
       "           0.9684,  0.6783,  2.5975,  2.3008, -1.6501,  0.2369,  2.3431,\n",
       "           2.6179,  0.7800, -1.8261, -1.7169, -0.7270,  1.7034,  0.9804,\n",
       "          -1.3235,  0.8785]]),\n",
       " tensor([[ 0.1990,  1.1785,  2.0770, -0.6572,  1.4083,  1.7443, -1.3374,\n",
       "          -0.8499, -1.8943,  1.6553, -0.0481,  3.1545, -0.3956,  1.7221,\n",
       "           2.3499,  0.0840, -1.5117, -1.2343,  3.2208, -0.3215,  1.2018,\n",
       "           1.7869,  1.7016,  0.3267, -0.6628, -0.0914,  1.5311, -0.7042,\n",
       "          -0.6586,  0.9311]]),\n",
       " tensor([[ 0.7883, -0.7398, -2.9073,  0.7349,  1.0722, -0.2319,  2.6071,\n",
       "           1.4553,  0.7219, -1.3400,  1.8334,  0.3529,  0.3355, -2.3729,\n",
       "          -0.5581,  0.0321,  1.9294,  3.1775, -2.2251, -0.5320,  1.9001,\n",
       "          -2.1938,  0.8713,  2.2785, -1.4086,  2.4612, -0.9680,  1.4678,\n",
       "           2.8026,  2.0201]]),\n",
       " tensor([[-1.5048, -0.8381,  1.8687,  1.5856,  1.4737, -0.3402,  1.2621,\n",
       "          -1.6818,  2.9369, -1.0836, -2.0799, -0.9510,  1.4773, -1.5577,\n",
       "          -0.3253,  1.5734, -0.4876,  0.7255, -1.2842, -1.5453,  1.6075,\n",
       "           2.3382, -1.2041,  1.6761, -1.8153, -1.3776,  0.2109,  0.0264,\n",
       "          -0.6905,  0.2712]]),\n",
       " tensor([[-0.7991, -1.8113,  0.9326, -1.1646,  0.0208,  0.8409,  0.0589,\n",
       "          -0.5838,  2.1432, -2.7803,  0.6742, -1.0736,  0.9753,  1.9342,\n",
       "           1.0414,  3.2073, -2.6515,  1.6185,  1.3807,  1.3900,  2.7848,\n",
       "          -1.2161,  0.5364,  1.5493,  0.4224,  1.0533, -0.1038, -0.2022,\n",
       "           1.3969,  1.0789]]),\n",
       " tensor([[-1.2453,  0.3008, -2.3823,  1.2228, -0.1772,  0.9840, -1.6783,\n",
       "          -1.5469,  3.2611,  1.4203,  0.6261,  0.1817,  2.0636,  1.0085,\n",
       "          -1.1936, -1.3357, -1.3038,  0.6626, -1.8933, -2.2046, -2.7934,\n",
       "          -0.7188, -2.7150,  1.6183,  0.1946,  1.8958,  2.2418,  0.2019,\n",
       "           1.9973, -0.4309]]),\n",
       " tensor([[ 1.1230, -1.2938, -0.9754,  0.5378, -0.5210, -1.4156, -0.8553,\n",
       "           1.2716,  1.6768, -1.7550, -2.0945,  0.1009,  1.6645,  0.0849,\n",
       "           2.9634,  2.0254, -2.3784, -1.9347,  0.4339, -2.4436, -1.7844,\n",
       "          -2.4401, -2.7979, -0.1286, -2.7707,  2.1196,  1.3605, -2.4580,\n",
       "          -1.0622, -2.7417]]),\n",
       " tensor([[-1.5503, -1.6678,  0.2232,  3.3901, -1.7940,  0.4130, -0.1529,\n",
       "          -1.0530,  2.2114, -1.1949,  0.7417, -0.9215, -2.6350, -0.5555,\n",
       "           1.8161,  0.0068, -1.0407,  0.9527,  0.4598, -0.1286,  1.9013,\n",
       "          -2.0349, -0.8189,  0.7816,  0.9169, -0.6703,  1.3015,  2.7650,\n",
       "          -0.6388, -0.8478]]),\n",
       " tensor([[ 1.4431,  0.2935,  1.4560,  2.8040, -1.3655, -0.4162, -0.7003,\n",
       "           0.7890, -0.9755,  0.1145, -2.1336,  1.4760,  1.6602, -1.4287,\n",
       "          -0.3054, -2.1383, -0.4799, -2.0609, -0.3588, -2.4698, -0.0264,\n",
       "          -1.5224, -1.8822,  1.0633,  2.2539, -1.1097, -0.9625, -1.5821,\n",
       "           2.1039,  1.0749]]),\n",
       " tensor([[ 1.4986,  0.0568, -1.0687, -2.1697,  0.8493, -0.2516,  2.5298,\n",
       "           0.9994,  1.0245, -1.6090, -1.2174, -1.1039,  2.7389, -0.8758,\n",
       "          -2.5909,  1.8392, -0.0466, -1.4853,  2.6454, -1.1185,  1.4245,\n",
       "           1.9749,  2.1186, -0.2937,  1.2094,  1.9306, -1.0791,  1.9439,\n",
       "          -1.0989,  2.6531]]),\n",
       " tensor([[ 2.7306,  1.8211, -2.1118,  1.5666, -2.1504, -1.2911,  0.6918,\n",
       "           2.9724, -2.3955,  1.4376, -0.0031,  0.8454,  0.9427, -1.7305,\n",
       "          -2.0851,  1.5173, -1.3266,  1.6283, -1.7427,  1.1952, -0.2944,\n",
       "          -1.3462,  0.7053, -2.3773,  0.6116, -0.3685,  0.5413, -2.8894,\n",
       "          -1.2777, -2.0562]]),\n",
       " tensor([[-0.1208,  0.0354, -0.7146,  2.1623, -1.1257,  1.3036,  1.8469,\n",
       "          -1.0270, -1.2607,  2.3409,  1.4492, -1.6787, -1.8532,  1.2532,\n",
       "          -1.3597,  0.4421, -1.5517,  0.0473,  1.5894,  2.7325, -0.2848,\n",
       "          -2.0041, -2.1951, -0.5370, -0.4820,  2.3829,  1.1289,  1.8739,\n",
       "           2.0822, -1.1562]]),\n",
       " tensor([[-0.1964,  0.6767, -1.5147,  3.0892, -3.2129, -0.7315, -1.3585,\n",
       "           2.9611, -1.9124,  0.2828, -2.1155,  0.2753,  0.0580, -0.2280,\n",
       "           0.6648, -0.1485,  1.5284,  1.1633, -1.5267,  1.2489,  2.0855,\n",
       "           0.1504, -0.9536, -1.1865,  1.7301,  0.4083, -0.6709,  0.8927,\n",
       "           0.4205,  0.3123]]),\n",
       " tensor([[-0.0446,  1.1960,  0.3929,  1.9720,  0.1041,  3.4972,  1.8501,\n",
       "           2.3168,  0.5688, -0.9424, -0.4173,  0.2984, -2.4549,  2.7493,\n",
       "           0.7443, -0.6605, -1.2870, -1.0637,  2.3316,  1.1159, -1.0814,\n",
       "          -0.5462,  0.4599, -1.4286,  1.9110,  0.4645,  2.9690,  1.2313,\n",
       "          -2.0102, -1.0378]]),\n",
       " tensor([[-0.8589,  1.9405, -1.3897,  0.1731,  1.9961, -2.2952, -0.6929,\n",
       "          -1.4158, -0.3507, -1.5448,  2.4589,  0.1937, -1.7181,  1.5170,\n",
       "          -1.7625,  2.0403, -0.2105,  3.5458,  0.0852, -1.1293, -2.2375,\n",
       "          -1.5000,  2.8161,  0.0441, -1.7804,  0.7914,  0.4864, -2.9973,\n",
       "          -1.3676,  0.3567]]),\n",
       " tensor([[-0.4658, -0.7700,  2.2780,  3.2039,  1.5731, -1.4575, -0.1234,\n",
       "           0.0583,  0.9622,  1.2724, -1.5763,  0.7706, -2.8027,  1.1631,\n",
       "           2.3989, -1.6623,  0.4681, -0.9342,  0.2155,  2.9750, -0.6644,\n",
       "           1.0685,  1.9159,  1.8320,  0.3802, -1.6967,  1.0658, -0.2426,\n",
       "           0.1898,  1.3086]]),\n",
       " tensor([[ 1.3590,  1.3911,  1.0507, -0.5484, -1.5645, -1.9979, -1.9879,\n",
       "          -0.6871,  2.7864,  1.3956, -2.1880,  0.4149,  1.4114,  0.7973,\n",
       "           3.1582,  0.8001, -1.7614,  2.5678, -1.0008, -2.8808,  0.0012,\n",
       "          -1.6583,  0.1592,  1.4461, -3.0678, -0.4532,  2.0035,  0.5358,\n",
       "          -2.1958, -0.6092]]),\n",
       " tensor([[ 2.2057,  2.2583, -2.2390,  0.4411,  0.8164, -1.3688,  0.7956,\n",
       "           0.7109,  0.4166, -0.0500,  2.6196, -1.0400, -1.4394,  0.3198,\n",
       "           0.4574,  2.8131,  0.1549,  0.8350,  3.0385,  1.0956,  0.8138,\n",
       "          -0.1302,  0.0762, -1.4416, -1.2746,  1.3096,  2.1900, -1.0012,\n",
       "           2.6012,  0.2223]]),\n",
       " tensor([[-0.5408,  2.3423,  1.1999, -0.7712,  1.7314,  1.3567, -0.1063,\n",
       "          -1.3815, -0.2539,  0.0692,  1.7720, -0.2266,  2.4683, -1.9914,\n",
       "          -0.7133, -2.3926, -0.3165,  2.3931, -0.6682,  1.5277, -0.3714,\n",
       "          -0.7189,  0.3574, -0.4193, -1.6847, -2.0898, -2.5357, -0.8647,\n",
       "           0.4380,  1.2868]]),\n",
       " tensor([[-0.3813,  1.0328, -0.9225, -2.2910,  2.2756,  3.5202,  1.7791,\n",
       "           0.5678, -1.3924, -0.3861,  0.7529, -1.0697, -0.9486,  0.2193,\n",
       "           0.1749,  1.1102,  1.1227,  0.8314,  1.4561,  0.8142,  2.0442,\n",
       "           1.9096,  2.0117,  0.7002,  0.6531, -0.6169, -0.3873,  1.0430,\n",
       "          -0.7652,  1.7680]]),\n",
       " tensor([[-0.5478,  1.3921, -1.6580,  1.8954, -1.8797,  0.0458,  1.1615,\n",
       "           0.6658,  0.6228,  2.1635, -1.3230, -0.8685, -0.7405,  1.1472,\n",
       "          -0.6961,  0.6610,  1.3321, -2.1961, -1.8785, -0.0645,  2.0759,\n",
       "           1.0787, -0.8033, -1.8607,  1.0203, -0.2249,  1.0538, -0.3836,\n",
       "          -1.3573,  0.2914]]),\n",
       " tensor([[ 0.6164, -0.4217,  1.8439,  0.6076,  2.8703,  0.4930, -1.1329,\n",
       "           1.8403, -2.6513,  1.7409,  0.9187,  1.4998, -1.5404,  0.9290,\n",
       "           2.6054, -0.4654,  2.1537, -0.8310,  1.0618,  1.6469, -0.7659,\n",
       "           0.5902,  1.4928, -0.4677,  2.6551,  0.2043, -2.1191, -0.1800,\n",
       "          -0.8505,  2.7026]]),\n",
       " tensor([[-1.6361, -0.8505,  1.1312, -0.9532,  1.1140,  1.5430, -0.5388,\n",
       "          -1.6287, -3.1145, -2.8539, -1.6604, -1.0351,  0.7086, -1.3651,\n",
       "           0.2754,  1.5238,  1.0556, -1.6602, -2.1802,  0.8448,  0.6368,\n",
       "          -1.0856,  2.5157, -0.7774, -0.2867, -1.1282, -2.9846, -1.6143,\n",
       "           1.5202, -0.4892]]),\n",
       " tensor([[ 0.5815,  1.0740,  1.9706,  3.1746,  0.7896,  2.3447, -2.4393,\n",
       "          -0.7889, -1.0043,  2.0751,  0.5296,  0.9484, -0.3751, -0.2079,\n",
       "          -0.3757, -1.5951, -0.5880,  0.8689, -1.8492, -2.5178, -1.7055,\n",
       "          -1.2925,  2.4284,  0.0974, -0.1795, -1.6088, -2.6904, -1.2829,\n",
       "           1.6457, -2.5409]]),\n",
       " tensor([[ 0.8405,  0.0417, -1.1469,  2.8132,  1.2939,  0.2358,  2.6447,\n",
       "           0.9641,  1.0078, -1.2347,  0.6845,  0.6456,  0.3421,  0.6031,\n",
       "           3.2939,  0.4078, -2.0456, -1.8477, -0.7469, -0.6355, -1.1559,\n",
       "          -0.6543,  1.7856, -0.3594, -0.5748, -0.5534, -0.6840, -0.1208,\n",
       "           2.6182, -0.4119]]),\n",
       " tensor([[-1.4881,  0.2063,  0.7558, -2.3693,  1.3562,  0.3658, -2.0008,\n",
       "          -1.3056,  1.7672, -0.6160, -2.5472,  2.9243,  2.3918,  0.1276,\n",
       "           2.4285, -2.6607,  0.2048,  1.6499,  3.1983,  2.3800, -1.4639,\n",
       "          -0.3602,  0.3224,  0.9940, -0.0605,  0.2161, -2.4095, -2.0393,\n",
       "           0.8771, -1.2424]]),\n",
       " tensor([[ 1.8264,  0.7915,  1.0475,  1.8648, -2.2538,  1.3736,  0.3508,\n",
       "           0.5433, -0.6550, -1.2123,  1.4050,  1.2571, -1.8191, -0.3552,\n",
       "           1.9030, -1.5393,  1.6935,  0.6288, -0.0976, -1.7994, -0.8400,\n",
       "           3.1581, -0.0325, -0.0063,  1.5616,  0.5013,  0.8257,  1.2292,\n",
       "           1.0296, -1.1062]]),\n",
       " tensor([[-2.2540,  0.8495, -1.3276, -2.0999, -0.1342, -2.1146, -0.6378,\n",
       "          -1.3574, -1.2785,  1.8407, -0.6197,  0.9822, -1.2941, -2.1847,\n",
       "           0.6923,  1.7822, -1.7216, -0.7575,  0.0717,  2.0667, -2.8424,\n",
       "          -2.1878, -1.4247,  0.5385,  2.2149,  0.2161,  2.2147, -2.3433,\n",
       "           1.4204,  1.5446]]),\n",
       " tensor([[-0.5203, -1.1595, -0.2179, -1.8535,  1.7804,  2.0656, -0.7902,\n",
       "          -1.9000,  1.5527,  2.6145, -0.2228,  1.3982, -0.0598,  0.2748,\n",
       "           2.1038,  0.2136, -0.5948, -0.5845,  1.2211,  1.1304,  2.4044,\n",
       "          -2.6608, -0.8868,  0.1244, -0.6815,  0.8546,  0.6177,  0.2619,\n",
       "          -1.7936,  0.9216]]),\n",
       " tensor([[-2.0425,  0.8017, -1.1758, -2.1095,  1.3620,  2.8292,  1.5322,\n",
       "          -1.0338,  2.3379, -2.5351,  2.3455, -1.0301,  0.0740,  3.0863,\n",
       "           1.7772,  0.1350, -2.1220, -2.6854,  2.6941,  1.9977, -2.3835,\n",
       "          -0.0248,  1.7417,  1.3103,  0.9119,  0.5131,  0.0610,  1.2811,\n",
       "          -1.1042,  0.9470]]),\n",
       " tensor([[-1.7120, -3.0867, -2.1923, -0.5140,  0.4463, -1.8346, -1.8208,\n",
       "          -1.3994, -2.3537, -1.1869, -0.2242, -1.3857, -0.5629,  0.0955,\n",
       "           0.6074,  0.0638,  1.5025, -1.4870, -0.7019, -0.7546, -1.6679,\n",
       "          -1.2908,  2.0814,  2.0106,  1.1076,  1.0531, -0.8516, -3.0625,\n",
       "          -2.7967,  0.7479]]),\n",
       " tensor([[ 2.3389, -0.8308, -2.6869, -2.4871, -0.5040,  0.9984, -2.7020,\n",
       "          -1.8695,  2.8681, -1.0342,  1.0830,  2.5704,  1.3717,  0.5858,\n",
       "          -0.9618,  1.2720,  0.4659, -0.9301, -1.8787,  1.0433, -2.0904,\n",
       "          -1.7514,  1.8377, -0.8585, -1.0433,  2.0380,  0.5011, -1.7589,\n",
       "           2.0710, -2.3041]]),\n",
       " tensor([[-0.4906, -2.4772,  2.1570,  3.3584,  2.2682,  1.8146,  1.1381,\n",
       "          -0.2383,  1.4157, -1.6073, -0.3634, -1.5185, -0.5643,  0.5694,\n",
       "           1.0430,  0.0046,  1.2677, -0.9117, -1.3418, -1.4651, -0.9510,\n",
       "           2.5264,  1.5142, -2.1727,  1.3724, -1.1657, -0.6964, -0.5938,\n",
       "          -0.4581,  2.3030]]),\n",
       " tensor([[ 1.6604, -0.1057, -2.1103, -0.3080,  0.9411, -1.2876, -0.4651,\n",
       "           0.9447, -2.3617, -0.9189,  3.0379, -1.2689,  2.0913, -0.4220,\n",
       "           0.5022,  0.7255, -0.1873, -1.7051, -1.9140,  1.3415,  1.2061,\n",
       "           2.0834, -2.3832, -2.6052, -2.3493, -0.1458,  0.1829, -2.9314,\n",
       "          -0.9770,  0.0311]]),\n",
       " tensor([[ 2.1188, -2.4847,  0.9379,  0.2795,  0.1498,  1.8816, -0.5886,\n",
       "          -0.8120, -0.7604, -0.9088, -1.7418, -1.8108,  0.6974, -1.0882,\n",
       "          -1.2148, -0.6206,  0.4459,  1.3856,  3.1470,  1.6492,  2.6109,\n",
       "          -1.5465,  0.8642, -2.0263, -1.9889,  1.2245, -1.4467,  1.8231,\n",
       "          -1.7922,  0.4884]]),\n",
       " tensor([[ 0.6726,  1.7267,  2.4592,  0.1025,  0.2553,  1.0929,  2.2762,\n",
       "           0.9830, -1.4366, -1.3765,  3.0088, -2.1088, -0.9359,  1.3556,\n",
       "          -2.7857, -0.9528,  1.4409,  3.1190, -1.2032, -0.1524, -0.7543,\n",
       "          -0.9880, -1.1116, -1.0639,  2.1645, -0.5729,  1.9451,  2.5798,\n",
       "           0.7258,  2.3949]]),\n",
       " tensor([[-1.1629,  0.5647,  1.4159, -1.0089, -2.0218,  0.2144,  0.0230,\n",
       "           1.0360, -0.2357,  2.5893, -2.7767, -0.5051, -1.2474,  1.4108,\n",
       "          -1.9531,  1.0858,  1.0871, -0.7459,  0.1185,  2.3840, -0.8231,\n",
       "           0.1483,  1.2114,  1.3785,  0.7668,  2.8526,  1.4244,  1.3709,\n",
       "          -1.6286,  2.3947]]),\n",
       " tensor([[ 2.4854, -2.6537, -2.5017, -0.0106, -1.2905, -1.6105,  1.8770,\n",
       "           0.7031, -0.3478,  1.9915,  2.2013, -2.6885, -1.0531, -0.9123,\n",
       "          -2.8731, -0.3844,  1.3450,  0.7277, -1.1835,  0.0428,  0.3287,\n",
       "          -2.6373, -0.8451,  0.7937,  0.9980,  0.6323,  1.1691,  2.3835,\n",
       "          -2.1700, -2.2380]]),\n",
       " tensor([[-0.8265,  0.0990, -1.0033,  0.6628,  2.5526, -1.4203,  0.7921,\n",
       "          -1.2917,  2.7388,  1.7087,  1.2971, -1.0790, -1.8332,  2.0798,\n",
       "           0.0969,  2.3442,  0.4940,  2.0853, -0.2946,  2.5449,  0.8024,\n",
       "          -0.2030,  1.3064, -0.5565,  2.4062, -1.3285, -0.3178, -1.7718,\n",
       "           0.7488,  0.9896]]),\n",
       " tensor([[-1.2370,  2.6066, -1.6891,  0.3507, -2.1257, -0.4470, -2.7374,\n",
       "           1.5555,  1.2348,  1.0103,  2.7918,  1.0064, -1.9450,  1.1451,\n",
       "           2.2947,  2.2535, -0.0006, -0.5850, -1.7559,  2.7616, -2.7067,\n",
       "          -0.5461, -0.5530, -1.0872, -1.0950,  2.3286, -0.9234,  1.9525,\n",
       "          -2.5422, -0.2479]]),\n",
       " tensor([[ 1.0923,  1.5726, -2.4554, -0.8525,  1.3416, -0.9894,  0.3519,\n",
       "           1.0543,  0.5390,  1.1700, -1.0662, -0.1291, -0.2874,  0.9204,\n",
       "          -0.0818,  1.9131, -0.3450,  0.6253, -1.4523, -0.0026,  1.8768,\n",
       "          -0.6114, -1.7331,  3.0746,  1.4969, -1.2729, -0.5577,  0.3443,\n",
       "           0.3312,  0.8164]]),\n",
       " tensor([[ 2.0727,  2.1241,  1.5725, -1.3258, -0.4744, -0.6853,  1.5180,\n",
       "           0.0813,  2.3378,  1.0519, -1.1025,  1.6830,  1.5239,  2.3839,\n",
       "          -1.4012, -1.4540,  3.5517, -2.2697,  0.7238, -1.6902,  0.0740,\n",
       "          -0.8183, -0.1742, -0.4710,  0.1319, -1.0694,  2.0092,  0.2020,\n",
       "          -2.1294,  1.7691]]),\n",
       " tensor([[-0.5481,  0.4946, -2.0581, -0.6784,  1.8072,  0.1032,  0.9897,\n",
       "          -1.9079,  1.4917,  0.5153, -2.3696,  0.2806,  1.3227, -1.0309,\n",
       "           0.8554,  1.8037, -0.1330,  0.1624, -1.5686, -2.5359,  2.1467,\n",
       "           1.2616, -0.2130, -0.8935, -0.7385, -0.2298, -0.9599,  1.6986,\n",
       "           1.4063,  2.6057]]),\n",
       " tensor([[ 0.9288,  1.3942,  0.4012, -0.3471,  0.1578, -1.3916, -1.1740,\n",
       "          -1.1163, -1.5319, -1.7566, -0.8262,  2.4486,  0.5385, -1.8623,\n",
       "          -2.3545, -0.8980, -1.6358, -1.4832, -2.8174, -2.0221, -2.4272,\n",
       "          -1.2098, -0.9527,  0.2619,  1.7202, -0.2406,  0.7650,  2.9741,\n",
       "           1.5793,  0.7925]]),\n",
       " tensor([[ 1.9775,  2.4859, -0.5436, -0.2911,  0.8208,  1.8896, -0.6120,\n",
       "          -0.0638, -1.2361,  1.2193, -1.5475,  2.4516,  0.3595, -0.4666,\n",
       "           2.0900, -1.0491,  0.1172,  0.4770,  2.5106,  2.5565,  0.1527,\n",
       "           2.2080,  1.0168, -1.7454, -0.4809, -0.7355,  0.8436, -1.9473,\n",
       "          -1.6647,  0.8442]]),\n",
       " tensor([[-0.0306,  0.2673,  0.4229, -2.3555,  2.8398,  1.8407,  0.6527,\n",
       "          -0.0057,  2.3515, -0.1643,  2.3688, -1.6871, -1.7362,  0.8129,\n",
       "          -0.0904, -0.8234,  2.6796, -2.2573, -1.9371, -0.3836,  0.8846,\n",
       "          -0.1452, -1.0016, -1.0090, -0.0354,  0.9729,  1.7539,  0.7498,\n",
       "          -1.1374,  1.9231]]),\n",
       " tensor([[-1.0119,  2.5834,  1.7719, -0.9227, -2.0483,  1.4404, -1.3875,\n",
       "          -1.5316, -0.7884,  1.8055,  3.0580,  2.2950, -0.9537, -2.2630,\n",
       "           2.1953, -0.6925, -2.2177, -1.2996, -0.9459,  0.7948,  3.3396,\n",
       "           0.0891, -2.4268, -2.2655,  1.1787, -1.1187, -0.6079, -2.8086,\n",
       "           0.5306,  0.8199]]),\n",
       " tensor([[ 1.6631,  1.4551, -0.5678,  0.5777,  0.5616,  1.7070,  0.3805,\n",
       "          -1.4876, -1.9639,  1.4657,  1.7645,  0.8144,  2.9474,  1.0387,\n",
       "          -2.0604,  1.5819,  0.6301,  0.3323, -0.8261,  2.8920,  2.0606,\n",
       "          -0.5331,  2.4814,  1.3967, -0.0194,  0.6540,  1.8489, -0.3199,\n",
       "           1.5058,  0.8273]]),\n",
       " tensor([[-0.0657, -1.9969,  1.8756,  1.8250, -1.1831,  1.0649, -0.4012,\n",
       "          -1.8928,  0.1553,  0.0777, -2.6414, -0.1102, -0.7530,  0.6701,\n",
       "           2.6232, -0.4842, -0.7044,  1.7505,  1.4268, -1.3134, -0.2359,\n",
       "           3.0852, -1.2013, -0.8348, -0.0480,  2.4045,  1.8569, -1.6665,\n",
       "           0.5853, -1.7076]]),\n",
       " tensor([[-1.7343, -1.4015,  2.1212, -0.7322,  1.7683, -0.3862,  1.2963,\n",
       "          -0.4379, -1.0563, -0.5282, -2.1320,  1.3332,  1.9627, -1.3226,\n",
       "          -0.8487,  3.2399,  2.9589,  0.2472,  0.5457, -2.5769, -0.0867,\n",
       "           1.8205,  0.4238, -1.8301,  1.4266,  1.0913,  0.4901,  0.4078,\n",
       "          -0.4775, -1.2921]]),\n",
       " tensor([[ 0.6876,  0.0494,  1.9826,  0.3312, -1.9664, -1.3904, -2.2474,\n",
       "          -0.4860, -0.4016,  1.8286,  0.6292,  0.9915,  1.2991,  2.7924,\n",
       "           2.1399,  1.3247,  1.9196,  2.7876,  0.9641, -1.1216, -2.4505,\n",
       "           0.8530, -1.5428, -2.4374, -0.7446,  1.6478, -0.9955, -2.5957,\n",
       "          -1.2060, -1.0374]]),\n",
       " tensor([[ 0.4114, -2.7174, -2.3988, -2.3613, -2.1585,  2.7058,  0.5839,\n",
       "          -0.1829, -0.3312, -2.3532, -1.8567,  0.3313, -0.8242,  2.0088,\n",
       "           0.9535, -0.4433,  1.2189, -0.6162, -0.0359, -0.1900,  2.2796,\n",
       "           3.2025, -0.8289,  0.3380,  2.5927, -1.6246,  0.6678, -0.5649,\n",
       "           0.7085, -1.8799]]),\n",
       " tensor([[ 0.8415,  1.1213,  3.0069,  1.3314,  2.5675, -2.5316,  0.0423,\n",
       "          -0.4738, -1.5937,  2.6843, -1.2615,  0.3662,  1.7438,  1.7154,\n",
       "           2.4883, -2.3955,  0.2727, -2.1495, -0.2968,  2.4558, -1.8014,\n",
       "           0.8317, -1.5139,  1.4770, -0.3621,  1.6594, -0.5803, -1.1578,\n",
       "          -1.9912, -0.5868]]),\n",
       " tensor([[ 0.0939, -0.5808, -0.3993,  1.9467, -1.9898, -1.5801,  2.3091,\n",
       "           0.1545, -0.7118, -0.7798,  2.7009, -1.3250,  0.1450, -0.3217,\n",
       "          -0.1608,  1.2191, -0.2273,  2.3408,  1.4021,  2.7012,  0.6027,\n",
       "          -0.5966, -2.4676,  0.7391,  2.4811,  0.2180,  1.1333, -0.6058,\n",
       "           2.3509,  2.7842]]),\n",
       " tensor([[-0.8735,  1.0363,  1.8935,  0.9560,  0.3646,  3.4255,  0.1993,\n",
       "          -1.5236,  1.8633,  1.3641, -0.9202,  1.3994, -1.2866,  1.3911,\n",
       "           2.1956, -2.4011,  0.3221,  2.5899,  0.9170, -0.9436,  0.2543,\n",
       "           0.8545, -1.6664,  0.1312,  0.0459, -0.6056,  0.3879, -0.8643,\n",
       "          -0.4629, -1.9544]]),\n",
       " tensor([[-0.1097,  0.1492, -1.5744, -0.4874, -0.6453,  0.6020, -0.7945,\n",
       "           2.6381, -1.6756,  1.3219,  2.2147,  1.4785, -0.2441,  0.0591,\n",
       "           0.4792,  2.0033, -1.3703, -0.0362, -0.3570,  1.3257, -0.0113,\n",
       "          -0.7282,  2.2377,  2.8774,  1.0424, -2.9842,  1.4900, -1.2906,\n",
       "          -1.8394,  1.6450]]),\n",
       " tensor([[ 0.4838,  1.6700,  2.5671, -1.2535, -0.3759,  1.4514,  1.9749,\n",
       "          -1.7941, -2.7556,  3.0362,  0.2852,  1.7085, -1.2374,  1.3746,\n",
       "           2.2651,  1.7798, -2.4066,  0.8245, -0.5291,  2.6797,  1.2826,\n",
       "           0.8405, -0.5283,  0.5832,  1.0739, -2.4628,  1.8186,  1.7552,\n",
       "           0.9734,  2.8437]]),\n",
       " tensor([[-2.3961,  1.3706, -0.3167,  1.6362, -1.0882, -2.0116,  0.6257,\n",
       "           0.5834,  1.4716, -1.9227,  0.6741,  2.4247,  1.5073,  1.3625,\n",
       "           0.5294,  1.4426,  1.5561,  0.7442,  1.4454,  2.2417, -2.2846,\n",
       "           0.3177, -0.5069, -2.7332, -1.8995,  2.9971, -0.7321, -0.3620,\n",
       "          -0.4184, -0.2837]]),\n",
       " tensor([[ 0.8555,  0.0287, -1.9651,  1.2867,  0.3858, -2.1484, -0.4595,\n",
       "           1.1783,  2.7867,  0.2679,  1.0226, -1.1398, -2.5441, -1.9405,\n",
       "          -1.2679,  2.2796, -2.8712,  1.3390,  3.2245,  1.1224, -2.8499,\n",
       "          -0.1175, -1.3589,  1.9900,  0.5646,  0.1231, -1.2539, -0.7069,\n",
       "          -1.5795,  2.2747]]),\n",
       " tensor([[-0.6695, -0.3481, -1.2109, -1.2516,  1.8648,  0.1529, -1.0916,\n",
       "          -2.4423, -0.4164, -1.3596, -0.6474,  1.5868, -0.3450,  2.3848,\n",
       "           1.3646, -0.5726,  1.6891, -1.2329,  0.1218,  1.7425,  0.3629,\n",
       "          -1.8716,  1.2274,  0.8689, -1.8390,  1.4727,  1.7959,  1.1660,\n",
       "           1.0056, -0.1536]]),\n",
       " tensor([[-1.2974, -0.1722, -0.3602,  0.9340,  0.4636,  1.4041,  1.1398,\n",
       "          -1.5602,  2.5539, -1.1154,  1.1044, -0.0215, -0.7909,  0.9404,\n",
       "          -2.0608, -0.5721, -1.4708,  2.3221, -1.5976,  1.2946,  0.9628,\n",
       "           0.2877,  3.0296,  0.8648,  1.2914, -2.9831,  2.5857,  1.1383,\n",
       "           1.3648,  0.3066]]),\n",
       " tensor([[-2.3646,  0.4362,  0.4325, -2.8180,  2.5254,  0.8828,  0.4493,\n",
       "          -2.0543, -0.7864,  2.8405, -0.6039, -2.4102,  0.6682,  2.2316,\n",
       "           1.2255, -0.4721,  0.9678, -1.3287,  0.0079,  1.2373,  2.5499,\n",
       "           1.1633, -0.6557, -0.3607, -0.6189,  2.4628, -1.1671,  1.3795,\n",
       "          -2.1304,  0.9102]]),\n",
       " tensor([[-0.1000, -0.4775,  1.0243,  0.8184, -2.1900,  1.0443, -2.2133,\n",
       "          -1.0665, -2.1897,  0.5078, -1.4929,  1.5010,  0.1925, -0.1062,\n",
       "           1.7403,  2.5015,  1.2989, -2.6369,  1.2338, -2.7009, -0.0108,\n",
       "          -1.1646,  1.5472, -0.6746, -0.2517,  1.0154,  1.6104, -1.5007,\n",
       "          -1.9039, -0.5926]]),\n",
       " tensor([[ 2.5676,  2.1434,  0.0520, -1.8667, -1.2194, -0.2917,  1.5741,\n",
       "           0.8669, -1.2720, -1.6658,  0.9682, -1.2706, -0.0137,  0.6198,\n",
       "           0.4366, -2.2885,  3.2662, -1.5221, -2.5528, -2.0765, -1.0832,\n",
       "           2.6646,  2.9898,  0.5747, -1.3824,  1.5916, -0.6539, -1.7385,\n",
       "           0.2389,  2.0591]]),\n",
       " tensor([[-1.7770, -0.1867, -0.7002,  2.7132, -2.1897, -1.9816, -0.3600,\n",
       "          -2.0871, -1.9864, -1.2034,  2.6343,  2.1906,  0.6985, -2.8435,\n",
       "          -1.6794,  1.7735,  3.2400,  1.3133,  2.8959,  0.6953, -1.0668,\n",
       "           0.1527,  2.1797, -1.4889,  0.8948,  3.2952, -2.7525, -1.1838,\n",
       "           2.3228,  0.5643]]),\n",
       " tensor([[-0.5712, -1.8384,  1.6729,  1.3294,  1.1998, -0.1930, -2.2433,\n",
       "           1.7839,  0.7845,  1.4555,  1.5921,  2.7585, -0.1009, -1.0208,\n",
       "          -2.4213, -1.0763,  1.7663,  2.7612,  3.1750, -1.4854, -0.1755,\n",
       "           0.7498,  2.5062, -1.5189, -2.2532,  1.5034,  0.2366, -0.9523,\n",
       "           1.4251,  2.8622]]),\n",
       " tensor([[-0.4738, -0.9514,  0.8059, -0.8513, -1.6711, -0.9965,  1.0236,\n",
       "           0.5613,  1.9590, -1.2385, -0.0960,  1.1609,  1.8528, -1.3532,\n",
       "          -1.7125, -0.5936, -1.9566,  0.3000, -0.0540,  2.8913,  1.7363,\n",
       "          -1.3735,  0.1632,  1.5672, -0.8984, -0.3823,  0.2427,  1.2465,\n",
       "          -0.3320, -1.6651]]),\n",
       " tensor([[-0.3756, -0.6846, -0.4205,  1.0259,  2.9300,  0.8057, -2.8089,\n",
       "          -1.9957,  2.1029,  0.1256, -0.4685,  1.9818,  2.2817,  1.4143,\n",
       "           1.4612,  2.3791,  0.8729, -0.6903,  2.9722, -1.2522,  1.6361,\n",
       "          -0.6605,  3.1820, -0.5270,  2.4191, -1.3964,  0.1030,  0.6208,\n",
       "          -2.3574, -1.0082]]),\n",
       " tensor([[ 1.2818,  1.5991,  0.3274,  0.4082,  1.2402,  1.2882,  0.3172,\n",
       "           1.4098, -1.8877, -0.0693, -0.6312, -2.0908,  3.2543,  1.7139,\n",
       "           2.9241,  1.4587,  0.7419, -0.4991, -0.9000, -0.2182,  1.4804,\n",
       "           1.4910,  1.8014,  0.5490,  1.6436,  0.9766, -1.7079, -0.8979,\n",
       "           0.3027, -0.4546]]),\n",
       " tensor([[ 2.1267,  1.3519, -0.9102, -2.6047,  0.3167, -1.0796,  1.3953,\n",
       "           1.3122,  1.6380, -2.0869, -2.6007, -0.2716,  0.6197,  1.9716,\n",
       "           1.1222, -0.6225,  0.1311, -1.1813,  0.1426, -0.8748, -0.3020,\n",
       "           0.7337,  1.6456,  2.0848, -0.1620,  0.6843, -0.5147, -2.1945,\n",
       "           1.1412,  1.6821]]),\n",
       " tensor([[-1.5697,  2.8816, -2.3413, -0.3476,  1.8962, -1.2449, -1.0367,\n",
       "          -1.4856, -1.8451, -0.0406,  1.0535,  1.7233,  0.3486, -1.0806,\n",
       "           1.2917,  2.1234, -0.2449, -1.7409, -1.6100, -0.4525,  1.9367,\n",
       "           1.0529,  1.8270, -0.2307, -0.2896,  1.2147, -1.0866,  0.4767,\n",
       "           1.2641, -0.9588]]),\n",
       " tensor([[-2.5836,  2.3632, -0.4674, -1.2825, -2.5506, -0.8805,  1.3715,\n",
       "           1.5881, -1.8469,  2.9288,  0.8576, -1.9613, -1.2146, -0.4621,\n",
       "           2.4898,  0.9628,  0.4690,  1.4098,  0.6918, -0.4964, -2.8639,\n",
       "           0.5701, -1.6806, -0.4244, -1.3482, -1.5825,  1.7275,  1.7853,\n",
       "           3.1289, -2.4840]]),\n",
       " tensor([[ 1.0979,  0.6899,  1.5388,  0.2874, -0.0414,  1.6615, -1.0069,\n",
       "          -0.4696, -2.2346,  0.4949, -1.2838, -1.9656,  1.7731,  1.7782,\n",
       "           1.9714,  2.1742,  1.6310, -2.2849,  0.9186, -0.7662,  0.0810,\n",
       "          -1.4802,  0.4829,  1.7156, -2.6576,  1.5498, -0.7029, -0.6629,\n",
       "           1.3946,  0.4806]]),\n",
       " tensor([[-2.1054, -0.0478,  0.2455, -0.3460,  1.5072, -2.8031, -1.5915,\n",
       "           1.7988,  2.7185,  2.7049,  1.0606, -1.5521, -2.5394, -0.9833,\n",
       "          -1.6493, -0.4691,  3.2420,  0.9974,  0.2356,  0.7294,  0.1656,\n",
       "          -2.1540, -0.2281,  2.1670, -1.1442,  0.9816,  1.4415, -1.6544,\n",
       "          -0.2474, -2.1008]]),\n",
       " tensor([[-2.3654,  2.7336,  1.8513, -1.3833, -0.3158,  2.3705,  1.4146,\n",
       "          -0.2871,  2.4481,  2.4575, -0.8211, -1.2724,  1.8573, -1.3478,\n",
       "          -0.8250,  1.0738,  2.4732,  0.9482, -0.0337,  2.3924,  2.4038,\n",
       "           0.2086, -2.3050,  1.7550,  2.5148,  1.6696,  0.9741,  0.1508,\n",
       "          -0.3056, -1.1019]]),\n",
       " tensor([[-0.4990,  0.5452,  2.6911,  2.9728, -0.2599,  1.7424, -0.6261,\n",
       "           2.6928,  1.4497, -2.8393, -0.1032,  0.4846, -0.2341,  0.3710,\n",
       "           0.0012, -1.5140,  1.2486,  0.9439, -0.0760, -2.0919,  2.8647,\n",
       "           1.0753,  1.7953, -0.3410, -0.2554,  1.0336,  1.5283,  0.0623,\n",
       "           0.5022, -1.4117]]),\n",
       " tensor([[ 0.3394,  1.8584, -0.6004, -1.3650,  0.9724, -1.8503,  0.4698,\n",
       "           0.6083,  1.6354,  0.2673,  0.8475, -0.9345,  0.8159,  0.4022,\n",
       "          -2.7337, -1.6898, -0.4174,  1.4930, -0.9038, -1.1620,  0.2717,\n",
       "           1.3491, -0.9294,  1.9046,  2.2571, -0.2727, -1.6360, -1.3650,\n",
       "          -2.2580,  1.5176]]),\n",
       " tensor([[ 2.6813,  2.3149, -1.0350, -1.3892,  2.6314, -1.0120,  1.1275,\n",
       "          -0.4496,  0.5465,  0.4389,  0.7590,  0.4809, -1.7027,  1.8085,\n",
       "          -0.4870,  1.2897,  0.2437, -1.0172, -0.4620,  1.7053, -0.8094,\n",
       "           2.7433,  1.8408, -1.1367, -1.0373, -1.7263,  0.8289, -0.7439,\n",
       "           0.1717,  2.0136]]),\n",
       " tensor([[ 2.0743, -0.3414,  0.7023,  2.5633,  2.6479,  0.5285, -0.4386,\n",
       "           2.6407,  1.5775, -0.2029,  2.0546,  0.0427,  1.5999,  1.1539,\n",
       "           0.0202, -2.0592, -1.5027, -1.8303, -1.9769, -0.7631,  1.4982,\n",
       "           1.4799, -1.9650,  0.7251,  2.4145, -0.1894,  0.3374, -2.0133,\n",
       "          -0.3941, -1.0721]]),\n",
       " tensor([[-1.6043, -1.6068,  0.2900,  0.5357,  1.0498,  2.7106, -0.9963,\n",
       "          -2.1908,  0.5947, -0.6966, -1.4986,  0.7649,  0.8288,  0.4006,\n",
       "          -0.2700,  1.8466,  1.1967,  2.5521,  2.2178, -0.2349, -0.3013,\n",
       "          -0.6035, -2.4981,  0.7450,  2.2718, -1.2585, -0.5225,  2.9595,\n",
       "          -0.9532, -1.7455]]),\n",
       " tensor([[ 0.3510,  0.3399, -0.1117, -0.0309, -2.3514,  1.2623,  2.8545,\n",
       "          -1.4912, -2.0122, -1.8209,  1.4845,  0.6621,  0.5551, -0.3060,\n",
       "           1.1429, -0.7554,  1.1180, -1.2908,  0.5888,  1.6496, -2.1798,\n",
       "          -0.0972, -1.2487,  1.8159, -2.1976,  1.2022,  0.9060, -1.9074,\n",
       "           0.7524, -1.4981]]),\n",
       " tensor([[ 3.5256,  1.6245, -0.8483,  1.8593,  2.2494, -1.1347, -0.8398,\n",
       "           2.1399, -0.0099,  0.7183,  0.6113,  1.9174, -0.0918,  2.1484,\n",
       "           0.8027, -0.8576,  0.4861, -1.0169,  1.7209, -0.9206, -0.7003,\n",
       "          -1.1958, -1.9561, -1.1993,  1.7816,  1.2057, -0.3315, -1.0648,\n",
       "           0.7877,  2.0688]]),\n",
       " tensor([[-1.8102,  1.6868, -1.2434,  1.8769,  1.2124, -1.5845, -0.6014,\n",
       "          -0.2403,  1.5786, -0.7737, -0.0179, -2.4543, -0.8684, -2.6077,\n",
       "          -2.4603,  2.7336,  1.9440,  2.6092,  0.0866, -0.9112,  0.4467,\n",
       "          -0.6548, -0.2562, -1.3968,  1.1819, -0.1168,  0.2615,  1.2474,\n",
       "          -0.1187,  0.1279]]),\n",
       " tensor([[-0.6832,  1.1363, -2.7516, -2.1608, -0.3218,  3.4739,  1.7604,\n",
       "          -1.5229,  0.7343,  0.8589, -1.0608,  2.7009,  2.1906, -2.3874,\n",
       "          -2.5041, -1.9363,  0.0318, -0.4419,  2.1237,  1.4841, -0.9151,\n",
       "           2.1775, -1.1623, -1.7248,  1.0919,  1.6041, -1.0760,  1.5233,\n",
       "          -2.3226, -0.5793]]),\n",
       " tensor([[ 0.7054, -1.0866, -1.4593,  0.0006, -0.4942,  2.3091, -0.7491,\n",
       "           1.2681,  1.0669,  3.0022, -1.5983,  2.9833, -1.1141,  0.3124,\n",
       "           1.1421, -2.3880, -1.9076,  1.8834,  1.2266,  0.7153,  0.4152,\n",
       "          -0.7252,  1.3807,  1.7165, -2.1467, -1.2810,  0.7876,  0.0699,\n",
       "           1.7958,  0.5737]]),\n",
       " tensor([[ 0.9711, -1.7934, -1.1273,  1.9039,  2.3033, -0.5331,  0.7018,\n",
       "          -2.4171, -1.7933,  1.0891, -1.1831,  0.9182,  2.5902, -0.8046,\n",
       "           1.0834, -2.0177,  1.3750, -2.9574, -2.7389, -0.6265,  2.6434,\n",
       "          -1.5180, -2.0350,  0.9157,  0.7212,  0.0295, -1.3213,  1.2899,\n",
       "           0.2985,  2.3335]]),\n",
       " tensor([[-1.1320,  2.9694,  2.6826, -0.7654,  1.7408,  0.6092, -1.4189,\n",
       "          -2.1000, -1.9834, -1.1263,  0.3326,  0.6586,  0.2094,  0.3613,\n",
       "           1.1934, -0.9356, -0.1967,  2.6378, -0.7280,  1.8870, -1.4436,\n",
       "           0.5918,  2.5932, -0.7791,  1.5608,  2.5863, -0.2805, -0.4398,\n",
       "          -2.5135,  1.3378]]),\n",
       " tensor([[ 1.4014, -0.4933,  0.8007,  1.6676, -0.0681, -0.9561,  1.2190,\n",
       "          -1.3029,  0.6674,  1.0957, -1.2409,  1.6118, -0.0039, -0.2334,\n",
       "           0.2724,  1.2575, -0.7425, -1.4989,  2.7197, -2.1284, -1.3179,\n",
       "           0.2069,  0.8060,  1.0657,  0.0255,  1.5152,  2.9647,  2.3641,\n",
       "           2.4725,  1.8781]]),\n",
       " tensor([[ 0.6587,  2.6755,  0.3965,  0.2516,  0.2693,  1.5814, -2.7415,\n",
       "           1.6185, -1.7094,  0.2985,  0.2199, -0.1951, -0.2801,  0.5091,\n",
       "          -0.7386,  0.3004,  0.1268,  0.0393,  0.8862,  1.3474, -0.7631,\n",
       "           0.5697, -0.3702,  1.2292,  1.3598, -0.1165,  0.9574,  3.0467,\n",
       "          -2.3858,  0.0519]]),\n",
       " tensor([[-1.9518,  1.4609, -1.2668,  1.1016,  0.6965,  1.8162, -0.1727,\n",
       "           0.8141, -2.1452, -0.3089,  1.9505,  0.7380,  0.5500,  2.3836,\n",
       "          -0.0394,  0.3657,  0.8607,  2.5887, -0.9975, -1.9036,  2.6082,\n",
       "          -2.2324,  2.0830, -1.8685, -0.0900, -1.7113, -1.1719, -1.1642,\n",
       "          -0.4184,  1.8755]]),\n",
       " tensor([[-0.5058, -2.2684,  0.8458, -1.3535,  1.4715,  0.5282,  0.5907,\n",
       "           1.8687,  0.2909,  2.1370, -0.3764,  1.6588, -0.3748, -2.0951,\n",
       "           1.3661,  0.7942, -2.1076,  2.4776, -2.1691, -1.3188,  2.9136,\n",
       "          -1.4898, -2.1826,  0.6339,  1.4838, -2.3198,  0.6245,  1.3719,\n",
       "          -0.6960,  1.6846]]),\n",
       " tensor([[-2.0193,  0.4832,  1.5456, -1.7035,  0.6485, -0.0949, -0.6632,\n",
       "          -1.1796, -0.0740,  0.4709,  1.1933, -1.6580, -0.2840, -0.4736,\n",
       "          -2.1696, -1.0329,  1.8073,  0.7816, -0.2876,  2.3103, -2.2218,\n",
       "          -0.9865,  0.8224, -1.6115, -1.4517,  2.3172, -2.5649,  1.1784,\n",
       "          -1.1683, -0.6022]])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can train and test model on the generated data\n",
    "synthentic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert array of tensor to tensors\n",
    "temp = torch.Tensor(99)\n",
    "synthentic_data = torch.cat(synthentic_data, out=temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.600459</td>\n",
       "      <td>0.465072</td>\n",
       "      <td>2.163063</td>\n",
       "      <td>-0.565845</td>\n",
       "      <td>-1.660129</td>\n",
       "      <td>-0.039433</td>\n",
       "      <td>-2.017517</td>\n",
       "      <td>0.405838</td>\n",
       "      <td>0.940746</td>\n",
       "      <td>0.296212</td>\n",
       "      <td>...</td>\n",
       "      <td>1.248405</td>\n",
       "      <td>0.751866</td>\n",
       "      <td>-0.118424</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>-0.524080</td>\n",
       "      <td>-2.590899</td>\n",
       "      <td>-0.145017</td>\n",
       "      <td>3.332857</td>\n",
       "      <td>-1.857148</td>\n",
       "      <td>-2.599469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.449250</td>\n",
       "      <td>-0.963133</td>\n",
       "      <td>0.122163</td>\n",
       "      <td>3.094393</td>\n",
       "      <td>-1.089969</td>\n",
       "      <td>0.146805</td>\n",
       "      <td>-0.838032</td>\n",
       "      <td>-2.348915</td>\n",
       "      <td>2.974649</td>\n",
       "      <td>-1.542879</td>\n",
       "      <td>...</td>\n",
       "      <td>2.851528</td>\n",
       "      <td>-2.390328</td>\n",
       "      <td>1.043473</td>\n",
       "      <td>1.997388</td>\n",
       "      <td>-1.397125</td>\n",
       "      <td>-2.826992</td>\n",
       "      <td>1.157478</td>\n",
       "      <td>1.132260</td>\n",
       "      <td>-0.694053</td>\n",
       "      <td>-0.698702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.936718</td>\n",
       "      <td>0.530892</td>\n",
       "      <td>0.156811</td>\n",
       "      <td>2.590857</td>\n",
       "      <td>0.366070</td>\n",
       "      <td>0.412720</td>\n",
       "      <td>0.389496</td>\n",
       "      <td>-0.173952</td>\n",
       "      <td>-1.402256</td>\n",
       "      <td>0.238771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.894853</td>\n",
       "      <td>2.267772</td>\n",
       "      <td>-2.292738</td>\n",
       "      <td>-0.419909</td>\n",
       "      <td>1.447357</td>\n",
       "      <td>2.251052</td>\n",
       "      <td>0.753249</td>\n",
       "      <td>0.521450</td>\n",
       "      <td>-0.223595</td>\n",
       "      <td>-1.614821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.297347</td>\n",
       "      <td>-0.527299</td>\n",
       "      <td>-1.314723</td>\n",
       "      <td>-1.911067</td>\n",
       "      <td>-0.576052</td>\n",
       "      <td>-1.177065</td>\n",
       "      <td>1.899386</td>\n",
       "      <td>-1.129185</td>\n",
       "      <td>-2.108203</td>\n",
       "      <td>1.491231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>1.458786</td>\n",
       "      <td>2.790831</td>\n",
       "      <td>1.173577</td>\n",
       "      <td>-2.438058</td>\n",
       "      <td>0.585100</td>\n",
       "      <td>2.298208</td>\n",
       "      <td>-0.278277</td>\n",
       "      <td>2.336517</td>\n",
       "      <td>0.996671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.897689</td>\n",
       "      <td>2.097216</td>\n",
       "      <td>3.001771</td>\n",
       "      <td>-0.100336</td>\n",
       "      <td>-0.914664</td>\n",
       "      <td>-0.842301</td>\n",
       "      <td>-1.916697</td>\n",
       "      <td>-1.935876</td>\n",
       "      <td>-0.346499</td>\n",
       "      <td>-1.618192</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.187718</td>\n",
       "      <td>2.269789</td>\n",
       "      <td>-0.561446</td>\n",
       "      <td>0.192087</td>\n",
       "      <td>-0.198630</td>\n",
       "      <td>0.538794</td>\n",
       "      <td>-1.406878</td>\n",
       "      <td>-1.585132</td>\n",
       "      <td>1.868896</td>\n",
       "      <td>0.673201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.600459  0.465072  2.163063 -0.565845 -1.660129 -0.039433 -2.017517   \n",
       "1  1.449250 -0.963133  0.122163  3.094393 -1.089969  0.146805 -0.838032   \n",
       "2  2.936718  0.530892  0.156811  2.590857  0.366070  0.412720  0.389496   \n",
       "3 -1.297347 -0.527299 -1.314723 -1.911067 -0.576052 -1.177065  1.899386   \n",
       "4  1.897689  2.097216  3.001771 -0.100336 -0.914664 -0.842301 -1.916697   \n",
       "\n",
       "         7         8         9     ...           20        21        22  \\\n",
       "0  0.405838  0.940746  0.296212    ...     1.248405  0.751866 -0.118424   \n",
       "1 -2.348915  2.974649 -1.542879    ...     2.851528 -2.390328  1.043473   \n",
       "2 -0.173952 -1.402256  0.238771    ...    -0.894853  2.267772 -2.292738   \n",
       "3 -1.129185 -2.108203  1.491231    ...     0.005204  1.458786  2.790831   \n",
       "4 -1.935876 -0.346499 -1.618192    ...    -1.187718  2.269789 -0.561446   \n",
       "\n",
       "         23        24        25        26        27        28        29  \n",
       "0  0.945000 -0.524080 -2.590899 -0.145017  3.332857 -1.857148 -2.599469  \n",
       "1  1.997388 -1.397125 -2.826992  1.157478  1.132260 -0.694053 -0.698702  \n",
       "2 -0.419909  1.447357  2.251052  0.753249  0.521450 -0.223595 -1.614821  \n",
       "3  1.173577 -2.438058  0.585100  2.298208 -0.278277  2.336517  0.996671  \n",
       "4  0.192087 -0.198630  0.538794 -1.406878 -1.585132  1.868896  0.673201  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tensor to PD data frame\n",
    "synthentic_data_df = pd.DataFrame(data=synthentic_data.data.numpy());\n",
    "synthentic_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns of the synthentic dataset\n",
    "cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "        'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount', 'Class']\n",
    "synthentic_data_df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normAmount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.600459</td>\n",
       "      <td>0.465072</td>\n",
       "      <td>2.163063</td>\n",
       "      <td>-0.565845</td>\n",
       "      <td>-1.660129</td>\n",
       "      <td>-0.039433</td>\n",
       "      <td>-2.017517</td>\n",
       "      <td>0.405838</td>\n",
       "      <td>0.940746</td>\n",
       "      <td>0.296212</td>\n",
       "      <td>...</td>\n",
       "      <td>1.248405</td>\n",
       "      <td>0.751866</td>\n",
       "      <td>-0.118424</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>-0.524080</td>\n",
       "      <td>-2.590899</td>\n",
       "      <td>-0.145017</td>\n",
       "      <td>3.332857</td>\n",
       "      <td>-1.857148</td>\n",
       "      <td>-2.599469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.449250</td>\n",
       "      <td>-0.963133</td>\n",
       "      <td>0.122163</td>\n",
       "      <td>3.094393</td>\n",
       "      <td>-1.089969</td>\n",
       "      <td>0.146805</td>\n",
       "      <td>-0.838032</td>\n",
       "      <td>-2.348915</td>\n",
       "      <td>2.974649</td>\n",
       "      <td>-1.542879</td>\n",
       "      <td>...</td>\n",
       "      <td>2.851528</td>\n",
       "      <td>-2.390328</td>\n",
       "      <td>1.043473</td>\n",
       "      <td>1.997388</td>\n",
       "      <td>-1.397125</td>\n",
       "      <td>-2.826992</td>\n",
       "      <td>1.157478</td>\n",
       "      <td>1.132260</td>\n",
       "      <td>-0.694053</td>\n",
       "      <td>-0.698702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.936718</td>\n",
       "      <td>0.530892</td>\n",
       "      <td>0.156811</td>\n",
       "      <td>2.590857</td>\n",
       "      <td>0.366070</td>\n",
       "      <td>0.412720</td>\n",
       "      <td>0.389496</td>\n",
       "      <td>-0.173952</td>\n",
       "      <td>-1.402256</td>\n",
       "      <td>0.238771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.894853</td>\n",
       "      <td>2.267772</td>\n",
       "      <td>-2.292738</td>\n",
       "      <td>-0.419909</td>\n",
       "      <td>1.447357</td>\n",
       "      <td>2.251052</td>\n",
       "      <td>0.753249</td>\n",
       "      <td>0.521450</td>\n",
       "      <td>-0.223595</td>\n",
       "      <td>-1.614821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.297347</td>\n",
       "      <td>-0.527299</td>\n",
       "      <td>-1.314723</td>\n",
       "      <td>-1.911067</td>\n",
       "      <td>-0.576052</td>\n",
       "      <td>-1.177065</td>\n",
       "      <td>1.899386</td>\n",
       "      <td>-1.129185</td>\n",
       "      <td>-2.108203</td>\n",
       "      <td>1.491231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>1.458786</td>\n",
       "      <td>2.790831</td>\n",
       "      <td>1.173577</td>\n",
       "      <td>-2.438058</td>\n",
       "      <td>0.585100</td>\n",
       "      <td>2.298208</td>\n",
       "      <td>-0.278277</td>\n",
       "      <td>2.336517</td>\n",
       "      <td>0.996671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.897689</td>\n",
       "      <td>2.097216</td>\n",
       "      <td>3.001771</td>\n",
       "      <td>-0.100336</td>\n",
       "      <td>-0.914664</td>\n",
       "      <td>-0.842301</td>\n",
       "      <td>-1.916697</td>\n",
       "      <td>-1.935876</td>\n",
       "      <td>-0.346499</td>\n",
       "      <td>-1.618192</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.187718</td>\n",
       "      <td>2.269789</td>\n",
       "      <td>-0.561446</td>\n",
       "      <td>0.192087</td>\n",
       "      <td>-0.198630</td>\n",
       "      <td>0.538794</td>\n",
       "      <td>-1.406878</td>\n",
       "      <td>-1.585132</td>\n",
       "      <td>1.868896</td>\n",
       "      <td>0.673201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.841477</td>\n",
       "      <td>0.686090</td>\n",
       "      <td>1.501176</td>\n",
       "      <td>1.392911</td>\n",
       "      <td>0.081821</td>\n",
       "      <td>1.222786</td>\n",
       "      <td>1.846111</td>\n",
       "      <td>-1.645100</td>\n",
       "      <td>-3.172367</td>\n",
       "      <td>0.943527</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.424388</td>\n",
       "      <td>-0.472715</td>\n",
       "      <td>2.589546</td>\n",
       "      <td>2.207823</td>\n",
       "      <td>2.039515</td>\n",
       "      <td>-0.784403</td>\n",
       "      <td>0.298666</td>\n",
       "      <td>2.318167</td>\n",
       "      <td>2.968308</td>\n",
       "      <td>2.367115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.674039</td>\n",
       "      <td>0.764513</td>\n",
       "      <td>-1.856403</td>\n",
       "      <td>0.981205</td>\n",
       "      <td>-1.359101</td>\n",
       "      <td>-1.876309</td>\n",
       "      <td>1.296131</td>\n",
       "      <td>-1.512398</td>\n",
       "      <td>0.680637</td>\n",
       "      <td>1.958164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.498954</td>\n",
       "      <td>-2.600328</td>\n",
       "      <td>-1.657590</td>\n",
       "      <td>-1.863385</td>\n",
       "      <td>1.834593</td>\n",
       "      <td>-1.034303</td>\n",
       "      <td>1.814876</td>\n",
       "      <td>-2.764538</td>\n",
       "      <td>0.865730</td>\n",
       "      <td>1.777931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.116215</td>\n",
       "      <td>-0.378341</td>\n",
       "      <td>-1.409316</td>\n",
       "      <td>2.403500</td>\n",
       "      <td>0.995584</td>\n",
       "      <td>-2.106048</td>\n",
       "      <td>-1.468643</td>\n",
       "      <td>-1.745988</td>\n",
       "      <td>-2.734196</td>\n",
       "      <td>-1.678218</td>\n",
       "      <td>...</td>\n",
       "      <td>2.343087</td>\n",
       "      <td>2.617907</td>\n",
       "      <td>0.780009</td>\n",
       "      <td>-1.826149</td>\n",
       "      <td>-1.716911</td>\n",
       "      <td>-0.726991</td>\n",
       "      <td>1.703366</td>\n",
       "      <td>0.980359</td>\n",
       "      <td>-1.323486</td>\n",
       "      <td>0.878476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.198998</td>\n",
       "      <td>1.178543</td>\n",
       "      <td>2.076991</td>\n",
       "      <td>-0.657220</td>\n",
       "      <td>1.408336</td>\n",
       "      <td>1.744330</td>\n",
       "      <td>-1.337416</td>\n",
       "      <td>-0.849860</td>\n",
       "      <td>-1.894325</td>\n",
       "      <td>1.655279</td>\n",
       "      <td>...</td>\n",
       "      <td>1.201754</td>\n",
       "      <td>1.786878</td>\n",
       "      <td>1.701576</td>\n",
       "      <td>0.326690</td>\n",
       "      <td>-0.662766</td>\n",
       "      <td>-0.091425</td>\n",
       "      <td>1.531055</td>\n",
       "      <td>-0.704209</td>\n",
       "      <td>-0.658600</td>\n",
       "      <td>0.931092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.788301</td>\n",
       "      <td>-0.739811</td>\n",
       "      <td>-2.907268</td>\n",
       "      <td>0.734854</td>\n",
       "      <td>1.072244</td>\n",
       "      <td>-0.231931</td>\n",
       "      <td>2.607131</td>\n",
       "      <td>1.455274</td>\n",
       "      <td>0.721918</td>\n",
       "      <td>-1.339980</td>\n",
       "      <td>...</td>\n",
       "      <td>1.900118</td>\n",
       "      <td>-2.193805</td>\n",
       "      <td>0.871282</td>\n",
       "      <td>2.278460</td>\n",
       "      <td>-1.408634</td>\n",
       "      <td>2.461220</td>\n",
       "      <td>-0.967964</td>\n",
       "      <td>1.467767</td>\n",
       "      <td>2.802646</td>\n",
       "      <td>2.020118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.504805</td>\n",
       "      <td>-0.838070</td>\n",
       "      <td>1.868727</td>\n",
       "      <td>1.585570</td>\n",
       "      <td>1.473725</td>\n",
       "      <td>-0.340161</td>\n",
       "      <td>1.262059</td>\n",
       "      <td>-1.681823</td>\n",
       "      <td>2.936888</td>\n",
       "      <td>-1.083564</td>\n",
       "      <td>...</td>\n",
       "      <td>1.607450</td>\n",
       "      <td>2.338186</td>\n",
       "      <td>-1.204149</td>\n",
       "      <td>1.676125</td>\n",
       "      <td>-1.815302</td>\n",
       "      <td>-1.377590</td>\n",
       "      <td>0.210884</td>\n",
       "      <td>0.026440</td>\n",
       "      <td>-0.690522</td>\n",
       "      <td>0.271184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.799095</td>\n",
       "      <td>-1.811295</td>\n",
       "      <td>0.932625</td>\n",
       "      <td>-1.164581</td>\n",
       "      <td>0.020809</td>\n",
       "      <td>0.840879</td>\n",
       "      <td>0.058872</td>\n",
       "      <td>-0.583777</td>\n",
       "      <td>2.143237</td>\n",
       "      <td>-2.780310</td>\n",
       "      <td>...</td>\n",
       "      <td>2.784830</td>\n",
       "      <td>-1.216063</td>\n",
       "      <td>0.536372</td>\n",
       "      <td>1.549329</td>\n",
       "      <td>0.422412</td>\n",
       "      <td>1.053319</td>\n",
       "      <td>-0.103838</td>\n",
       "      <td>-0.202190</td>\n",
       "      <td>1.396888</td>\n",
       "      <td>1.078869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.245275</td>\n",
       "      <td>0.300843</td>\n",
       "      <td>-2.382328</td>\n",
       "      <td>1.222822</td>\n",
       "      <td>-0.177228</td>\n",
       "      <td>0.983967</td>\n",
       "      <td>-1.678256</td>\n",
       "      <td>-1.546943</td>\n",
       "      <td>3.261077</td>\n",
       "      <td>1.420339</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.793432</td>\n",
       "      <td>-0.718838</td>\n",
       "      <td>-2.714966</td>\n",
       "      <td>1.618331</td>\n",
       "      <td>0.194571</td>\n",
       "      <td>1.895786</td>\n",
       "      <td>2.241811</td>\n",
       "      <td>0.201873</td>\n",
       "      <td>1.997298</td>\n",
       "      <td>-0.430924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.122997</td>\n",
       "      <td>-1.293750</td>\n",
       "      <td>-0.975367</td>\n",
       "      <td>0.537844</td>\n",
       "      <td>-0.520960</td>\n",
       "      <td>-1.415625</td>\n",
       "      <td>-0.855277</td>\n",
       "      <td>1.271568</td>\n",
       "      <td>1.676777</td>\n",
       "      <td>-1.755009</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.784371</td>\n",
       "      <td>-2.440114</td>\n",
       "      <td>-2.797911</td>\n",
       "      <td>-0.128556</td>\n",
       "      <td>-2.770728</td>\n",
       "      <td>2.119603</td>\n",
       "      <td>1.360482</td>\n",
       "      <td>-2.457953</td>\n",
       "      <td>-1.062162</td>\n",
       "      <td>-2.741693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.550296</td>\n",
       "      <td>-1.667787</td>\n",
       "      <td>0.223213</td>\n",
       "      <td>3.390149</td>\n",
       "      <td>-1.793954</td>\n",
       "      <td>0.412991</td>\n",
       "      <td>-0.152919</td>\n",
       "      <td>-1.052962</td>\n",
       "      <td>2.211414</td>\n",
       "      <td>-1.194888</td>\n",
       "      <td>...</td>\n",
       "      <td>1.901253</td>\n",
       "      <td>-2.034873</td>\n",
       "      <td>-0.818937</td>\n",
       "      <td>0.781639</td>\n",
       "      <td>0.916908</td>\n",
       "      <td>-0.670265</td>\n",
       "      <td>1.301549</td>\n",
       "      <td>2.764951</td>\n",
       "      <td>-0.638750</td>\n",
       "      <td>-0.847817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.443144</td>\n",
       "      <td>0.293532</td>\n",
       "      <td>1.455991</td>\n",
       "      <td>2.803993</td>\n",
       "      <td>-1.365505</td>\n",
       "      <td>-0.416239</td>\n",
       "      <td>-0.700274</td>\n",
       "      <td>0.789046</td>\n",
       "      <td>-0.975508</td>\n",
       "      <td>0.114520</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026396</td>\n",
       "      <td>-1.522424</td>\n",
       "      <td>-1.882224</td>\n",
       "      <td>1.063282</td>\n",
       "      <td>2.253901</td>\n",
       "      <td>-1.109699</td>\n",
       "      <td>-0.962529</td>\n",
       "      <td>-1.582086</td>\n",
       "      <td>2.103916</td>\n",
       "      <td>1.074912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.498578</td>\n",
       "      <td>0.056829</td>\n",
       "      <td>-1.068713</td>\n",
       "      <td>-2.169666</td>\n",
       "      <td>0.849271</td>\n",
       "      <td>-0.251622</td>\n",
       "      <td>2.529812</td>\n",
       "      <td>0.999371</td>\n",
       "      <td>1.024537</td>\n",
       "      <td>-1.608983</td>\n",
       "      <td>...</td>\n",
       "      <td>1.424545</td>\n",
       "      <td>1.974907</td>\n",
       "      <td>2.118568</td>\n",
       "      <td>-0.293720</td>\n",
       "      <td>1.209372</td>\n",
       "      <td>1.930620</td>\n",
       "      <td>-1.079120</td>\n",
       "      <td>1.943943</td>\n",
       "      <td>-1.098946</td>\n",
       "      <td>2.653083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.730551</td>\n",
       "      <td>1.821098</td>\n",
       "      <td>-2.111818</td>\n",
       "      <td>1.566589</td>\n",
       "      <td>-2.150423</td>\n",
       "      <td>-1.291118</td>\n",
       "      <td>0.691757</td>\n",
       "      <td>2.972403</td>\n",
       "      <td>-2.395458</td>\n",
       "      <td>1.437602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294359</td>\n",
       "      <td>-1.346198</td>\n",
       "      <td>0.705346</td>\n",
       "      <td>-2.377261</td>\n",
       "      <td>0.611575</td>\n",
       "      <td>-0.368537</td>\n",
       "      <td>0.541286</td>\n",
       "      <td>-2.889394</td>\n",
       "      <td>-1.277738</td>\n",
       "      <td>-2.056166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.120776</td>\n",
       "      <td>0.035422</td>\n",
       "      <td>-0.714570</td>\n",
       "      <td>2.162347</td>\n",
       "      <td>-1.125672</td>\n",
       "      <td>1.303599</td>\n",
       "      <td>1.846936</td>\n",
       "      <td>-1.026972</td>\n",
       "      <td>-1.260720</td>\n",
       "      <td>2.340878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.284803</td>\n",
       "      <td>-2.004086</td>\n",
       "      <td>-2.195062</td>\n",
       "      <td>-0.536993</td>\n",
       "      <td>-0.482016</td>\n",
       "      <td>2.382937</td>\n",
       "      <td>1.128931</td>\n",
       "      <td>1.873874</td>\n",
       "      <td>2.082211</td>\n",
       "      <td>-1.156228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.196361</td>\n",
       "      <td>0.676729</td>\n",
       "      <td>-1.514749</td>\n",
       "      <td>3.089247</td>\n",
       "      <td>-3.212932</td>\n",
       "      <td>-0.731547</td>\n",
       "      <td>-1.358505</td>\n",
       "      <td>2.961115</td>\n",
       "      <td>-1.912404</td>\n",
       "      <td>0.282804</td>\n",
       "      <td>...</td>\n",
       "      <td>2.085512</td>\n",
       "      <td>0.150370</td>\n",
       "      <td>-0.953595</td>\n",
       "      <td>-1.186476</td>\n",
       "      <td>1.730094</td>\n",
       "      <td>0.408279</td>\n",
       "      <td>-0.670922</td>\n",
       "      <td>0.892726</td>\n",
       "      <td>0.420487</td>\n",
       "      <td>0.312339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.044580</td>\n",
       "      <td>1.196031</td>\n",
       "      <td>0.392948</td>\n",
       "      <td>1.971966</td>\n",
       "      <td>0.104099</td>\n",
       "      <td>3.497200</td>\n",
       "      <td>1.850080</td>\n",
       "      <td>2.316787</td>\n",
       "      <td>0.568782</td>\n",
       "      <td>-0.942415</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.081369</td>\n",
       "      <td>-0.546244</td>\n",
       "      <td>0.459855</td>\n",
       "      <td>-1.428629</td>\n",
       "      <td>1.911027</td>\n",
       "      <td>0.464544</td>\n",
       "      <td>2.969005</td>\n",
       "      <td>1.231313</td>\n",
       "      <td>-2.010177</td>\n",
       "      <td>-1.037753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.858882</td>\n",
       "      <td>1.940481</td>\n",
       "      <td>-1.389707</td>\n",
       "      <td>0.173122</td>\n",
       "      <td>1.996055</td>\n",
       "      <td>-2.295202</td>\n",
       "      <td>-0.692892</td>\n",
       "      <td>-1.415835</td>\n",
       "      <td>-0.350687</td>\n",
       "      <td>-1.544771</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.237505</td>\n",
       "      <td>-1.500020</td>\n",
       "      <td>2.816105</td>\n",
       "      <td>0.044149</td>\n",
       "      <td>-1.780366</td>\n",
       "      <td>0.791416</td>\n",
       "      <td>0.486390</td>\n",
       "      <td>-2.997272</td>\n",
       "      <td>-1.367625</td>\n",
       "      <td>0.356710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.465762</td>\n",
       "      <td>-0.770028</td>\n",
       "      <td>2.277954</td>\n",
       "      <td>3.203862</td>\n",
       "      <td>1.573081</td>\n",
       "      <td>-1.457499</td>\n",
       "      <td>-0.123389</td>\n",
       "      <td>0.058298</td>\n",
       "      <td>0.962228</td>\n",
       "      <td>1.272375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.664413</td>\n",
       "      <td>1.068482</td>\n",
       "      <td>1.915859</td>\n",
       "      <td>1.832015</td>\n",
       "      <td>0.380164</td>\n",
       "      <td>-1.696734</td>\n",
       "      <td>1.065792</td>\n",
       "      <td>-0.242615</td>\n",
       "      <td>0.189778</td>\n",
       "      <td>1.308599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.359012</td>\n",
       "      <td>1.391064</td>\n",
       "      <td>1.050687</td>\n",
       "      <td>-0.548445</td>\n",
       "      <td>-1.564474</td>\n",
       "      <td>-1.997897</td>\n",
       "      <td>-1.987910</td>\n",
       "      <td>-0.687101</td>\n",
       "      <td>2.786352</td>\n",
       "      <td>1.395608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>-1.658274</td>\n",
       "      <td>0.159214</td>\n",
       "      <td>1.446121</td>\n",
       "      <td>-3.067762</td>\n",
       "      <td>-0.453153</td>\n",
       "      <td>2.003506</td>\n",
       "      <td>0.535796</td>\n",
       "      <td>-2.195817</td>\n",
       "      <td>-0.609175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.205715</td>\n",
       "      <td>2.258283</td>\n",
       "      <td>-2.239023</td>\n",
       "      <td>0.441075</td>\n",
       "      <td>0.816423</td>\n",
       "      <td>-1.368810</td>\n",
       "      <td>0.795632</td>\n",
       "      <td>0.710915</td>\n",
       "      <td>0.416584</td>\n",
       "      <td>-0.050015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813826</td>\n",
       "      <td>-0.130219</td>\n",
       "      <td>0.076235</td>\n",
       "      <td>-1.441617</td>\n",
       "      <td>-1.274551</td>\n",
       "      <td>1.309576</td>\n",
       "      <td>2.189999</td>\n",
       "      <td>-1.001213</td>\n",
       "      <td>2.601167</td>\n",
       "      <td>0.222268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.540828</td>\n",
       "      <td>2.342333</td>\n",
       "      <td>1.199874</td>\n",
       "      <td>-0.771240</td>\n",
       "      <td>1.731433</td>\n",
       "      <td>1.356663</td>\n",
       "      <td>-0.106313</td>\n",
       "      <td>-1.381462</td>\n",
       "      <td>-0.253902</td>\n",
       "      <td>0.069172</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.371445</td>\n",
       "      <td>-0.718862</td>\n",
       "      <td>0.357398</td>\n",
       "      <td>-0.419320</td>\n",
       "      <td>-1.684707</td>\n",
       "      <td>-2.089788</td>\n",
       "      <td>-2.535735</td>\n",
       "      <td>-0.864661</td>\n",
       "      <td>0.438038</td>\n",
       "      <td>1.286815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.381326</td>\n",
       "      <td>1.032832</td>\n",
       "      <td>-0.922544</td>\n",
       "      <td>-2.291042</td>\n",
       "      <td>2.275565</td>\n",
       "      <td>3.520229</td>\n",
       "      <td>1.779079</td>\n",
       "      <td>0.567841</td>\n",
       "      <td>-1.392370</td>\n",
       "      <td>-0.386074</td>\n",
       "      <td>...</td>\n",
       "      <td>2.044211</td>\n",
       "      <td>1.909560</td>\n",
       "      <td>2.011679</td>\n",
       "      <td>0.700221</td>\n",
       "      <td>0.653066</td>\n",
       "      <td>-0.616933</td>\n",
       "      <td>-0.387280</td>\n",
       "      <td>1.042991</td>\n",
       "      <td>-0.765159</td>\n",
       "      <td>1.768036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.547806</td>\n",
       "      <td>1.392127</td>\n",
       "      <td>-1.658029</td>\n",
       "      <td>1.895423</td>\n",
       "      <td>-1.879749</td>\n",
       "      <td>0.045843</td>\n",
       "      <td>1.161495</td>\n",
       "      <td>0.665809</td>\n",
       "      <td>0.622788</td>\n",
       "      <td>2.163539</td>\n",
       "      <td>...</td>\n",
       "      <td>2.075864</td>\n",
       "      <td>1.078690</td>\n",
       "      <td>-0.803313</td>\n",
       "      <td>-1.860701</td>\n",
       "      <td>1.020282</td>\n",
       "      <td>-0.224923</td>\n",
       "      <td>1.053827</td>\n",
       "      <td>-0.383650</td>\n",
       "      <td>-1.357260</td>\n",
       "      <td>0.291406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.616354</td>\n",
       "      <td>-0.421650</td>\n",
       "      <td>1.843947</td>\n",
       "      <td>0.607560</td>\n",
       "      <td>2.870280</td>\n",
       "      <td>0.493006</td>\n",
       "      <td>-1.132888</td>\n",
       "      <td>1.840303</td>\n",
       "      <td>-2.651301</td>\n",
       "      <td>1.740929</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.765910</td>\n",
       "      <td>0.590166</td>\n",
       "      <td>1.492783</td>\n",
       "      <td>-0.467719</td>\n",
       "      <td>2.655117</td>\n",
       "      <td>0.204340</td>\n",
       "      <td>-2.119135</td>\n",
       "      <td>-0.179999</td>\n",
       "      <td>-0.850549</td>\n",
       "      <td>2.702554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.636150</td>\n",
       "      <td>-0.850488</td>\n",
       "      <td>1.131212</td>\n",
       "      <td>-0.953202</td>\n",
       "      <td>1.113983</td>\n",
       "      <td>1.543028</td>\n",
       "      <td>-0.538776</td>\n",
       "      <td>-1.628742</td>\n",
       "      <td>-3.114527</td>\n",
       "      <td>-2.853863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636810</td>\n",
       "      <td>-1.085599</td>\n",
       "      <td>2.515739</td>\n",
       "      <td>-0.777421</td>\n",
       "      <td>-0.286711</td>\n",
       "      <td>-1.128219</td>\n",
       "      <td>-2.984562</td>\n",
       "      <td>-1.614320</td>\n",
       "      <td>1.520204</td>\n",
       "      <td>-0.489206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.581503</td>\n",
       "      <td>1.073954</td>\n",
       "      <td>1.970564</td>\n",
       "      <td>3.174562</td>\n",
       "      <td>0.789634</td>\n",
       "      <td>2.344677</td>\n",
       "      <td>-2.439322</td>\n",
       "      <td>-0.788925</td>\n",
       "      <td>-1.004254</td>\n",
       "      <td>2.075066</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.705513</td>\n",
       "      <td>-1.292469</td>\n",
       "      <td>2.428370</td>\n",
       "      <td>0.097358</td>\n",
       "      <td>-0.179477</td>\n",
       "      <td>-1.608789</td>\n",
       "      <td>-2.690409</td>\n",
       "      <td>-1.282900</td>\n",
       "      <td>1.645668</td>\n",
       "      <td>-2.540887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.840544</td>\n",
       "      <td>0.041722</td>\n",
       "      <td>-1.146934</td>\n",
       "      <td>2.813190</td>\n",
       "      <td>1.293851</td>\n",
       "      <td>0.235821</td>\n",
       "      <td>2.644681</td>\n",
       "      <td>0.964099</td>\n",
       "      <td>1.007796</td>\n",
       "      <td>-1.234705</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.155873</td>\n",
       "      <td>-0.654301</td>\n",
       "      <td>1.785598</td>\n",
       "      <td>-0.359354</td>\n",
       "      <td>-0.574774</td>\n",
       "      <td>-0.553436</td>\n",
       "      <td>-0.684024</td>\n",
       "      <td>-0.120790</td>\n",
       "      <td>2.618229</td>\n",
       "      <td>-0.411936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-1.488068</td>\n",
       "      <td>0.206263</td>\n",
       "      <td>0.755815</td>\n",
       "      <td>-2.369265</td>\n",
       "      <td>1.356240</td>\n",
       "      <td>0.365835</td>\n",
       "      <td>-2.000767</td>\n",
       "      <td>-1.305564</td>\n",
       "      <td>1.767195</td>\n",
       "      <td>-0.616018</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.463915</td>\n",
       "      <td>-0.360197</td>\n",
       "      <td>0.322392</td>\n",
       "      <td>0.993981</td>\n",
       "      <td>-0.060454</td>\n",
       "      <td>0.216134</td>\n",
       "      <td>-2.409540</td>\n",
       "      <td>-2.039349</td>\n",
       "      <td>0.877067</td>\n",
       "      <td>-1.242393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.826388</td>\n",
       "      <td>0.791473</td>\n",
       "      <td>1.047520</td>\n",
       "      <td>1.864849</td>\n",
       "      <td>-2.253810</td>\n",
       "      <td>1.373623</td>\n",
       "      <td>0.350820</td>\n",
       "      <td>0.543334</td>\n",
       "      <td>-0.655048</td>\n",
       "      <td>-1.212314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.840015</td>\n",
       "      <td>3.158090</td>\n",
       "      <td>-0.032486</td>\n",
       "      <td>-0.006277</td>\n",
       "      <td>1.561643</td>\n",
       "      <td>0.501326</td>\n",
       "      <td>0.825687</td>\n",
       "      <td>1.229238</td>\n",
       "      <td>1.029585</td>\n",
       "      <td>-1.106227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-2.253985</td>\n",
       "      <td>0.849512</td>\n",
       "      <td>-1.327602</td>\n",
       "      <td>-2.099950</td>\n",
       "      <td>-0.134250</td>\n",
       "      <td>-2.114557</td>\n",
       "      <td>-0.637801</td>\n",
       "      <td>-1.357370</td>\n",
       "      <td>-1.278521</td>\n",
       "      <td>1.840667</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.842359</td>\n",
       "      <td>-2.187818</td>\n",
       "      <td>-1.424749</td>\n",
       "      <td>0.538510</td>\n",
       "      <td>2.214858</td>\n",
       "      <td>0.216103</td>\n",
       "      <td>2.214677</td>\n",
       "      <td>-2.343302</td>\n",
       "      <td>1.420371</td>\n",
       "      <td>1.544581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.520257</td>\n",
       "      <td>-1.159475</td>\n",
       "      <td>-0.217890</td>\n",
       "      <td>-1.853503</td>\n",
       "      <td>1.780377</td>\n",
       "      <td>2.065596</td>\n",
       "      <td>-0.790216</td>\n",
       "      <td>-1.899966</td>\n",
       "      <td>1.552720</td>\n",
       "      <td>2.614528</td>\n",
       "      <td>...</td>\n",
       "      <td>2.404394</td>\n",
       "      <td>-2.660806</td>\n",
       "      <td>-0.886755</td>\n",
       "      <td>0.124420</td>\n",
       "      <td>-0.681452</td>\n",
       "      <td>0.854649</td>\n",
       "      <td>0.617738</td>\n",
       "      <td>0.261883</td>\n",
       "      <td>-1.793564</td>\n",
       "      <td>0.921635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-2.042477</td>\n",
       "      <td>0.801739</td>\n",
       "      <td>-1.175841</td>\n",
       "      <td>-2.109541</td>\n",
       "      <td>1.361980</td>\n",
       "      <td>2.829167</td>\n",
       "      <td>1.532165</td>\n",
       "      <td>-1.033845</td>\n",
       "      <td>2.337936</td>\n",
       "      <td>-2.535114</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.383539</td>\n",
       "      <td>-0.024795</td>\n",
       "      <td>1.741685</td>\n",
       "      <td>1.310254</td>\n",
       "      <td>0.911855</td>\n",
       "      <td>0.513116</td>\n",
       "      <td>0.061037</td>\n",
       "      <td>1.281103</td>\n",
       "      <td>-1.104223</td>\n",
       "      <td>0.947006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-1.711991</td>\n",
       "      <td>-3.086671</td>\n",
       "      <td>-2.192298</td>\n",
       "      <td>-0.514017</td>\n",
       "      <td>0.446342</td>\n",
       "      <td>-1.834618</td>\n",
       "      <td>-1.820794</td>\n",
       "      <td>-1.399414</td>\n",
       "      <td>-2.353735</td>\n",
       "      <td>-1.186887</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.667855</td>\n",
       "      <td>-1.290839</td>\n",
       "      <td>2.081406</td>\n",
       "      <td>2.010641</td>\n",
       "      <td>1.107637</td>\n",
       "      <td>1.053138</td>\n",
       "      <td>-0.851609</td>\n",
       "      <td>-3.062452</td>\n",
       "      <td>-2.796665</td>\n",
       "      <td>0.747867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2.338915</td>\n",
       "      <td>-0.830763</td>\n",
       "      <td>-2.686939</td>\n",
       "      <td>-2.487080</td>\n",
       "      <td>-0.503993</td>\n",
       "      <td>0.998351</td>\n",
       "      <td>-2.701973</td>\n",
       "      <td>-1.869540</td>\n",
       "      <td>2.868066</td>\n",
       "      <td>-1.034177</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.090401</td>\n",
       "      <td>-1.751357</td>\n",
       "      <td>1.837686</td>\n",
       "      <td>-0.858548</td>\n",
       "      <td>-1.043265</td>\n",
       "      <td>2.038017</td>\n",
       "      <td>0.501092</td>\n",
       "      <td>-1.758894</td>\n",
       "      <td>2.070997</td>\n",
       "      <td>-2.304055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.490629</td>\n",
       "      <td>-2.477206</td>\n",
       "      <td>2.157042</td>\n",
       "      <td>3.358436</td>\n",
       "      <td>2.268159</td>\n",
       "      <td>1.814564</td>\n",
       "      <td>1.138076</td>\n",
       "      <td>-0.238324</td>\n",
       "      <td>1.415738</td>\n",
       "      <td>-1.607291</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.951028</td>\n",
       "      <td>2.526382</td>\n",
       "      <td>1.514231</td>\n",
       "      <td>-2.172680</td>\n",
       "      <td>1.372450</td>\n",
       "      <td>-1.165728</td>\n",
       "      <td>-0.696423</td>\n",
       "      <td>-0.593812</td>\n",
       "      <td>-0.458076</td>\n",
       "      <td>2.303037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.660435</td>\n",
       "      <td>-0.105651</td>\n",
       "      <td>-2.110337</td>\n",
       "      <td>-0.307958</td>\n",
       "      <td>0.941098</td>\n",
       "      <td>-1.287572</td>\n",
       "      <td>-0.465075</td>\n",
       "      <td>0.944679</td>\n",
       "      <td>-2.361664</td>\n",
       "      <td>-0.918949</td>\n",
       "      <td>...</td>\n",
       "      <td>1.206135</td>\n",
       "      <td>2.083416</td>\n",
       "      <td>-2.383151</td>\n",
       "      <td>-2.605177</td>\n",
       "      <td>-2.349266</td>\n",
       "      <td>-0.145792</td>\n",
       "      <td>0.182895</td>\n",
       "      <td>-2.931438</td>\n",
       "      <td>-0.976986</td>\n",
       "      <td>0.031077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2.118795</td>\n",
       "      <td>-2.484672</td>\n",
       "      <td>0.937892</td>\n",
       "      <td>0.279473</td>\n",
       "      <td>0.149813</td>\n",
       "      <td>1.881645</td>\n",
       "      <td>-0.588553</td>\n",
       "      <td>-0.811973</td>\n",
       "      <td>-0.760386</td>\n",
       "      <td>-0.908837</td>\n",
       "      <td>...</td>\n",
       "      <td>2.610909</td>\n",
       "      <td>-1.546528</td>\n",
       "      <td>0.864218</td>\n",
       "      <td>-2.026262</td>\n",
       "      <td>-1.988901</td>\n",
       "      <td>1.224517</td>\n",
       "      <td>-1.446732</td>\n",
       "      <td>1.823060</td>\n",
       "      <td>-1.792182</td>\n",
       "      <td>0.488375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.672624</td>\n",
       "      <td>1.726688</td>\n",
       "      <td>2.459166</td>\n",
       "      <td>0.102522</td>\n",
       "      <td>0.255259</td>\n",
       "      <td>1.092900</td>\n",
       "      <td>2.276226</td>\n",
       "      <td>0.983024</td>\n",
       "      <td>-1.436564</td>\n",
       "      <td>-1.376451</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.754334</td>\n",
       "      <td>-0.988029</td>\n",
       "      <td>-1.111593</td>\n",
       "      <td>-1.063920</td>\n",
       "      <td>2.164549</td>\n",
       "      <td>-0.572885</td>\n",
       "      <td>1.945136</td>\n",
       "      <td>2.579797</td>\n",
       "      <td>0.725779</td>\n",
       "      <td>2.394920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-1.162858</td>\n",
       "      <td>0.564682</td>\n",
       "      <td>1.415858</td>\n",
       "      <td>-1.008932</td>\n",
       "      <td>-2.021793</td>\n",
       "      <td>0.214441</td>\n",
       "      <td>0.022994</td>\n",
       "      <td>1.036017</td>\n",
       "      <td>-0.235716</td>\n",
       "      <td>2.589316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.823135</td>\n",
       "      <td>0.148328</td>\n",
       "      <td>1.211387</td>\n",
       "      <td>1.378476</td>\n",
       "      <td>0.766805</td>\n",
       "      <td>2.852576</td>\n",
       "      <td>1.424371</td>\n",
       "      <td>1.370872</td>\n",
       "      <td>-1.628567</td>\n",
       "      <td>2.394736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2.485361</td>\n",
       "      <td>-2.653741</td>\n",
       "      <td>-2.501678</td>\n",
       "      <td>-0.010577</td>\n",
       "      <td>-1.290545</td>\n",
       "      <td>-1.610488</td>\n",
       "      <td>1.877021</td>\n",
       "      <td>0.703051</td>\n",
       "      <td>-0.347794</td>\n",
       "      <td>1.991476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328659</td>\n",
       "      <td>-2.637326</td>\n",
       "      <td>-0.845121</td>\n",
       "      <td>0.793707</td>\n",
       "      <td>0.998038</td>\n",
       "      <td>0.632303</td>\n",
       "      <td>1.169078</td>\n",
       "      <td>2.383529</td>\n",
       "      <td>-2.170013</td>\n",
       "      <td>-2.237979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.826477</td>\n",
       "      <td>0.098994</td>\n",
       "      <td>-1.003280</td>\n",
       "      <td>0.662819</td>\n",
       "      <td>2.552633</td>\n",
       "      <td>-1.420308</td>\n",
       "      <td>0.792076</td>\n",
       "      <td>-1.291704</td>\n",
       "      <td>2.738754</td>\n",
       "      <td>1.708732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802370</td>\n",
       "      <td>-0.202991</td>\n",
       "      <td>1.306362</td>\n",
       "      <td>-0.556450</td>\n",
       "      <td>2.406199</td>\n",
       "      <td>-1.328537</td>\n",
       "      <td>-0.317835</td>\n",
       "      <td>-1.771803</td>\n",
       "      <td>0.748841</td>\n",
       "      <td>0.989631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-1.237033</td>\n",
       "      <td>2.606594</td>\n",
       "      <td>-1.689142</td>\n",
       "      <td>0.350748</td>\n",
       "      <td>-2.125721</td>\n",
       "      <td>-0.446986</td>\n",
       "      <td>-2.737426</td>\n",
       "      <td>1.555543</td>\n",
       "      <td>1.234792</td>\n",
       "      <td>1.010278</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.706659</td>\n",
       "      <td>-0.546117</td>\n",
       "      <td>-0.552956</td>\n",
       "      <td>-1.087226</td>\n",
       "      <td>-1.095044</td>\n",
       "      <td>2.328578</td>\n",
       "      <td>-0.923442</td>\n",
       "      <td>1.952542</td>\n",
       "      <td>-2.542175</td>\n",
       "      <td>-0.247947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.092323</td>\n",
       "      <td>1.572649</td>\n",
       "      <td>-2.455369</td>\n",
       "      <td>-0.852549</td>\n",
       "      <td>1.341565</td>\n",
       "      <td>-0.989404</td>\n",
       "      <td>0.351934</td>\n",
       "      <td>1.054313</td>\n",
       "      <td>0.539021</td>\n",
       "      <td>1.170020</td>\n",
       "      <td>...</td>\n",
       "      <td>1.876797</td>\n",
       "      <td>-0.611365</td>\n",
       "      <td>-1.733052</td>\n",
       "      <td>3.074629</td>\n",
       "      <td>1.496918</td>\n",
       "      <td>-1.272919</td>\n",
       "      <td>-0.557683</td>\n",
       "      <td>0.344344</td>\n",
       "      <td>0.331202</td>\n",
       "      <td>0.816362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2.072744</td>\n",
       "      <td>2.124130</td>\n",
       "      <td>1.572455</td>\n",
       "      <td>-1.325779</td>\n",
       "      <td>-0.474365</td>\n",
       "      <td>-0.685275</td>\n",
       "      <td>1.517998</td>\n",
       "      <td>0.081256</td>\n",
       "      <td>2.337808</td>\n",
       "      <td>1.051909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074013</td>\n",
       "      <td>-0.818292</td>\n",
       "      <td>-0.174248</td>\n",
       "      <td>-0.471047</td>\n",
       "      <td>0.131862</td>\n",
       "      <td>-1.069362</td>\n",
       "      <td>2.009211</td>\n",
       "      <td>0.202034</td>\n",
       "      <td>-2.129408</td>\n",
       "      <td>1.769070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.548068</td>\n",
       "      <td>0.494560</td>\n",
       "      <td>-2.058122</td>\n",
       "      <td>-0.678363</td>\n",
       "      <td>1.807228</td>\n",
       "      <td>0.103154</td>\n",
       "      <td>0.989662</td>\n",
       "      <td>-1.907886</td>\n",
       "      <td>1.491675</td>\n",
       "      <td>0.515306</td>\n",
       "      <td>...</td>\n",
       "      <td>2.146713</td>\n",
       "      <td>1.261631</td>\n",
       "      <td>-0.213034</td>\n",
       "      <td>-0.893540</td>\n",
       "      <td>-0.738467</td>\n",
       "      <td>-0.229848</td>\n",
       "      <td>-0.959929</td>\n",
       "      <td>1.698625</td>\n",
       "      <td>1.406340</td>\n",
       "      <td>2.605697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.928819</td>\n",
       "      <td>1.394164</td>\n",
       "      <td>0.401236</td>\n",
       "      <td>-0.347056</td>\n",
       "      <td>0.157810</td>\n",
       "      <td>-1.391565</td>\n",
       "      <td>-1.174050</td>\n",
       "      <td>-1.116280</td>\n",
       "      <td>-1.531923</td>\n",
       "      <td>-1.756606</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.427249</td>\n",
       "      <td>-1.209755</td>\n",
       "      <td>-0.952695</td>\n",
       "      <td>0.261940</td>\n",
       "      <td>1.720220</td>\n",
       "      <td>-0.240599</td>\n",
       "      <td>0.765029</td>\n",
       "      <td>2.974128</td>\n",
       "      <td>1.579319</td>\n",
       "      <td>0.792532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.977549</td>\n",
       "      <td>2.485913</td>\n",
       "      <td>-0.543551</td>\n",
       "      <td>-0.291072</td>\n",
       "      <td>0.820829</td>\n",
       "      <td>1.889552</td>\n",
       "      <td>-0.611988</td>\n",
       "      <td>-0.063783</td>\n",
       "      <td>-1.236141</td>\n",
       "      <td>1.219350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152678</td>\n",
       "      <td>2.208021</td>\n",
       "      <td>1.016771</td>\n",
       "      <td>-1.745433</td>\n",
       "      <td>-0.480861</td>\n",
       "      <td>-0.735489</td>\n",
       "      <td>0.843561</td>\n",
       "      <td>-1.947332</td>\n",
       "      <td>-1.664727</td>\n",
       "      <td>0.844249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.030628</td>\n",
       "      <td>0.267319</td>\n",
       "      <td>0.422861</td>\n",
       "      <td>-2.355502</td>\n",
       "      <td>2.839832</td>\n",
       "      <td>1.840684</td>\n",
       "      <td>0.652698</td>\n",
       "      <td>-0.005724</td>\n",
       "      <td>2.351479</td>\n",
       "      <td>-0.164283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884620</td>\n",
       "      <td>-0.145220</td>\n",
       "      <td>-1.001622</td>\n",
       "      <td>-1.009034</td>\n",
       "      <td>-0.035440</td>\n",
       "      <td>0.972867</td>\n",
       "      <td>1.753937</td>\n",
       "      <td>0.749817</td>\n",
       "      <td>-1.137417</td>\n",
       "      <td>1.923116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-1.011933</td>\n",
       "      <td>2.583377</td>\n",
       "      <td>1.771884</td>\n",
       "      <td>-0.922667</td>\n",
       "      <td>-2.048273</td>\n",
       "      <td>1.440448</td>\n",
       "      <td>-1.387520</td>\n",
       "      <td>-1.531619</td>\n",
       "      <td>-0.788424</td>\n",
       "      <td>1.805533</td>\n",
       "      <td>...</td>\n",
       "      <td>3.339606</td>\n",
       "      <td>0.089080</td>\n",
       "      <td>-2.426756</td>\n",
       "      <td>-2.265483</td>\n",
       "      <td>1.178711</td>\n",
       "      <td>-1.118676</td>\n",
       "      <td>-0.607919</td>\n",
       "      <td>-2.808583</td>\n",
       "      <td>0.530645</td>\n",
       "      <td>0.819920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.663075</td>\n",
       "      <td>1.455083</td>\n",
       "      <td>-0.567837</td>\n",
       "      <td>0.577716</td>\n",
       "      <td>0.561600</td>\n",
       "      <td>1.706991</td>\n",
       "      <td>0.380459</td>\n",
       "      <td>-1.487576</td>\n",
       "      <td>-1.963895</td>\n",
       "      <td>1.465738</td>\n",
       "      <td>...</td>\n",
       "      <td>2.060556</td>\n",
       "      <td>-0.533051</td>\n",
       "      <td>2.481429</td>\n",
       "      <td>1.396675</td>\n",
       "      <td>-0.019388</td>\n",
       "      <td>0.654019</td>\n",
       "      <td>1.848939</td>\n",
       "      <td>-0.319920</td>\n",
       "      <td>1.505835</td>\n",
       "      <td>0.827340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-0.065728</td>\n",
       "      <td>-1.996908</td>\n",
       "      <td>1.875569</td>\n",
       "      <td>1.824977</td>\n",
       "      <td>-1.183075</td>\n",
       "      <td>1.064878</td>\n",
       "      <td>-0.401157</td>\n",
       "      <td>-1.892815</td>\n",
       "      <td>0.155309</td>\n",
       "      <td>0.077738</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.235875</td>\n",
       "      <td>3.085191</td>\n",
       "      <td>-1.201283</td>\n",
       "      <td>-0.834833</td>\n",
       "      <td>-0.048020</td>\n",
       "      <td>2.404510</td>\n",
       "      <td>1.856946</td>\n",
       "      <td>-1.666458</td>\n",
       "      <td>0.585333</td>\n",
       "      <td>-1.707573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-1.734321</td>\n",
       "      <td>-1.401498</td>\n",
       "      <td>2.121152</td>\n",
       "      <td>-0.732239</td>\n",
       "      <td>1.768268</td>\n",
       "      <td>-0.386217</td>\n",
       "      <td>1.296324</td>\n",
       "      <td>-0.437929</td>\n",
       "      <td>-1.056304</td>\n",
       "      <td>-0.528242</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086710</td>\n",
       "      <td>1.820531</td>\n",
       "      <td>0.423824</td>\n",
       "      <td>-1.830103</td>\n",
       "      <td>1.426650</td>\n",
       "      <td>1.091330</td>\n",
       "      <td>0.490082</td>\n",
       "      <td>0.407764</td>\n",
       "      <td>-0.477460</td>\n",
       "      <td>-1.292083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.687612</td>\n",
       "      <td>0.049426</td>\n",
       "      <td>1.982613</td>\n",
       "      <td>0.331171</td>\n",
       "      <td>-1.966438</td>\n",
       "      <td>-1.390407</td>\n",
       "      <td>-2.247402</td>\n",
       "      <td>-0.486000</td>\n",
       "      <td>-0.401555</td>\n",
       "      <td>1.828628</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.450472</td>\n",
       "      <td>0.853021</td>\n",
       "      <td>-1.542803</td>\n",
       "      <td>-2.437380</td>\n",
       "      <td>-0.744556</td>\n",
       "      <td>1.647763</td>\n",
       "      <td>-0.995472</td>\n",
       "      <td>-2.595721</td>\n",
       "      <td>-1.205965</td>\n",
       "      <td>-1.037440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.411429</td>\n",
       "      <td>-2.717397</td>\n",
       "      <td>-2.398757</td>\n",
       "      <td>-2.361262</td>\n",
       "      <td>-2.158541</td>\n",
       "      <td>2.705843</td>\n",
       "      <td>0.583887</td>\n",
       "      <td>-0.182940</td>\n",
       "      <td>-0.331152</td>\n",
       "      <td>-2.353216</td>\n",
       "      <td>...</td>\n",
       "      <td>2.279609</td>\n",
       "      <td>3.202481</td>\n",
       "      <td>-0.828873</td>\n",
       "      <td>0.337966</td>\n",
       "      <td>2.592664</td>\n",
       "      <td>-1.624571</td>\n",
       "      <td>0.667841</td>\n",
       "      <td>-0.564916</td>\n",
       "      <td>0.708520</td>\n",
       "      <td>-1.879944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.841468</td>\n",
       "      <td>1.121284</td>\n",
       "      <td>3.006881</td>\n",
       "      <td>1.331355</td>\n",
       "      <td>2.567476</td>\n",
       "      <td>-2.531555</td>\n",
       "      <td>0.042319</td>\n",
       "      <td>-0.473796</td>\n",
       "      <td>-1.593654</td>\n",
       "      <td>2.684292</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.801368</td>\n",
       "      <td>0.831723</td>\n",
       "      <td>-1.513940</td>\n",
       "      <td>1.476962</td>\n",
       "      <td>-0.362147</td>\n",
       "      <td>1.659358</td>\n",
       "      <td>-0.580301</td>\n",
       "      <td>-1.157847</td>\n",
       "      <td>-1.991232</td>\n",
       "      <td>-0.586784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  -0.600459  0.465072  2.163063 -0.565845 -1.660129 -0.039433 -2.017517   \n",
       "1   1.449250 -0.963133  0.122163  3.094393 -1.089969  0.146805 -0.838032   \n",
       "2   2.936718  0.530892  0.156811  2.590857  0.366070  0.412720  0.389496   \n",
       "3  -1.297347 -0.527299 -1.314723 -1.911067 -0.576052 -1.177065  1.899386   \n",
       "4   1.897689  2.097216  3.001771 -0.100336 -0.914664 -0.842301 -1.916697   \n",
       "5   0.841477  0.686090  1.501176  1.392911  0.081821  1.222786  1.846111   \n",
       "6   0.674039  0.764513 -1.856403  0.981205 -1.359101 -1.876309  1.296131   \n",
       "7  -1.116215 -0.378341 -1.409316  2.403500  0.995584 -2.106048 -1.468643   \n",
       "8   0.198998  1.178543  2.076991 -0.657220  1.408336  1.744330 -1.337416   \n",
       "9   0.788301 -0.739811 -2.907268  0.734854  1.072244 -0.231931  2.607131   \n",
       "10 -1.504805 -0.838070  1.868727  1.585570  1.473725 -0.340161  1.262059   \n",
       "11 -0.799095 -1.811295  0.932625 -1.164581  0.020809  0.840879  0.058872   \n",
       "12 -1.245275  0.300843 -2.382328  1.222822 -0.177228  0.983967 -1.678256   \n",
       "13  1.122997 -1.293750 -0.975367  0.537844 -0.520960 -1.415625 -0.855277   \n",
       "14 -1.550296 -1.667787  0.223213  3.390149 -1.793954  0.412991 -0.152919   \n",
       "15  1.443144  0.293532  1.455991  2.803993 -1.365505 -0.416239 -0.700274   \n",
       "16  1.498578  0.056829 -1.068713 -2.169666  0.849271 -0.251622  2.529812   \n",
       "17  2.730551  1.821098 -2.111818  1.566589 -2.150423 -1.291118  0.691757   \n",
       "18 -0.120776  0.035422 -0.714570  2.162347 -1.125672  1.303599  1.846936   \n",
       "19 -0.196361  0.676729 -1.514749  3.089247 -3.212932 -0.731547 -1.358505   \n",
       "20 -0.044580  1.196031  0.392948  1.971966  0.104099  3.497200  1.850080   \n",
       "21 -0.858882  1.940481 -1.389707  0.173122  1.996055 -2.295202 -0.692892   \n",
       "22 -0.465762 -0.770028  2.277954  3.203862  1.573081 -1.457499 -0.123389   \n",
       "23  1.359012  1.391064  1.050687 -0.548445 -1.564474 -1.997897 -1.987910   \n",
       "24  2.205715  2.258283 -2.239023  0.441075  0.816423 -1.368810  0.795632   \n",
       "25 -0.540828  2.342333  1.199874 -0.771240  1.731433  1.356663 -0.106313   \n",
       "26 -0.381326  1.032832 -0.922544 -2.291042  2.275565  3.520229  1.779079   \n",
       "27 -0.547806  1.392127 -1.658029  1.895423 -1.879749  0.045843  1.161495   \n",
       "28  0.616354 -0.421650  1.843947  0.607560  2.870280  0.493006 -1.132888   \n",
       "29 -1.636150 -0.850488  1.131212 -0.953202  1.113983  1.543028 -0.538776   \n",
       "30  0.581503  1.073954  1.970564  3.174562  0.789634  2.344677 -2.439322   \n",
       "31  0.840544  0.041722 -1.146934  2.813190  1.293851  0.235821  2.644681   \n",
       "32 -1.488068  0.206263  0.755815 -2.369265  1.356240  0.365835 -2.000767   \n",
       "33  1.826388  0.791473  1.047520  1.864849 -2.253810  1.373623  0.350820   \n",
       "34 -2.253985  0.849512 -1.327602 -2.099950 -0.134250 -2.114557 -0.637801   \n",
       "35 -0.520257 -1.159475 -0.217890 -1.853503  1.780377  2.065596 -0.790216   \n",
       "36 -2.042477  0.801739 -1.175841 -2.109541  1.361980  2.829167  1.532165   \n",
       "37 -1.711991 -3.086671 -2.192298 -0.514017  0.446342 -1.834618 -1.820794   \n",
       "38  2.338915 -0.830763 -2.686939 -2.487080 -0.503993  0.998351 -2.701973   \n",
       "39 -0.490629 -2.477206  2.157042  3.358436  2.268159  1.814564  1.138076   \n",
       "40  1.660435 -0.105651 -2.110337 -0.307958  0.941098 -1.287572 -0.465075   \n",
       "41  2.118795 -2.484672  0.937892  0.279473  0.149813  1.881645 -0.588553   \n",
       "42  0.672624  1.726688  2.459166  0.102522  0.255259  1.092900  2.276226   \n",
       "43 -1.162858  0.564682  1.415858 -1.008932 -2.021793  0.214441  0.022994   \n",
       "44  2.485361 -2.653741 -2.501678 -0.010577 -1.290545 -1.610488  1.877021   \n",
       "45 -0.826477  0.098994 -1.003280  0.662819  2.552633 -1.420308  0.792076   \n",
       "46 -1.237033  2.606594 -1.689142  0.350748 -2.125721 -0.446986 -2.737426   \n",
       "47  1.092323  1.572649 -2.455369 -0.852549  1.341565 -0.989404  0.351934   \n",
       "48  2.072744  2.124130  1.572455 -1.325779 -0.474365 -0.685275  1.517998   \n",
       "49 -0.548068  0.494560 -2.058122 -0.678363  1.807228  0.103154  0.989662   \n",
       "50  0.928819  1.394164  0.401236 -0.347056  0.157810 -1.391565 -1.174050   \n",
       "51  1.977549  2.485913 -0.543551 -0.291072  0.820829  1.889552 -0.611988   \n",
       "52 -0.030628  0.267319  0.422861 -2.355502  2.839832  1.840684  0.652698   \n",
       "53 -1.011933  2.583377  1.771884 -0.922667 -2.048273  1.440448 -1.387520   \n",
       "54  1.663075  1.455083 -0.567837  0.577716  0.561600  1.706991  0.380459   \n",
       "55 -0.065728 -1.996908  1.875569  1.824977 -1.183075  1.064878 -0.401157   \n",
       "56 -1.734321 -1.401498  2.121152 -0.732239  1.768268 -0.386217  1.296324   \n",
       "57  0.687612  0.049426  1.982613  0.331171 -1.966438 -1.390407 -2.247402   \n",
       "58  0.411429 -2.717397 -2.398757 -2.361262 -2.158541  2.705843  0.583887   \n",
       "59  0.841468  1.121284  3.006881  1.331355  2.567476 -2.531555  0.042319   \n",
       "\n",
       "          V8        V9       V10    ...          V21       V22       V23  \\\n",
       "0   0.405838  0.940746  0.296212    ...     1.248405  0.751866 -0.118424   \n",
       "1  -2.348915  2.974649 -1.542879    ...     2.851528 -2.390328  1.043473   \n",
       "2  -0.173952 -1.402256  0.238771    ...    -0.894853  2.267772 -2.292738   \n",
       "3  -1.129185 -2.108203  1.491231    ...     0.005204  1.458786  2.790831   \n",
       "4  -1.935876 -0.346499 -1.618192    ...    -1.187718  2.269789 -0.561446   \n",
       "5  -1.645100 -3.172367  0.943527    ...    -1.424388 -0.472715  2.589546   \n",
       "6  -1.512398  0.680637  1.958164    ...    -0.498954 -2.600328 -1.657590   \n",
       "7  -1.745988 -2.734196 -1.678218    ...     2.343087  2.617907  0.780009   \n",
       "8  -0.849860 -1.894325  1.655279    ...     1.201754  1.786878  1.701576   \n",
       "9   1.455274  0.721918 -1.339980    ...     1.900118 -2.193805  0.871282   \n",
       "10 -1.681823  2.936888 -1.083564    ...     1.607450  2.338186 -1.204149   \n",
       "11 -0.583777  2.143237 -2.780310    ...     2.784830 -1.216063  0.536372   \n",
       "12 -1.546943  3.261077  1.420339    ...    -2.793432 -0.718838 -2.714966   \n",
       "13  1.271568  1.676777 -1.755009    ...    -1.784371 -2.440114 -2.797911   \n",
       "14 -1.052962  2.211414 -1.194888    ...     1.901253 -2.034873 -0.818937   \n",
       "15  0.789046 -0.975508  0.114520    ...    -0.026396 -1.522424 -1.882224   \n",
       "16  0.999371  1.024537 -1.608983    ...     1.424545  1.974907  2.118568   \n",
       "17  2.972403 -2.395458  1.437602    ...    -0.294359 -1.346198  0.705346   \n",
       "18 -1.026972 -1.260720  2.340878    ...    -0.284803 -2.004086 -2.195062   \n",
       "19  2.961115 -1.912404  0.282804    ...     2.085512  0.150370 -0.953595   \n",
       "20  2.316787  0.568782 -0.942415    ...    -1.081369 -0.546244  0.459855   \n",
       "21 -1.415835 -0.350687 -1.544771    ...    -2.237505 -1.500020  2.816105   \n",
       "22  0.058298  0.962228  1.272375    ...    -0.664413  1.068482  1.915859   \n",
       "23 -0.687101  2.786352  1.395608    ...     0.001163 -1.658274  0.159214   \n",
       "24  0.710915  0.416584 -0.050015    ...     0.813826 -0.130219  0.076235   \n",
       "25 -1.381462 -0.253902  0.069172    ...    -0.371445 -0.718862  0.357398   \n",
       "26  0.567841 -1.392370 -0.386074    ...     2.044211  1.909560  2.011679   \n",
       "27  0.665809  0.622788  2.163539    ...     2.075864  1.078690 -0.803313   \n",
       "28  1.840303 -2.651301  1.740929    ...    -0.765910  0.590166  1.492783   \n",
       "29 -1.628742 -3.114527 -2.853863    ...     0.636810 -1.085599  2.515739   \n",
       "30 -0.788925 -1.004254  2.075066    ...    -1.705513 -1.292469  2.428370   \n",
       "31  0.964099  1.007796 -1.234705    ...    -1.155873 -0.654301  1.785598   \n",
       "32 -1.305564  1.767195 -0.616018    ...    -1.463915 -0.360197  0.322392   \n",
       "33  0.543334 -0.655048 -1.212314    ...    -0.840015  3.158090 -0.032486   \n",
       "34 -1.357370 -1.278521  1.840667    ...    -2.842359 -2.187818 -1.424749   \n",
       "35 -1.899966  1.552720  2.614528    ...     2.404394 -2.660806 -0.886755   \n",
       "36 -1.033845  2.337936 -2.535114    ...    -2.383539 -0.024795  1.741685   \n",
       "37 -1.399414 -2.353735 -1.186887    ...    -1.667855 -1.290839  2.081406   \n",
       "38 -1.869540  2.868066 -1.034177    ...    -2.090401 -1.751357  1.837686   \n",
       "39 -0.238324  1.415738 -1.607291    ...    -0.951028  2.526382  1.514231   \n",
       "40  0.944679 -2.361664 -0.918949    ...     1.206135  2.083416 -2.383151   \n",
       "41 -0.811973 -0.760386 -0.908837    ...     2.610909 -1.546528  0.864218   \n",
       "42  0.983024 -1.436564 -1.376451    ...    -0.754334 -0.988029 -1.111593   \n",
       "43  1.036017 -0.235716  2.589316    ...    -0.823135  0.148328  1.211387   \n",
       "44  0.703051 -0.347794  1.991476    ...     0.328659 -2.637326 -0.845121   \n",
       "45 -1.291704  2.738754  1.708732    ...     0.802370 -0.202991  1.306362   \n",
       "46  1.555543  1.234792  1.010278    ...    -2.706659 -0.546117 -0.552956   \n",
       "47  1.054313  0.539021  1.170020    ...     1.876797 -0.611365 -1.733052   \n",
       "48  0.081256  2.337808  1.051909    ...     0.074013 -0.818292 -0.174248   \n",
       "49 -1.907886  1.491675  0.515306    ...     2.146713  1.261631 -0.213034   \n",
       "50 -1.116280 -1.531923 -1.756606    ...    -2.427249 -1.209755 -0.952695   \n",
       "51 -0.063783 -1.236141  1.219350    ...     0.152678  2.208021  1.016771   \n",
       "52 -0.005724  2.351479 -0.164283    ...     0.884620 -0.145220 -1.001622   \n",
       "53 -1.531619 -0.788424  1.805533    ...     3.339606  0.089080 -2.426756   \n",
       "54 -1.487576 -1.963895  1.465738    ...     2.060556 -0.533051  2.481429   \n",
       "55 -1.892815  0.155309  0.077738    ...    -0.235875  3.085191 -1.201283   \n",
       "56 -0.437929 -1.056304 -0.528242    ...    -0.086710  1.820531  0.423824   \n",
       "57 -0.486000 -0.401555  1.828628    ...    -2.450472  0.853021 -1.542803   \n",
       "58 -0.182940 -0.331152 -2.353216    ...     2.279609  3.202481 -0.828873   \n",
       "59 -0.473796 -1.593654  2.684292    ...    -1.801368  0.831723 -1.513940   \n",
       "\n",
       "         V24       V25       V26       V27       V28  normAmount     Class  \n",
       "0   0.945000 -0.524080 -2.590899 -0.145017  3.332857   -1.857148 -2.599469  \n",
       "1   1.997388 -1.397125 -2.826992  1.157478  1.132260   -0.694053 -0.698702  \n",
       "2  -0.419909  1.447357  2.251052  0.753249  0.521450   -0.223595 -1.614821  \n",
       "3   1.173577 -2.438058  0.585100  2.298208 -0.278277    2.336517  0.996671  \n",
       "4   0.192087 -0.198630  0.538794 -1.406878 -1.585132    1.868896  0.673201  \n",
       "5   2.207823  2.039515 -0.784403  0.298666  2.318167    2.968308  2.367115  \n",
       "6  -1.863385  1.834593 -1.034303  1.814876 -2.764538    0.865730  1.777931  \n",
       "7  -1.826149 -1.716911 -0.726991  1.703366  0.980359   -1.323486  0.878476  \n",
       "8   0.326690 -0.662766 -0.091425  1.531055 -0.704209   -0.658600  0.931092  \n",
       "9   2.278460 -1.408634  2.461220 -0.967964  1.467767    2.802646  2.020118  \n",
       "10  1.676125 -1.815302 -1.377590  0.210884  0.026440   -0.690522  0.271184  \n",
       "11  1.549329  0.422412  1.053319 -0.103838 -0.202190    1.396888  1.078869  \n",
       "12  1.618331  0.194571  1.895786  2.241811  0.201873    1.997298 -0.430924  \n",
       "13 -0.128556 -2.770728  2.119603  1.360482 -2.457953   -1.062162 -2.741693  \n",
       "14  0.781639  0.916908 -0.670265  1.301549  2.764951   -0.638750 -0.847817  \n",
       "15  1.063282  2.253901 -1.109699 -0.962529 -1.582086    2.103916  1.074912  \n",
       "16 -0.293720  1.209372  1.930620 -1.079120  1.943943   -1.098946  2.653083  \n",
       "17 -2.377261  0.611575 -0.368537  0.541286 -2.889394   -1.277738 -2.056166  \n",
       "18 -0.536993 -0.482016  2.382937  1.128931  1.873874    2.082211 -1.156228  \n",
       "19 -1.186476  1.730094  0.408279 -0.670922  0.892726    0.420487  0.312339  \n",
       "20 -1.428629  1.911027  0.464544  2.969005  1.231313   -2.010177 -1.037753  \n",
       "21  0.044149 -1.780366  0.791416  0.486390 -2.997272   -1.367625  0.356710  \n",
       "22  1.832015  0.380164 -1.696734  1.065792 -0.242615    0.189778  1.308599  \n",
       "23  1.446121 -3.067762 -0.453153  2.003506  0.535796   -2.195817 -0.609175  \n",
       "24 -1.441617 -1.274551  1.309576  2.189999 -1.001213    2.601167  0.222268  \n",
       "25 -0.419320 -1.684707 -2.089788 -2.535735 -0.864661    0.438038  1.286815  \n",
       "26  0.700221  0.653066 -0.616933 -0.387280  1.042991   -0.765159  1.768036  \n",
       "27 -1.860701  1.020282 -0.224923  1.053827 -0.383650   -1.357260  0.291406  \n",
       "28 -0.467719  2.655117  0.204340 -2.119135 -0.179999   -0.850549  2.702554  \n",
       "29 -0.777421 -0.286711 -1.128219 -2.984562 -1.614320    1.520204 -0.489206  \n",
       "30  0.097358 -0.179477 -1.608789 -2.690409 -1.282900    1.645668 -2.540887  \n",
       "31 -0.359354 -0.574774 -0.553436 -0.684024 -0.120790    2.618229 -0.411936  \n",
       "32  0.993981 -0.060454  0.216134 -2.409540 -2.039349    0.877067 -1.242393  \n",
       "33 -0.006277  1.561643  0.501326  0.825687  1.229238    1.029585 -1.106227  \n",
       "34  0.538510  2.214858  0.216103  2.214677 -2.343302    1.420371  1.544581  \n",
       "35  0.124420 -0.681452  0.854649  0.617738  0.261883   -1.793564  0.921635  \n",
       "36  1.310254  0.911855  0.513116  0.061037  1.281103   -1.104223  0.947006  \n",
       "37  2.010641  1.107637  1.053138 -0.851609 -3.062452   -2.796665  0.747867  \n",
       "38 -0.858548 -1.043265  2.038017  0.501092 -1.758894    2.070997 -2.304055  \n",
       "39 -2.172680  1.372450 -1.165728 -0.696423 -0.593812   -0.458076  2.303037  \n",
       "40 -2.605177 -2.349266 -0.145792  0.182895 -2.931438   -0.976986  0.031077  \n",
       "41 -2.026262 -1.988901  1.224517 -1.446732  1.823060   -1.792182  0.488375  \n",
       "42 -1.063920  2.164549 -0.572885  1.945136  2.579797    0.725779  2.394920  \n",
       "43  1.378476  0.766805  2.852576  1.424371  1.370872   -1.628567  2.394736  \n",
       "44  0.793707  0.998038  0.632303  1.169078  2.383529   -2.170013 -2.237979  \n",
       "45 -0.556450  2.406199 -1.328537 -0.317835 -1.771803    0.748841  0.989631  \n",
       "46 -1.087226 -1.095044  2.328578 -0.923442  1.952542   -2.542175 -0.247947  \n",
       "47  3.074629  1.496918 -1.272919 -0.557683  0.344344    0.331202  0.816362  \n",
       "48 -0.471047  0.131862 -1.069362  2.009211  0.202034   -2.129408  1.769070  \n",
       "49 -0.893540 -0.738467 -0.229848 -0.959929  1.698625    1.406340  2.605697  \n",
       "50  0.261940  1.720220 -0.240599  0.765029  2.974128    1.579319  0.792532  \n",
       "51 -1.745433 -0.480861 -0.735489  0.843561 -1.947332   -1.664727  0.844249  \n",
       "52 -1.009034 -0.035440  0.972867  1.753937  0.749817   -1.137417  1.923116  \n",
       "53 -2.265483  1.178711 -1.118676 -0.607919 -2.808583    0.530645  0.819920  \n",
       "54  1.396675 -0.019388  0.654019  1.848939 -0.319920    1.505835  0.827340  \n",
       "55 -0.834833 -0.048020  2.404510  1.856946 -1.666458    0.585333 -1.707573  \n",
       "56 -1.830103  1.426650  1.091330  0.490082  0.407764   -0.477460 -1.292083  \n",
       "57 -2.437380 -0.744556  1.647763 -0.995472 -2.595721   -1.205965 -1.037440  \n",
       "58  0.337966  2.592664 -1.624571  0.667841 -0.564916    0.708520 -1.879944  \n",
       "59  1.476962 -0.362147  1.659358 -0.580301 -1.157847   -1.991232 -0.586784  \n",
       "\n",
       "[60 rows x 30 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tensor to PD data frame\n",
    "#synthentic_data_df = pd.DataFrame(data=synthentic_data.data.numpy());\n",
    "synthentic_data_df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat old fraud data and newly generated fraud data\n",
    "new_data = pd.concat([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.8420,  -0.3655,  -2.4641,   4.8209,   0.7755,  -0.6148,\n",
       "           1.3680,  -0.5263,  -0.1214,  -0.3576,   0.6378,  -2.8189,\n",
       "           2.3606,  -1.1087,  -1.4500,   1.5737,   2.2027,   0.8005,\n",
       "          -2.1036,   0.9449,  -0.1106,  -1.2578,  -0.3244,  -0.4200,\n",
       "          -0.2195,  -0.2689,  -0.1446,   0.1045,   1.9316,   1.0000],\n",
       "        [ -4.1243,   3.7486,  -7.9265,   7.7632,  -0.7694,  -2.0312,\n",
       "          -3.4745,   0.1071,  -1.5514,  -2.4113,   4.4800,  -8.8325,\n",
       "          -1.2152, -13.7257,   1.2327,  -4.3897,  -5.8109,  -1.2554,\n",
       "           1.1140,  -0.1820,   0.5471,   0.6879,   0.4299,  -0.6206,\n",
       "          -0.3697,   0.3673,  -2.6648,   0.4171,  -0.3492,   1.0000]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
