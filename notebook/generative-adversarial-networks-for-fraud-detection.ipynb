{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import recall_score\n",
    "import torch\n",
    "#from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quadro P5000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if Cuda is running\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomize the data, just to be sure not to get any pathological ordering effects that might harm the performane of Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124011</th>\n",
       "      <td>77147.0</td>\n",
       "      <td>-1.437314</td>\n",
       "      <td>-0.672735</td>\n",
       "      <td>0.925002</td>\n",
       "      <td>-0.728117</td>\n",
       "      <td>-0.498919</td>\n",
       "      <td>-1.088232</td>\n",
       "      <td>0.098312</td>\n",
       "      <td>0.294362</td>\n",
       "      <td>-1.763211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032172</td>\n",
       "      <td>-0.254662</td>\n",
       "      <td>0.569343</td>\n",
       "      <td>0.599137</td>\n",
       "      <td>-0.909906</td>\n",
       "      <td>0.070153</td>\n",
       "      <td>0.017838</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>149.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182926</th>\n",
       "      <td>125586.0</td>\n",
       "      <td>-1.188026</td>\n",
       "      <td>0.648003</td>\n",
       "      <td>0.195708</td>\n",
       "      <td>-2.300317</td>\n",
       "      <td>-1.876858</td>\n",
       "      <td>0.483442</td>\n",
       "      <td>-0.257970</td>\n",
       "      <td>-4.314653</td>\n",
       "      <td>1.394378</td>\n",
       "      <td>...</td>\n",
       "      <td>3.773234</td>\n",
       "      <td>-1.129681</td>\n",
       "      <td>-1.190132</td>\n",
       "      <td>0.066215</td>\n",
       "      <td>0.806729</td>\n",
       "      <td>0.669240</td>\n",
       "      <td>0.580496</td>\n",
       "      <td>0.090244</td>\n",
       "      <td>392.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>2364.0</td>\n",
       "      <td>1.571567</td>\n",
       "      <td>-0.852696</td>\n",
       "      <td>-0.411170</td>\n",
       "      <td>-1.735533</td>\n",
       "      <td>-0.574247</td>\n",
       "      <td>-0.298426</td>\n",
       "      <td>-0.606201</td>\n",
       "      <td>-0.165091</td>\n",
       "      <td>-2.384797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.546954</td>\n",
       "      <td>-1.269700</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>-0.896239</td>\n",
       "      <td>0.433003</td>\n",
       "      <td>-0.425964</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>-0.003500</td>\n",
       "      <td>19.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10848</th>\n",
       "      <td>18552.0</td>\n",
       "      <td>-0.265284</td>\n",
       "      <td>0.081207</td>\n",
       "      <td>1.595559</td>\n",
       "      <td>-1.671107</td>\n",
       "      <td>-0.359368</td>\n",
       "      <td>-0.158713</td>\n",
       "      <td>-0.188054</td>\n",
       "      <td>0.271057</td>\n",
       "      <td>2.710338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102560</td>\n",
       "      <td>0.673973</td>\n",
       "      <td>0.126516</td>\n",
       "      <td>-0.065758</td>\n",
       "      <td>-1.159539</td>\n",
       "      <td>-1.066871</td>\n",
       "      <td>0.283226</td>\n",
       "      <td>0.230268</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250966</th>\n",
       "      <td>155148.0</td>\n",
       "      <td>-1.071928</td>\n",
       "      <td>0.594211</td>\n",
       "      <td>-0.352817</td>\n",
       "      <td>-0.819417</td>\n",
       "      <td>1.439513</td>\n",
       "      <td>0.755137</td>\n",
       "      <td>0.594540</td>\n",
       "      <td>0.448423</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085680</td>\n",
       "      <td>0.088365</td>\n",
       "      <td>0.361190</td>\n",
       "      <td>-0.336159</td>\n",
       "      <td>-1.006584</td>\n",
       "      <td>0.176926</td>\n",
       "      <td>-0.098098</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>14.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "124011   77147.0 -1.437314 -0.672735  0.925002 -0.728117 -0.498919 -1.088232   \n",
       "182926  125586.0 -1.188026  0.648003  0.195708 -2.300317 -1.876858  0.483442   \n",
       "2807      2364.0  1.571567 -0.852696 -0.411170 -1.735533 -0.574247 -0.298426   \n",
       "10848    18552.0 -0.265284  0.081207  1.595559 -1.671107 -0.359368 -0.158713   \n",
       "250966  155148.0 -1.071928  0.594211 -0.352817 -0.819417  1.439513  0.755137   \n",
       "\n",
       "              V7        V8        V9  ...         V21       V22       V23  \\\n",
       "124011  0.098312  0.294362 -1.763211  ...   -0.032172 -0.254662  0.569343   \n",
       "182926 -0.257970 -4.314653  1.394378  ...    3.773234 -1.129681 -1.190132   \n",
       "2807   -0.606201 -0.165091 -2.384797  ...   -0.546954 -1.269700  0.009546   \n",
       "10848  -0.188054  0.271057  2.710338  ...    0.102560  0.673973  0.126516   \n",
       "250966  0.594540  0.448423  0.063034  ...   -0.085680  0.088365  0.361190   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "124011  0.599137 -0.909906  0.070153  0.017838  0.001861  149.20      0  \n",
       "182926  0.066215  0.806729  0.669240  0.580496  0.090244  392.00      0  \n",
       "2807   -0.896239  0.433003 -0.425964  0.000977 -0.003500   19.75      0  \n",
       "10848  -0.065758 -1.159539 -1.066871  0.283226  0.230268   11.85      0  \n",
       "250966 -0.336159 -1.006584  0.176926 -0.098098  0.001136   14.75      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.reindex(np.random.permutation(data.index))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.174225e-15</td>\n",
       "      <td>3.429687e-16</td>\n",
       "      <td>-1.386421e-15</td>\n",
       "      <td>2.073779e-15</td>\n",
       "      <td>9.939598e-16</td>\n",
       "      <td>1.493625e-15</td>\n",
       "      <td>-5.931037e-16</td>\n",
       "      <td>1.318317e-16</td>\n",
       "      <td>-2.414318e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.416845e-16</td>\n",
       "      <td>-3.515296e-16</td>\n",
       "      <td>2.727492e-16</td>\n",
       "      <td>4.482012e-15</td>\n",
       "      <td>5.203181e-16</td>\n",
       "      <td>1.689590e-15</td>\n",
       "      <td>-3.712632e-16</td>\n",
       "      <td>-1.159267e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.174225e-15  3.429687e-16 -1.386421e-15  2.073779e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.939598e-16  1.493625e-15 -5.931037e-16  1.318317e-16 -2.414318e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...        1.416845e-16 -3.515296e-16  2.727492e-16  4.482012e-15   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.203181e-16  1.689590e-15 -3.712632e-16 -1.159267e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Frequency')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = pd.value_counts(data['Class'], sort = True).sort_index()\n",
    "classes.plot(kind = 'bar')\n",
    "plt.title(\"Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Class')['Class'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Data is hihgly imbalance. 284315 Normal transaction vs 492 Fraud transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run with Normalising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns != 'Class']\n",
    "y = data.loc[:, data.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape\n",
      "(199364, 30)\n",
      "xtest shape\n",
      "(85443, 30)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "print('xtrain shape')\n",
    "print(X_train.shape)\n",
    "print('xtest shape')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the training data and test data\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit classifier to a model\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85287,     4],\n",
       "       [   45,   107]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     85291\n",
      "          1       0.96      0.70      0.81       152\n",
      "\n",
      "avg / total       1.00      1.00      1.00     85443\n",
      "\n",
      "Accuracy : 0.999427\n",
      "Area under the curve : 0.851950\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print('Accuracy : %f' % (metrics.accuracy_score(y_test, y_pred)))\n",
    "print('Area under the curve : %f' % (metrics.roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run with Over Sampling data using SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.349671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.349231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.127897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.053373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.221892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10     ...           V21       V22       V23  \\\n",
       "0  0.098698  0.363787  0.090794     ...     -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425 -0.166974     ...     -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654  0.207643     ...      0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024 -0.054952     ...     -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739  0.753074     ...     -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  normAmount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0   -0.349671  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   -0.349231  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0   -0.127897  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0   -0.053373  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0   -0.221892  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data2['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "data2 = data2.drop(['Time','Amount'],axis=1)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (284807, 30)\n",
      "Shape of y: (284807, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(data.ix[:, data.columns != 'Class'])\n",
    "y = np.array(data.ix[:, data.columns == 'Class'])\n",
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape\n",
      "(199364, 30)\n",
      "xtest shape\n",
      "(85443, 30)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print('xtrain shape')\n",
    "print(X_train.shape)\n",
    "print('xtest shape')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over Sampling data using SMOTE\n",
    "smote = SMOTE(random_state=2)\n",
    "X_train_resample, y_train_resample = smote.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train_resample, y_train_resample.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85274,    17],\n",
       "       [   26,   126]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a Confision Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     85291\n",
      "          1       0.88      0.83      0.85       152\n",
      "\n",
      "avg / total       1.00      1.00      1.00     85443\n",
      "\n",
      "Accuracy : 0.999497\n",
      "Area under the curve : 0.914374\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print('Accuracy : %f' % (metrics.accuracy_score(y_test, y_pred)))\n",
    "print('Area under the curve : %f' % (metrics.roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run with DCGANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Custom DataLoader\n",
    "class FraudDataset(Dataset):\n",
    "    \n",
    "    # Initialize the data\n",
    "    def __init__(self):\n",
    "        data = pd.read_csv(\"creditcard.csv\")\n",
    "        data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "        data = data.drop(['Time','Amount'],axis=1)\n",
    "        \n",
    "        # Rearrange columns to the right order\n",
    "        cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "        'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount', 'Class']\n",
    "        data = data[cols]\n",
    "        \n",
    "        fraud_data = data.loc[data['Class']==1]\n",
    "        fraud_data = fraud_data.drop('Class', 1)\n",
    "        self.len = fraud_data.shape[0]\n",
    "        \n",
    "        self.fraud_data = torch.FloatTensor(np.array(fraud_data))\n",
    "        \n",
    "        #self.X = np.array(data.loc[:, data.columns != 'Class'])\n",
    "        #self.y = np.array(data.loc[:, data.columns == 'Class'])\n",
    "        \n",
    "        #self.X = torch.FloatTensor(self.X)\n",
    "        #self.y = torch.FloatTensor(self.y)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.fraud_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FraudDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=5,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator's paragrams\n",
    "g_input_size = 29     # Random noise dimension\n",
    "g_hidden_size = 50   # Generator complexity\n",
    "g_output_size = 1   \n",
    "g_learning_rate = 0.0002\n",
    "\n",
    "#Discriminator's paragrams\n",
    "d_input_size = 29   # Minibatch size\n",
    "d_hidden_size = 50  # Discriminator complexity\n",
    "d_output_size = 1   # Single dimension for 'real' vs. 'fake'\n",
    "d_learning_rate = 0.0002\n",
    "\n",
    "minibatch_size = d_input_size\n",
    "\n",
    "num_epochs = 50\n",
    "print_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ELU (Exponential Linear Unit) function tends to converge cost to zero faster and produce more accurate results\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.sigmoid(self.map2(x))\n",
    "        return self.map3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.map2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.map3(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "discriminator = Discriminator(input_size=d_input_size, hidden_size=d_hidden_size, output_size=d_output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these 2 lines to run on GPU\n",
    "#generator.cuda()\n",
    "#discriminator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(disc):\n",
    "    h=0.1\n",
    "    x_min, x_max = data[:, 0].min() - .5, data[:, 0].max() + .5\n",
    "    y_min, y_max = data[:, 1].min() - .5, data[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    x_test = np.c_[xx.ravel(), yy.ravel()]\n",
    "    y_hat_test = disc.forward_with_sigmoid(Variable(torch.from_numpy(x_test).float()))\n",
    "\n",
    "    plt.pcolormesh(xx, yy, y_hat_test.data.numpy().reshape(xx.shape), cmap=plt.cm.Paired)\n",
    "    plt.colorbar()\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y*20, alpha=0.1, cmap=plt.cm.flag, s=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Binary Cross Entropy loss\n",
    "BCE_loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizers\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=d_learning_rate/2, betas=(beta_1, beta_2))\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=g_learning_rate, betas=(beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Discriminator Loss: 0.992, Generator Loss: 0.758\n",
      "Epoch 11 - Discriminator Loss: 0.320, Generator Loss: 1.665\n",
      "Epoch 21 - Discriminator Loss: 0.246, Generator Loss: 2.674\n",
      "Epoch 31 - Discriminator Loss: 0.571, Generator Loss: 2.698\n",
      "Epoch 41 - Discriminator Loss: 0.441, Generator Loss: 3.209\n"
     ]
    }
   ],
   "source": [
    "# Training DCGANs\n",
    "for epoch in range(num_epochs):\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    synthentic_data = []\n",
    "    for i, fraud_data in enumerate(train_loader):\n",
    "        # Updating the weights of the Discriminator\n",
    "        discriminator.zero_grad() # Initialize gradients of the Discriminator to 0\n",
    "        \n",
    "        mini_batch = fraud_data.size()[0]\n",
    "        \n",
    "        # Wrap data in PyTorch Variable\n",
    "        d_real_data = Variable(fraud_data[0])\n",
    "        y_real = Variable(torch.ones(1))\n",
    "        y_fake = Variable(torch.zeros(1))\n",
    "\n",
    "        # Training the Discriminator with real data\n",
    "        d_real_result = discriminator(d_real_data) # Forward propagate this real data into the neural network\n",
    "        d_real_loss = BCE_loss(d_real_result, y_real) # Compute the loss between the prediction and actual\n",
    "        d_real_loss.backward()\n",
    "    \n",
    "        # Inject fake data to the generator\n",
    "        d_gen_input = Variable(torch.randn(minibatch_size, g_input_size))\n",
    "        d_fake_data = generator(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "        \n",
    "        # Train the Discriminator with a fake data generated by the Generator\n",
    "        d_fake_result = discriminator(d_fake_data.t())\n",
    "        d_fake_loss = BCE_loss(d_fake_result, y_fake)  # zeros = fake\n",
    "        d_fake_loss.backward()\n",
    "        \n",
    "        # Combine discriminator loss from real data and fake data\n",
    "        d_train_loss = d_real_loss + d_fake_loss\n",
    "        \n",
    "        #d_train_loss.backward()\n",
    "        d_optimizer.step()     # Apply SGD to update the weight\n",
    "        d_losses.append(d_train_loss.data[0])\n",
    "        \n",
    "        # Update the weight of the Generator \n",
    "        generator.zero_grad()\n",
    "        gen_input = Variable(torch.randn(minibatch_size, g_input_size))  \n",
    "        g_fake_data = generator(gen_input)\n",
    "        \n",
    "        dg_fake_result = discriminator(g_fake_data.t())\n",
    "        g_loss = BCE_loss(dg_fake_result, y_real)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        g_losses.append(g_loss.data[0])\n",
    "        \n",
    "        synthentic_data.append(d_fake_data.t())\n",
    "        \n",
    "    if epoch % print_interval == 0:       \n",
    "        print('Epoch {} - Discriminator Loss: {:.3f}, Generator Loss: {:.3f}'.format((epoch + 1), \n",
    "                          torch.mean(torch.FloatTensor(d_losses)), torch.mean(torch.FloatTensor(g_losses))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a1a566cf8>]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(d_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a1a6a35c0>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.8784,  1.1667,  1.2671, -1.7199, -2.5015, -1.2234, -2.4209,\n",
       "          -1.0354, -3.4353,  0.9306, -3.4985,  3.2542,  3.1455,  1.5114,\n",
       "          -1.1195,  0.3157,  0.2248,  0.7054, -3.2295,  1.5139,  1.2577,\n",
       "           0.8339,  2.6861,  2.1388,  1.5549, -3.6569, -2.1506, -0.3078,\n",
       "           1.3698]]),\n",
       " tensor([[ 3.2863,  1.7771, -1.6219,  0.6940, -1.9395,  1.8656,  2.5490,\n",
       "           2.6673,  0.5106, -2.8246,  0.7647,  0.4604, -1.5331,  0.5374,\n",
       "          -2.2743, -0.1050,  1.7366,  1.6673, -1.1246, -1.6717, -0.0382,\n",
       "           1.5288,  1.5320,  3.7370, -1.8383,  1.0751, -1.6985, -0.1015,\n",
       "           0.9652]]),\n",
       " tensor([[-1.7456,  1.2269, -2.2872,  1.7300, -0.6816, -2.1846,  1.0330,\n",
       "          -0.9044, -3.3970,  2.0657,  1.7152, -1.0442,  1.0915,  0.9967,\n",
       "          -2.9899,  1.1809,  0.8800,  0.2966, -3.1595, -4.1140,  2.5320,\n",
       "           3.1736,  1.9617, -1.0912, -1.4560, -1.2177, -0.7118, -2.3616,\n",
       "          -1.6460]]),\n",
       " tensor([[ 2.1116, -1.2176, -1.8866,  2.5699, -2.6370,  2.7286, -3.2633,\n",
       "          -0.7544, -1.2241,  3.5030, -2.6551, -3.2851,  0.4379, -1.6114,\n",
       "          -1.5858, -0.5306,  2.2512, -1.0752,  2.2132,  2.4613,  0.0809,\n",
       "          -1.1854, -2.3802, -1.3442, -2.0757,  2.5466,  1.7475, -1.4641,\n",
       "           0.1979]]),\n",
       " tensor([[ 1.5143, -0.9330,  2.5055,  2.1377, -0.8549, -2.2374, -0.5444,\n",
       "           1.8986, -2.8344,  0.5548, -2.7647,  1.8266,  2.3652, -1.0123,\n",
       "           2.9221,  0.8031, -0.0583, -2.0171,  0.8470,  2.8272, -1.9172,\n",
       "           0.4459,  1.9176, -3.7247,  0.6421,  0.9809,  0.3578, -0.6784,\n",
       "          -0.0751]]),\n",
       " tensor([[ 3.8709,  2.9834, -0.0672,  0.6673,  1.5664,  3.2081,  1.1647,\n",
       "           0.8526,  3.2920,  0.3595,  1.3061, -0.9192, -2.7435,  1.3064,\n",
       "           1.2843, -1.9866, -0.0258,  3.7258,  0.0937, -0.2731,  3.1895,\n",
       "          -3.5862, -0.2312,  2.4645, -1.3340, -1.1156,  0.4465,  0.0917,\n",
       "           2.6085]]),\n",
       " tensor([[ 2.6201, -2.3441, -3.4121, -2.3308, -3.1304, -2.1821,  2.9571,\n",
       "          -0.7529,  0.8613,  0.4074, -3.0241,  0.2072, -0.6396,  0.8780,\n",
       "           1.2449,  1.1396,  1.3147,  0.1466,  2.1462, -2.9094,  0.2188,\n",
       "           0.7243, -0.9739, -0.8602, -2.0391, -2.3320, -0.7503, -2.4715,\n",
       "           2.3663]]),\n",
       " tensor([[-1.7290,  2.5678, -2.4752, -1.2170,  0.7525, -0.1505,  3.8717,\n",
       "          -2.8631, -1.3913, -1.2023,  1.5463,  0.2623, -0.3328,  0.4008,\n",
       "          -3.7870, -0.5458, -2.3945,  1.6265,  0.3060, -2.8240, -0.8112,\n",
       "           3.2602,  1.4329, -0.7147,  0.6561, -2.6996,  3.8169,  0.3831,\n",
       "           1.3559]]),\n",
       " tensor([[-2.9858, -1.7425, -1.2426,  3.3244,  0.8560,  0.3510,  0.8682,\n",
       "          -2.2894, -3.2800, -1.1979,  3.1827, -3.2623, -2.8534,  1.0047,\n",
       "          -0.7208, -1.8506, -2.0805,  1.1454,  1.5995, -2.4011, -0.8791,\n",
       "           0.6067,  0.5465, -0.6409,  1.9887, -1.8832, -3.4278,  1.5668,\n",
       "          -1.1314]]),\n",
       " tensor([[-3.7643, -0.2098, -3.6389, -3.6700,  3.8406,  1.9131,  1.5793,\n",
       "          -2.5764, -2.3344, -2.6958,  3.5602,  0.2430, -1.1049,  2.2081,\n",
       "           1.8533,  3.4055,  0.7508, -0.2771, -0.9010,  0.3884,  0.8819,\n",
       "          -2.3655, -0.9761,  3.0692, -1.2611, -0.8655,  0.4200,  1.7351,\n",
       "          -3.3336]]),\n",
       " tensor([[ 1.4885,  1.9209,  2.1428, -2.5917,  1.1809, -1.5493, -2.8925,\n",
       "           0.9801, -0.6323,  1.5024,  0.4320,  0.2581,  0.2351, -1.8809,\n",
       "           0.2648,  1.8531, -0.3570, -1.2163, -1.2746,  2.0328, -1.4084,\n",
       "           0.0003, -1.9059,  1.9514,  1.1014,  1.0258,  1.2865,  1.8573,\n",
       "          -1.9166]]),\n",
       " tensor([[-2.0929,  0.8560,  1.6556, -0.0725, -0.5107, -1.2444,  1.4468,\n",
       "          -0.7600, -1.5743,  2.7351,  0.2851,  0.9781, -0.7022,  0.4489,\n",
       "          -2.4615, -1.2164, -0.0101, -2.3900,  0.7341,  2.5100, -0.9624,\n",
       "           0.4524, -3.0108,  0.3069,  1.6704,  2.2767,  0.9029,  0.9083,\n",
       "          -0.2285]]),\n",
       " tensor([[ 0.0006,  2.3913, -3.9098, -1.5106, -0.0323,  0.6292,  0.5771,\n",
       "           0.4221, -0.2113,  2.3655,  1.8484, -0.8191, -0.4258, -0.2744,\n",
       "           1.2617, -2.7505, -1.4385,  2.2977, -2.4929,  2.4109,  0.0640,\n",
       "           0.7074, -1.5470, -2.9244,  0.5858,  1.2159, -2.5428,  1.7356,\n",
       "           1.4180]]),\n",
       " tensor([[-2.6799, -1.2138,  1.7287,  0.0937, -0.9042,  0.1866,  1.3045,\n",
       "           0.1329,  2.8783, -2.2654, -2.6507,  1.5298, -1.2103,  0.4671,\n",
       "          -1.3990,  1.8208, -0.6789,  1.1142, -3.3444,  3.1607, -1.7061,\n",
       "          -2.5606,  1.1410,  2.6326,  1.3374, -0.5571, -1.5332,  3.7933,\n",
       "           1.2358]]),\n",
       " tensor([[-3.1532,  0.9316,  1.7028, -1.0478, -1.4794, -2.5133, -0.3132,\n",
       "          -2.0613,  1.7201, -3.3023, -1.0785, -1.8201, -2.0878, -3.4933,\n",
       "          -1.0276, -1.0320,  2.0177,  1.3601,  3.0266, -0.4247, -3.4602,\n",
       "           2.6834, -2.2743, -1.5559,  1.4862, -0.7779,  2.7223, -3.3697,\n",
       "          -2.5711]]),\n",
       " tensor([[-0.1173, -2.6495, -1.0593, -1.5351,  1.1116,  3.3548,  2.2420,\n",
       "           3.4637, -1.3139, -3.0398, -1.3438,  1.8987, -1.3139,  1.0027,\n",
       "          -1.8313,  1.5401,  0.6966,  0.5979, -2.8710, -0.8069,  2.3021,\n",
       "          -1.2339, -0.3996,  1.9595,  3.3201, -3.5016, -2.8409, -2.3577,\n",
       "           0.0125]]),\n",
       " tensor([[-1.1752, -3.7057,  0.4248, -0.2621, -0.9404,  2.7640, -1.1167,\n",
       "           0.3639,  1.1474, -2.8594,  0.8950, -3.8546, -0.3366,  2.2716,\n",
       "          -0.8391,  1.8228,  1.9010, -1.6916, -2.3345,  0.1563, -0.6363,\n",
       "           3.2833,  2.2837,  1.9656,  2.9422,  2.1986,  1.6285, -0.1825,\n",
       "           1.6291]]),\n",
       " tensor([[-0.7579, -0.3791,  2.5915, -0.1107,  0.2642,  1.0689,  2.5395,\n",
       "          -1.4441, -0.1447, -1.3609,  3.1560,  0.9731,  1.6684, -1.3150,\n",
       "          -2.1709,  0.1436, -0.1612, -1.1143,  1.0143,  1.8659, -0.0931,\n",
       "           2.8053, -1.1676,  2.8287, -1.4562, -1.6421,  0.6307, -2.2054,\n",
       "           3.1008]]),\n",
       " tensor([[ 2.0282,  1.6270,  0.4959,  2.6637, -0.5450,  0.1930, -3.8452,\n",
       "          -1.7213,  0.3410, -0.1817, -2.2711, -0.3651,  1.7489,  1.3120,\n",
       "          -0.1264, -2.5837,  0.2030,  0.0210,  0.8991,  0.3965,  1.1575,\n",
       "           2.7164, -1.1328, -0.2009,  3.4065,  2.9410, -2.4378,  1.8045,\n",
       "           2.6663]]),\n",
       " tensor([[-2.9817, -0.9071,  3.2090, -2.2072,  1.9197, -0.0536, -1.7364,\n",
       "          -0.9084, -1.2692, -3.1369, -0.5750, -3.4214, -1.0590, -3.3726,\n",
       "           0.8876, -0.6894,  2.5901, -0.9126,  1.2155, -0.5946, -2.9386,\n",
       "          -2.9706,  2.0192,  0.8591,  2.5576,  2.2436, -2.8323, -1.1429,\n",
       "          -1.2757]]),\n",
       " tensor([[ 0.8377,  1.0685, -2.1450, -0.6838, -1.6892, -3.1214,  0.5608,\n",
       "          -2.4917, -2.0384,  2.2592,  3.2775,  1.3302,  2.9490,  2.0433,\n",
       "          -2.2244,  2.0218, -1.0117, -0.4549,  2.0140,  1.3460, -1.5833,\n",
       "          -3.1406, -2.8119, -1.4066, -1.4311,  2.1462,  1.0922,  0.3323,\n",
       "          -2.7739]]),\n",
       " tensor([[ 2.5823,  3.7009,  1.7295, -2.2811,  2.5649, -1.8467, -2.9278,\n",
       "           3.1411, -0.5078,  0.9076,  0.0507,  0.0380, -3.6487, -0.0776,\n",
       "          -1.8705,  2.5588,  1.8380,  1.5525, -0.4935,  2.8597,  1.1905,\n",
       "           0.9787, -0.0863, -3.0076, -3.4472,  3.1275, -1.7924,  0.6309,\n",
       "           1.8056]]),\n",
       " tensor([[ 0.0035,  0.9284, -0.8902, -0.0128,  1.8303, -1.2128, -0.3919,\n",
       "           0.2024,  0.3930,  2.6653, -3.6170,  1.0211, -2.1963,  1.4748,\n",
       "           1.0569,  2.8787, -1.4564, -3.4973, -2.7438, -0.0344, -0.6902,\n",
       "          -1.2745,  1.6781,  1.1442,  1.7417,  2.4166,  0.3060,  2.8674,\n",
       "           2.0035]]),\n",
       " tensor([[-0.6847, -2.1855,  3.1758, -3.0394, -1.9520, -2.2553, -0.2413,\n",
       "          -0.8594,  0.9637,  2.4872,  0.6613, -0.3050, -1.0609, -0.0285,\n",
       "          -0.9296,  1.5215,  0.9599,  1.3438, -0.4590,  0.0227,  3.0015,\n",
       "           3.2205, -1.3562, -3.2156, -1.5430, -0.0200, -2.6867, -1.2393,\n",
       "           0.5086]]),\n",
       " tensor([[-0.8350, -3.1797, -0.9958, -1.7900,  3.0619,  2.3835,  0.3219,\n",
       "           2.5660,  3.1565, -1.3710,  2.3712, -1.0774, -0.6779,  1.7794,\n",
       "          -0.8214, -0.9068, -1.1946,  0.0527,  2.4175, -0.1881,  2.3179,\n",
       "          -2.0700,  0.5719,  0.5311,  1.6414,  0.4435,  2.3141,  3.1399,\n",
       "          -0.7415]]),\n",
       " tensor([[-2.4229, -2.7533,  1.4188, -2.7475,  0.0133, -0.4266, -0.9129,\n",
       "          -2.5190,  0.7354,  1.0996,  3.5370,  2.6002, -1.4421,  2.4822,\n",
       "          -2.7957, -0.6983,  0.1597,  1.5496,  2.2404, -2.9333, -1.3184,\n",
       "           0.2163, -1.5878, -3.5384, -1.7389, -0.5363, -3.1038,  0.3298,\n",
       "          -0.8521]]),\n",
       " tensor([[ 2.3212,  2.2665,  2.7346,  2.4274,  3.1095,  1.0372, -3.3429,\n",
       "           2.2108,  2.2857, -0.5353, -0.7353,  3.0927, -0.0984,  1.5598,\n",
       "           0.1439,  1.3706,  0.2407, -1.4828,  3.3537, -0.3131,  3.3629,\n",
       "          -2.0097, -0.2790, -0.5763, -0.7701,  3.4939, -0.0562, -1.5250,\n",
       "           2.3342]]),\n",
       " tensor([[-1.4147, -2.5605, -0.2234, -0.6102, -3.4848,  3.4923,  3.5363,\n",
       "          -3.4855,  2.5547, -1.2170,  1.7221,  1.9499, -1.2835,  2.2526,\n",
       "           1.7414, -2.8544,  2.7471, -1.0179,  2.7011,  1.5797, -1.0225,\n",
       "          -0.6672,  0.5382,  0.3906, -1.7676,  0.5458,  0.5235,  1.2549,\n",
       "           0.5905]]),\n",
       " tensor([[ 0.5484,  1.1327,  0.3989,  3.3336, -0.5679,  1.2825,  1.1854,\n",
       "           0.4898,  1.8824,  0.5048, -1.8175,  1.5407, -0.0612,  1.2331,\n",
       "          -3.9064,  1.3233,  0.5525,  1.2316,  2.3565, -1.3475, -0.5991,\n",
       "          -1.6045, -1.0383, -0.7866,  0.3095,  1.8939,  2.7203,  2.4403,\n",
       "           2.2870]]),\n",
       " tensor([[ 0.4625, -1.5524, -2.8535, -0.6297, -3.6776, -1.9819,  0.4653,\n",
       "          -2.9379, -3.1683, -1.0225,  0.6474,  2.7495, -0.3555,  0.9934,\n",
       "          -1.0552,  2.2022, -2.7722,  0.1507,  1.0384,  2.0827, -0.2515,\n",
       "           0.0923,  1.3404,  1.9890, -0.4390, -0.6255,  2.9918,  2.0120,\n",
       "          -1.8461]]),\n",
       " tensor([[ 8.1029e-05,  5.3000e-01,  9.4811e-01,  2.0184e+00,  9.4258e-01,\n",
       "          -2.4209e+00,  2.1483e+00,  5.1940e-01, -1.0127e+00,  1.3513e+00,\n",
       "          -2.1830e-01, -2.3919e+00,  6.3214e-02, -1.7093e+00, -1.7163e+00,\n",
       "           1.8807e+00, -1.8911e+00,  1.0854e+00, -1.5056e+00, -6.4161e-01,\n",
       "           2.5367e+00, -2.8220e+00,  2.8252e-01,  4.9996e-01, -1.3949e-01,\n",
       "          -2.9569e+00,  3.1174e-01, -1.9483e+00,  1.9439e+00]]),\n",
       " tensor([[-0.5957, -0.4849, -1.0794, -3.0903, -0.3808, -2.3617,  1.0814,\n",
       "          -2.4414,  2.9886, -2.0574, -0.2710,  2.1287, -0.0143,  0.5516,\n",
       "          -1.6550,  1.6828,  2.8244,  1.8080, -1.6750, -0.6451, -3.1217,\n",
       "          -2.8157, -1.2096,  1.4227,  2.4804, -0.8835, -1.8909,  1.2449,\n",
       "          -3.3308]]),\n",
       " tensor([[ 2.0565, -2.4586,  1.8952, -1.0215, -1.1810, -2.4618, -1.3296,\n",
       "           1.4232,  0.8188,  0.8841,  0.0344, -0.1345,  1.2110, -0.0047,\n",
       "          -3.7276,  1.6288,  1.6493, -3.5451,  0.9208,  0.8152, -3.6456,\n",
       "           0.8283, -0.1213, -3.2246,  1.0409, -2.7697,  0.0364, -0.2390,\n",
       "           2.0311]]),\n",
       " tensor([[-2.0358,  3.5413,  2.1993,  1.7370,  3.4837,  2.1427, -3.0710,\n",
       "           1.8852,  1.7865,  1.7489, -0.0617,  2.4188,  1.3691, -0.5443,\n",
       "           1.7336,  1.1309,  2.8834,  1.8669, -0.8458,  1.0192, -3.5078,\n",
       "          -0.0914,  1.4894,  3.1568,  0.8926, -0.0025,  0.2530,  1.4484,\n",
       "           1.6310]]),\n",
       " tensor([[-0.4842, -1.6342,  3.4701,  1.2330,  0.2898, -2.4151, -2.6990,\n",
       "           0.7371,  1.0608,  2.0437,  1.5097, -0.3290, -2.0034,  0.0985,\n",
       "          -2.0336,  1.7792, -0.8872,  1.1811,  0.4269, -0.0052, -0.0406,\n",
       "          -1.3894,  3.4851,  2.0162, -2.4551, -0.0262,  3.6299,  2.0573,\n",
       "           1.4525]]),\n",
       " tensor([[-1.9568, -1.1869,  1.8892, -0.6331, -0.1683, -3.8238, -0.5421,\n",
       "          -0.5919,  1.1299,  2.2320,  3.0497, -2.6945, -1.9435, -3.6287,\n",
       "          -1.5592, -1.5271, -0.6604, -0.4835,  0.4509, -1.0347,  3.0189,\n",
       "          -0.5076,  2.1677, -0.6245, -0.1726,  0.4879, -0.3311, -1.3898,\n",
       "          -0.8855]]),\n",
       " tensor([[-1.6480, -0.2939, -3.1693,  3.3698,  1.3107, -3.4432, -0.0094,\n",
       "          -0.6274,  1.6473, -2.8741, -0.0445,  2.4003,  0.9912, -0.8450,\n",
       "           2.0967, -1.8852,  2.3218,  1.1918,  0.2512,  0.2460, -1.7221,\n",
       "           0.5333,  3.0395, -2.4348, -0.9909,  1.7917,  2.6798,  1.1653,\n",
       "           0.1153]]),\n",
       " tensor([[ 0.6230,  3.5683, -0.9011, -1.5461, -3.3431,  1.7429, -2.5470,\n",
       "          -1.5541,  0.0719, -0.3671,  2.6099,  3.2923,  0.5082,  2.0377,\n",
       "           2.4354,  2.0588, -1.1533, -2.0411, -0.8287, -2.9830,  0.0191,\n",
       "          -3.1409, -2.2684,  0.0622,  0.4857, -0.9239,  0.8954, -3.5243,\n",
       "          -0.1664]]),\n",
       " tensor([[-3.7050,  0.6476, -2.9623,  0.9530,  0.9941,  0.1202,  1.0131,\n",
       "          -1.1861,  1.4981,  1.6022,  3.4158,  0.8033,  0.0569,  0.3965,\n",
       "           2.7726, -2.2946,  0.2221,  1.1960, -2.6957, -1.4697, -3.5411,\n",
       "           1.8944,  0.6393, -1.2061,  2.9572,  2.1066,  0.7847,  0.3731,\n",
       "           0.8644]]),\n",
       " tensor([[-1.0668, -2.1658,  0.7072,  3.0066,  2.7823, -2.4285,  1.3424,\n",
       "          -1.9894,  1.3120,  0.8434, -0.1424,  2.4629, -0.9651, -3.0803,\n",
       "           2.2601, -1.0919,  0.8667, -0.0939,  2.1536,  2.0303, -0.4321,\n",
       "          -3.5964,  0.5067,  3.0050,  3.6323, -2.6634,  1.7924, -3.0611,\n",
       "           0.8331]]),\n",
       " tensor([[-0.5197, -3.2419,  0.2545, -1.7896,  2.8683,  2.2899, -0.1000,\n",
       "           2.3389,  1.3866, -2.9128,  1.5420,  1.8216, -0.3122,  1.2634,\n",
       "          -0.4486,  2.1238, -0.2657, -2.9352, -0.1304,  3.3828,  3.3880,\n",
       "          -2.3736, -0.7920,  0.5089, -1.4953,  0.1694, -1.9598, -1.7177,\n",
       "          -0.7921]]),\n",
       " tensor([[ 0.2469,  1.8480,  0.2330, -1.1912, -3.1603,  3.4272,  2.0819,\n",
       "          -0.9376, -2.2976, -3.3253, -1.9790, -2.4689, -2.1819,  0.7612,\n",
       "          -3.1471,  0.6092, -0.9865,  1.6416,  2.5867, -1.4256, -2.1391,\n",
       "           0.2866,  2.7088,  2.6437,  1.3298, -2.2605, -0.2497, -0.4083,\n",
       "          -2.8694]]),\n",
       " tensor([[ 3.4842, -0.3941, -0.3295,  0.6353,  0.6636,  2.2778,  0.0391,\n",
       "          -0.1684, -1.1872, -0.3081,  0.4934,  2.1183,  0.0265, -0.0243,\n",
       "           1.1295,  0.3253,  1.0658, -1.7368,  1.1766,  1.0805, -0.4485,\n",
       "           1.6780,  2.3462, -0.3478,  0.2645,  2.9293,  0.9651,  1.4449,\n",
       "           2.3081]]),\n",
       " tensor([[-0.5994,  0.4927,  2.1309, -0.5517,  0.5841, -3.1501,  1.9553,\n",
       "          -0.4009,  3.9000,  3.1129, -1.6365, -0.1694, -1.6960, -1.9791,\n",
       "          -0.6070, -1.1097, -0.7014,  0.5783, -1.1429, -0.2545, -2.2739,\n",
       "           1.4485,  1.6601, -2.6467,  0.8564,  1.1898, -3.2912,  2.8932,\n",
       "           2.3144]]),\n",
       " tensor([[-1.3082,  2.8774, -0.7891,  2.5741,  1.7102, -0.8330, -1.1502,\n",
       "           0.8395,  0.7662,  2.7780,  2.4621,  2.5133,  2.2121,  0.4082,\n",
       "          -0.4202, -1.9502, -1.4176,  2.3208, -1.4356, -1.9985,  2.9616,\n",
       "           1.3921, -0.2119, -1.3498,  0.3085,  2.8982,  2.3839, -1.1242,\n",
       "           2.5942]]),\n",
       " tensor([[-0.9553,  2.8524, -0.7221, -1.8813,  3.0232,  2.3198, -0.9933,\n",
       "           2.3001, -1.6308,  0.0231, -2.0798, -2.1635, -2.6298,  2.7904,\n",
       "          -2.6055, -0.3223, -2.1090,  1.3052,  2.5499, -3.0352,  0.6728,\n",
       "           0.2186, -2.0213, -2.3898, -1.2383,  1.4515,  3.2971, -2.7350,\n",
       "           2.5643]]),\n",
       " tensor([[-1.0157, -0.0146, -0.6000,  2.4048, -0.5754, -2.6515,  0.7216,\n",
       "          -0.2063,  1.0357,  1.6486, -0.7901, -2.1557, -0.2768,  3.7416,\n",
       "          -0.4291,  1.6540, -1.9081, -2.9768,  1.4668,  1.9516, -0.5105,\n",
       "           1.0243,  2.1418, -1.2712, -2.0303, -0.6958,  2.6944,  1.9736,\n",
       "           1.3046]]),\n",
       " tensor([[ 1.7273,  2.3632, -2.8282, -1.4808, -1.7233, -1.5329, -2.2199,\n",
       "           0.6401,  0.9238,  2.3935,  1.3863, -2.8277,  3.0554,  3.1768,\n",
       "           0.8830,  0.9068,  1.6003,  2.7229, -1.1842,  2.4693, -0.7506,\n",
       "           0.8923, -1.7384,  0.2533,  2.7746,  1.1443, -1.8870, -2.9999,\n",
       "          -0.7574]]),\n",
       " tensor([[ 2.5162, -1.5541, -0.8186, -1.7256,  1.3167,  2.0426, -1.0090,\n",
       "          -1.3497,  2.6407,  2.8755, -0.3681, -0.9866, -3.2877, -2.5968,\n",
       "          -2.8669, -0.9019,  0.1270, -0.3679,  0.2564, -3.0219, -0.7485,\n",
       "           2.0858,  1.2042,  1.6200, -1.7143, -0.4366, -0.1272, -1.1932,\n",
       "          -0.2355]]),\n",
       " tensor([[ 2.5072,  3.2135,  1.6165,  1.6990, -3.4072,  1.1425, -1.6073,\n",
       "          -1.8222,  2.6479, -0.2402,  3.0113,  2.2203, -1.1308, -2.3308,\n",
       "           3.3344, -2.0676,  0.9323,  0.5630, -1.7810, -2.5615, -0.3952,\n",
       "           3.3867,  1.1418, -2.4026, -2.6138,  1.5616,  3.4866, -0.5834,\n",
       "           0.6405]]),\n",
       " tensor([[-0.4343, -0.5792, -1.5424,  1.4883,  0.2517, -0.6367,  1.9800,\n",
       "          -1.1911,  1.5056, -0.7101,  2.3339, -1.1820, -1.9722, -2.5377,\n",
       "           0.1457,  1.7740,  1.3864, -1.8866, -2.3725, -3.6230, -1.5772,\n",
       "           1.9507,  0.6755,  1.5532,  2.5980,  2.2971,  0.3780,  2.1437,\n",
       "           2.9267]]),\n",
       " tensor([[ 1.4950, -1.2986, -1.3888,  1.8469,  1.7468, -2.5158, -0.4662,\n",
       "           3.0440, -2.6415, -0.0496, -1.3544, -2.6824, -2.4563,  0.0869,\n",
       "           0.9074,  0.6457, -1.6805, -0.3787,  2.5211,  0.8944, -0.8657,\n",
       "          -3.0386, -0.0128,  2.0231,  1.1688,  1.7878, -3.4778,  1.0264,\n",
       "          -3.5708]]),\n",
       " tensor([[ 3.2677, -1.0436,  0.9805,  0.1858, -2.4311, -2.2238,  3.4434,\n",
       "           1.4939, -2.1380,  1.8960,  1.0970,  3.7445,  3.0443, -3.6574,\n",
       "          -0.9338,  1.3900, -1.0293, -1.9567, -3.4362,  0.4428, -1.4750,\n",
       "          -3.7446, -1.9850, -1.0407,  0.0127, -0.5033,  0.3537,  1.4305,\n",
       "           0.1517]]),\n",
       " tensor([[ 3.0801, -3.8770, -3.2544,  2.9982, -1.6199,  2.3465, -3.4357,\n",
       "           0.9061, -1.9929,  1.9451, -2.2134,  1.3300,  0.9576, -3.8534,\n",
       "           1.5136,  1.4354, -1.3626,  0.0273, -3.1063, -1.8905, -1.1231,\n",
       "          -1.3831,  1.4200, -3.1209,  2.8716, -2.5627,  3.2296,  2.4110,\n",
       "          -0.1523]]),\n",
       " tensor([[ 0.1269, -0.1816, -0.9625, -2.6759, -1.1229,  2.3918, -1.1007,\n",
       "           2.6629, -3.5621, -0.4412,  0.8857,  1.3640,  0.8870, -1.3530,\n",
       "          -1.1682, -1.1442,  1.4848, -1.5999, -1.8685,  2.2507, -0.6782,\n",
       "          -1.7761,  0.1007,  1.8754, -0.7792,  2.9436,  1.4897, -2.5838,\n",
       "           0.8137]]),\n",
       " tensor([[ 0.5637, -3.0931,  2.3418, -0.1120,  3.4437, -1.2976,  0.4522,\n",
       "           2.7388,  0.3477,  2.9942,  0.6986, -0.3870, -2.7578, -2.6082,\n",
       "           1.2501, -2.7358, -0.8953,  2.6618, -0.2819,  0.9724,  1.2066,\n",
       "          -1.8850,  0.6263,  3.4022,  1.8651,  0.0788,  0.2711, -1.3944,\n",
       "           2.8659]]),\n",
       " tensor([[ 2.7441,  2.8738, -0.6336,  2.0195, -0.9201,  1.7978,  1.8412,\n",
       "          -1.5012, -2.7794,  1.3084, -1.4229, -0.2624, -1.8103, -2.0675,\n",
       "           1.7676, -1.2016,  2.9071,  3.1645, -0.6971,  0.1970, -3.5527,\n",
       "          -2.9518, -2.5773,  3.3704, -1.2303,  1.5603,  1.0945,  0.9835,\n",
       "           2.4629]]),\n",
       " tensor([[-0.2390,  0.7105,  2.6718,  0.5036,  3.3393, -2.7730, -0.0175,\n",
       "           0.6688, -1.6869, -1.2534, -2.7545, -1.7376, -3.2133,  0.6887,\n",
       "           0.2129, -3.6739,  0.4163,  3.0512, -1.5844,  2.5460, -2.1094,\n",
       "          -1.4528,  0.6411, -1.7295, -2.4298,  0.6131, -2.0699, -1.0581,\n",
       "           0.6468]]),\n",
       " tensor([[ 2.2857,  3.1049,  0.5822,  2.9985,  0.8008, -0.4041, -1.9981,\n",
       "          -0.7851, -1.5436, -2.0080, -1.4229, -1.2446, -0.3472,  2.2638,\n",
       "           1.1045,  3.0007,  0.3865, -3.1305,  3.4350, -0.1163,  2.7664,\n",
       "           0.3164,  2.8724,  0.4808,  1.7901,  2.0942,  3.0029,  0.9900,\n",
       "           2.2143]]),\n",
       " tensor([[ 1.4889, -2.4525, -0.5712, -2.7585, -1.9003,  0.0975,  3.1411,\n",
       "           0.0573,  3.4653, -2.1009, -1.1464,  1.2713, -0.8509,  3.3103,\n",
       "          -2.6668,  2.0640,  1.0428, -1.4989,  0.7413,  1.2642,  0.5403,\n",
       "           3.1611, -0.7573,  2.2780, -2.0284, -0.4673,  1.3011, -3.4654,\n",
       "           2.8303]]),\n",
       " tensor([[-2.3847, -1.2146, -1.1450,  1.8294,  0.8639, -0.5209, -2.6249,\n",
       "           0.3923, -1.1508,  3.0436, -1.4813,  0.7270,  0.2106,  0.7827,\n",
       "           1.5338, -1.3610, -1.2688, -1.2440,  0.2613,  2.1110,  1.5166,\n",
       "          -0.7697,  1.4487,  2.8977,  2.8012,  1.1915,  3.3353, -2.4905,\n",
       "           0.3906]]),\n",
       " tensor([[ 2.1418,  1.7572,  1.6112,  0.1788, -0.8332, -0.4212, -1.4374,\n",
       "           3.0270,  3.5234,  3.7845, -1.4685,  0.7515, -1.2912,  1.7831,\n",
       "          -3.4283,  1.8172,  1.9470, -1.2075, -2.0092, -2.8098, -0.8736,\n",
       "          -2.2503, -2.3196,  2.2137,  2.9628,  0.6220, -0.2963,  1.5031,\n",
       "          -0.8091]]),\n",
       " tensor([[-2.3772, -0.0781,  0.3381,  3.6789, -0.6685,  1.5337,  2.0869,\n",
       "          -0.5785,  1.4926,  0.4234, -0.1777, -0.3486, -1.9368,  1.3742,\n",
       "          -0.6897,  0.0262, -2.6613,  0.5770, -0.6179,  1.3259,  3.6260,\n",
       "          -0.5522,  2.8619,  0.0690,  1.6082,  1.3070, -0.1940, -1.5355,\n",
       "           1.0421]]),\n",
       " tensor([[ 0.7156,  0.0098, -1.0310,  2.3531, -2.9099,  0.7944,  0.7125,\n",
       "          -2.6160,  2.8087, -2.3246, -1.0128,  2.7550,  0.8836, -2.7437,\n",
       "          -0.4202,  1.8347,  1.9201, -1.9692, -0.4030, -0.3034,  3.0162,\n",
       "           0.0585, -3.1456, -0.7579,  2.9737, -1.5666, -2.6462, -3.2239,\n",
       "           2.5672]]),\n",
       " tensor([[ 0.5435, -2.5900,  0.4283,  3.3833, -3.9509, -2.5932,  0.1389,\n",
       "          -1.6690,  0.9688,  1.0419, -1.9529, -1.9935,  1.0371,  0.5437,\n",
       "          -1.0591,  0.4118, -1.9685,  2.2511, -3.3831,  1.3320,  2.7899,\n",
       "          -0.5327,  0.7592, -0.4702, -1.9096, -3.2342,  2.2135, -1.6739,\n",
       "           3.6772]]),\n",
       " tensor([[ 0.9597, -1.6503,  0.3242, -0.0400, -0.8621,  2.4500,  1.0327,\n",
       "          -1.6664, -0.6612,  2.2060,  2.1698, -1.5543,  0.9615, -0.7342,\n",
       "           1.4194,  1.7305, -3.1000, -1.7231, -2.0050, -2.3907,  3.0214,\n",
       "           3.4232,  1.4314,  0.8308,  2.3399,  1.6758, -0.9073,  2.6093,\n",
       "          -0.4868]]),\n",
       " tensor([[ 2.2561,  0.5734,  1.5147,  1.7176, -1.5488,  1.9721, -0.2917,\n",
       "          -2.7079, -2.8875, -2.7860, -3.1854, -2.7412, -1.6023, -1.4463,\n",
       "           2.7276,  1.9665, -2.0580,  2.0741, -3.3678,  1.1790, -0.3787,\n",
       "           2.6795, -2.3423,  1.2203,  2.6784,  1.9168,  1.9839, -2.4764,\n",
       "          -0.4997]]),\n",
       " tensor([[ 1.3293,  0.6732, -1.4058,  1.0498, -0.7911, -1.0116,  0.4189,\n",
       "           1.0682,  1.4026, -3.0249, -1.0312, -1.8095,  1.9855, -3.0591,\n",
       "           2.1999, -0.0708, -2.5434,  1.4178,  0.5262, -1.7669,  3.4529,\n",
       "          -2.9846, -1.5868, -0.3761, -2.2593, -1.9619,  3.2603, -2.6571,\n",
       "           2.5409]]),\n",
       " tensor([[-1.3241,  1.8193,  2.2193,  1.3046, -1.8362,  3.3783,  2.1551,\n",
       "           0.3590, -2.5570, -0.1366,  0.5175, -0.0441, -1.3620, -1.1338,\n",
       "          -1.6845,  2.6187, -0.9316,  0.5713,  1.5409,  1.7495, -1.2882,\n",
       "          -0.2236, -0.1278,  1.0592,  0.8799, -2.7213,  0.4358,  1.0929,\n",
       "           2.9084]]),\n",
       " tensor([[-2.7343,  1.1988,  1.4023,  3.4333,  1.1193, -2.3077, -3.4491,\n",
       "          -3.4288, -0.1353, -0.9204, -2.9442, -1.6462, -1.1428,  3.0678,\n",
       "          -1.5722,  0.0181, -2.9209, -1.2728,  2.1150,  1.3825,  0.2321,\n",
       "           2.3666,  0.2290, -2.7955,  2.6603,  2.5949, -0.5410, -0.7369,\n",
       "           0.5622]]),\n",
       " tensor([[-1.8654, -3.1354, -0.0833,  0.9696, -1.6494, -0.1858,  2.6714,\n",
       "          -0.1092,  0.4267,  3.6187,  1.2820, -0.0008,  0.6839, -1.3263,\n",
       "          -0.1141,  2.3349,  1.7249, -1.6635,  2.3962, -0.2337, -0.0123,\n",
       "           1.0494, -0.7164, -1.2472,  2.6660, -1.7530,  1.3318,  1.6559,\n",
       "           2.2038]]),\n",
       " tensor([[ 0.8269,  2.1552, -0.6979, -1.7045, -0.0975,  1.5980, -1.9451,\n",
       "          -2.0144,  2.6778,  1.2882, -1.7471, -2.8635,  1.3464,  0.7907,\n",
       "          -0.4870,  1.6369, -1.3577, -1.7570, -0.4605,  2.1604, -2.9756,\n",
       "          -0.7762,  2.9970, -2.6683,  1.2862,  0.7780,  1.5373, -1.4965,\n",
       "           1.6288]]),\n",
       " tensor([[ 2.5224, -1.9152, -1.5950,  3.4803, -1.5481,  2.0579,  1.4010,\n",
       "           0.2831, -2.7562, -1.7823,  1.4260, -1.2836,  1.2997,  1.5879,\n",
       "           0.1439, -0.2108, -1.6968,  0.9955,  1.9009,  0.5043,  3.0835,\n",
       "           1.2099,  1.9049,  0.5066,  0.9140,  2.2976,  0.4115,  1.7043,\n",
       "          -2.2381]]),\n",
       " tensor([[-2.0239,  0.4230, -1.8380, -2.0208, -0.8163,  3.2897,  1.4626,\n",
       "           1.3649, -0.2729,  0.2525,  0.2459, -0.3535, -1.9484, -2.5319,\n",
       "          -1.1211,  0.4821,  1.0363,  1.5234, -1.5675,  0.5735, -3.2634,\n",
       "          -1.7876, -1.4340,  0.3841, -3.4024,  2.6984, -0.4403, -2.0452,\n",
       "          -0.4175]]),\n",
       " tensor([[-2.4845,  1.9321, -1.2674, -1.0842, -0.2473, -1.6124,  0.9063,\n",
       "           2.6923, -2.1433, -2.9886, -2.4343,  0.2099, -2.4553,  3.1595,\n",
       "          -2.3634,  2.8192, -1.3498, -0.3212,  2.7076, -1.1722,  0.4896,\n",
       "          -0.8570,  0.4187, -3.1700,  2.8773,  2.3728, -3.5027, -1.3197,\n",
       "           2.0686]]),\n",
       " tensor([[-0.2152,  2.6952,  1.3926, -3.2851, -1.7812,  0.7887,  1.0182,\n",
       "          -2.2652, -1.9611,  1.1169,  1.9734, -1.7459, -1.0023,  1.6247,\n",
       "          -0.9881, -3.4420, -1.0883,  1.3589,  1.2497,  1.2379, -1.6878,\n",
       "           2.8725, -0.7472,  0.7873, -2.1574,  2.1434,  1.6374,  2.6459,\n",
       "          -1.7443]]),\n",
       " tensor([[-3.4834,  0.3115,  0.8868, -0.5309, -0.4020,  1.9933,  3.0711,\n",
       "          -1.2125,  2.0284,  1.5760,  1.2039,  0.3018,  0.1392,  1.8557,\n",
       "           2.8364, -1.2135,  2.3226,  1.3457, -2.1276, -0.1050, -3.1111,\n",
       "           1.3639, -2.7320,  1.1863, -0.2581, -1.1606,  3.2627, -0.5491,\n",
       "          -0.5026]]),\n",
       " tensor([[-0.9889,  1.5151,  0.2348, -0.7358, -1.2442, -0.9954, -1.5603,\n",
       "          -1.0775,  1.5772,  0.9228, -0.0439, -1.0759, -0.2900, -1.8709,\n",
       "           1.1278,  2.8634,  0.0139, -2.3354,  1.8849, -1.4870, -1.6868,\n",
       "           1.0906,  0.4813,  2.8213,  1.0327,  0.9193, -2.0791,  2.4288,\n",
       "           3.1803]]),\n",
       " tensor([[ 0.0705, -1.3667, -3.4037,  0.0464, -1.8057,  1.2424,  0.9587,\n",
       "           1.6114, -1.6036, -1.3919, -0.4671, -3.0147,  3.4358, -2.3468,\n",
       "           0.2188,  1.4825,  1.0224,  1.2892,  3.4569, -3.2784,  0.7872,\n",
       "           0.7977, -0.9575,  2.9263, -1.1096, -1.0204, -0.6761,  2.4492,\n",
       "           1.7266]]),\n",
       " tensor([[-0.8525, -0.0328,  2.7519,  0.5015,  2.3855, -2.6610,  1.7355,\n",
       "          -3.4089, -2.5344, -2.9680,  2.7366, -2.3173, -2.5011, -1.2240,\n",
       "          -2.4943,  0.5360, -0.2104, -2.3717, -2.4961, -2.2785, -0.7714,\n",
       "          -2.7500, -1.5674,  1.0770, -2.3956, -3.6776,  2.1225,  0.1746,\n",
       "           2.8559]]),\n",
       " tensor([[ 0.9944, -1.9137,  2.6523,  0.5499, -1.0513,  1.5925, -2.0292,\n",
       "          -0.1838,  1.8023, -0.2280, -0.7109, -3.4792,  0.9107,  1.5672,\n",
       "           3.4722, -2.2664,  1.3680, -2.8191, -1.6350, -2.6634,  2.7876,\n",
       "           2.5190, -1.9677, -1.6116, -0.1114,  1.3864, -1.9981,  1.9089,\n",
       "           2.6632]]),\n",
       " tensor([[ 0.9244,  1.4957, -1.1293, -1.9131, -2.4110,  2.7343, -2.4370,\n",
       "           1.1647, -2.6206, -1.2426, -1.7084,  3.4872,  0.3937,  1.9899,\n",
       "           2.5879, -1.6982,  0.8625,  1.6131, -2.5145, -1.6883, -0.0224,\n",
       "          -2.9669, -1.9682,  0.9094,  2.8895,  1.2895, -1.6475, -2.0886,\n",
       "           1.4900]]),\n",
       " tensor([[ 2.4043, -0.2720, -2.5052,  1.1039,  3.1862,  2.2050,  0.1832,\n",
       "           0.9643,  0.6701,  0.7419,  0.6511,  1.8351,  2.8057,  1.5983,\n",
       "          -0.8612,  0.6474,  1.2215, -0.6573, -3.0866, -2.2769,  3.3410,\n",
       "           0.3520, -1.6730, -1.0991,  0.7145,  1.3293,  0.6602, -2.1520,\n",
       "          -1.1616]]),\n",
       " tensor([[ 0.1771,  3.1424,  0.5849, -0.5908,  3.2828, -0.6422, -0.3301,\n",
       "           0.5019, -1.3151,  3.0883,  2.7388,  2.3555,  1.6142,  2.4885,\n",
       "          -1.0145, -2.4118,  0.3277,  2.3122, -0.5329, -1.5096, -1.5509,\n",
       "           1.5308, -1.2153,  2.5108, -2.6579, -2.6385, -2.9382,  0.6244,\n",
       "           3.3569]]),\n",
       " tensor([[-2.5763,  3.0956,  2.0067, -3.6823, -3.3301, -2.0461,  0.4748,\n",
       "           1.4530, -1.3043, -0.7214,  1.9284,  0.8462, -1.5120,  1.8884,\n",
       "          -2.5329,  1.2096,  2.7222,  0.9468,  1.3571,  1.6809, -0.1651,\n",
       "          -2.8427, -2.0001,  0.2080, -1.3824,  2.1802,  2.7079, -0.3617,\n",
       "          -1.6586]]),\n",
       " tensor([[ 1.2017, -1.0222,  0.8895, -2.8211, -0.9050, -3.6249,  0.1543,\n",
       "          -1.4510, -1.2333,  3.1662, -0.8044, -1.7581,  0.4984,  2.0422,\n",
       "           0.0694, -3.2414, -0.3764, -1.3417, -1.1403,  0.7316,  1.1629,\n",
       "          -3.4214,  1.4406, -2.1653,  2.1290,  2.6022, -3.1871,  2.0151,\n",
       "           0.9413]]),\n",
       " tensor([[-3.5424,  0.7813, -2.8325, -1.5061,  1.2542, -0.2031, -2.1381,\n",
       "           1.5991,  0.1179, -1.4790,  3.1742,  2.6720, -0.8044,  1.4114,\n",
       "          -0.3713, -0.5119, -2.1938, -0.3943, -1.8273, -2.0987, -2.7890,\n",
       "          -0.0594,  2.5078, -2.1471, -1.2906,  1.2226, -2.8735, -1.0748,\n",
       "           1.2688]]),\n",
       " tensor([[ 0.2323, -0.7660, -2.2806, -1.9645, -3.2879,  1.0500,  1.4732,\n",
       "           0.9975, -2.4628, -1.0007, -2.4713,  3.2198,  1.4821, -0.9100,\n",
       "           1.1214, -2.5775,  0.0665, -0.3167, -1.6166,  2.9834, -0.2068,\n",
       "           1.5068, -0.7481,  1.7473,  2.3046,  1.4460, -3.4246, -3.3397,\n",
       "          -1.5651]]),\n",
       " tensor([[-0.1773,  0.5841,  0.7770,  1.6180, -2.0748, -1.1483, -0.5595,\n",
       "           1.3792,  0.0918,  1.6969,  2.0174, -3.6351, -1.2986, -0.9672,\n",
       "          -3.1499, -2.6073,  2.0362, -0.3877, -2.5374, -2.6768, -1.8525,\n",
       "          -0.4942,  0.2057, -3.4280, -0.8358, -1.7412, -2.5016,  0.8423,\n",
       "          -2.9682]]),\n",
       " tensor([[ 1.5448,  1.7047,  3.4332,  1.5622,  0.4473,  3.0015,  1.3267,\n",
       "          -1.0091, -2.7751, -1.2459,  3.2124, -1.7673, -2.8814,  2.5051,\n",
       "           3.0157, -2.2442, -2.3195, -0.1894, -1.9824,  0.9398, -0.4482,\n",
       "          -1.4778, -2.6651,  0.1674,  1.2800,  1.5848,  3.3948,  1.8234,\n",
       "          -3.1522]]),\n",
       " tensor([[-1.9235, -1.0549, -3.7469,  2.3346,  1.2293,  2.8919,  1.0233,\n",
       "          -2.5322,  1.3367, -1.3485, -1.5259, -1.0248,  3.5573, -2.3656,\n",
       "           2.6493,  1.1145,  1.1605,  3.3515,  1.7348, -1.9764,  1.5017,\n",
       "          -2.7830, -1.4115,  1.8549, -0.9108,  1.9126,  0.3048,  2.8296,\n",
       "          -1.9887]]),\n",
       " tensor([[-0.2459, -0.2696, -2.6313, -2.1329, -3.4841, -2.7836,  0.1689,\n",
       "          -0.1404, -1.9233, -0.8888,  2.5446, -1.5026,  2.2396, -2.0900,\n",
       "           2.3514, -3.2250, -1.8119, -2.5549,  1.9623,  2.9596,  1.5304,\n",
       "          -1.5247, -3.3671, -1.1611,  1.0416, -0.8550,  0.5644,  1.5255,\n",
       "           1.5380]]),\n",
       " tensor([[-0.1177, -2.0241, -0.0141,  2.4908,  0.4017,  2.8435, -2.7840,\n",
       "           2.2809,  2.0561, -1.8300, -2.0402,  0.8160, -1.3270, -3.0256,\n",
       "           2.6413, -3.3974, -1.3675, -0.8198,  2.5739,  2.5351,  1.5000,\n",
       "          -0.1074, -0.6117,  2.7105,  0.3826, -0.4231, -1.0544, -2.7288,\n",
       "           1.2862]]),\n",
       " tensor([[-2.8452, -3.9778,  3.3363,  0.2324,  0.6484,  2.8152,  2.7984,\n",
       "          -2.7159,  3.6371,  1.0166,  2.0707,  0.8221, -2.0035, -2.9269,\n",
       "          -0.9658,  2.8381,  1.8288, -1.9366, -0.9637,  0.7718, -0.7383,\n",
       "           0.3313,  0.8770, -1.7013,  2.9807,  0.8790, -0.4178,  0.8875,\n",
       "           0.4577]]),\n",
       " tensor([[ 2.9055, -1.2130, -0.4731,  3.6928,  0.9373,  1.8850,  2.0685,\n",
       "           0.9370,  3.2142, -2.9214,  0.9908,  0.2297,  1.3907,  2.7381,\n",
       "          -2.1899, -1.3224,  2.1915, -1.5523, -1.4432, -0.1383,  0.0287,\n",
       "           0.1824,  3.4663,  0.3400, -0.2424, -0.0671,  1.6611, -1.0680,\n",
       "          -2.7965]]),\n",
       " tensor([[ 3.5556, -2.5196,  1.8340, -3.8578, -0.9944,  1.7700, -1.0014,\n",
       "          -1.8572, -0.6846,  1.6922, -2.4857,  2.4387,  0.0088,  3.5029,\n",
       "           0.8025, -0.6217,  1.6886,  2.9134,  1.3258,  1.8363,  2.0048,\n",
       "          -1.7967,  0.3157,  1.6089,  1.2517,  2.8949,  0.0500, -1.6821,\n",
       "           1.6100]]),\n",
       " tensor([[ 1.5360,  2.1195, -0.9487,  1.2306,  0.1247,  1.6265, -1.8932,\n",
       "          -2.5185, -2.2414, -3.6145, -3.3054, -1.0849,  0.1598, -3.0548,\n",
       "          -1.0987, -0.5240, -1.2674,  0.7385, -2.3406,  3.4867, -2.5410,\n",
       "          -2.7007, -1.1759,  1.5791, -1.9382, -2.4152, -2.8505, -1.1377,\n",
       "           2.9704]]),\n",
       " tensor([[-0.0867,  2.2254, -1.8161, -3.7284,  1.1796, -0.2372,  2.0494,\n",
       "           2.3405, -2.9448,  0.0190, -2.6672,  2.9132, -0.9467, -2.6000,\n",
       "           1.5852,  2.3837,  1.4376,  3.4792,  1.8173,  3.0469, -0.2353,\n",
       "           1.2180,  1.8620, -1.3352, -2.5833, -2.6701, -2.1983, -1.9103,\n",
       "           3.3591]]),\n",
       " tensor([[-3.2352,  0.1308, -0.7089, -3.2466,  0.5843, -2.9034,  0.9197,\n",
       "          -1.1674,  0.5710,  1.4944, -1.3863, -3.1258,  3.1879, -0.2849,\n",
       "          -2.1715,  2.7219, -1.0125, -2.8983, -2.7474, -0.4288,  0.4421,\n",
       "          -1.7932,  0.7828,  0.8437, -1.8343,  2.2133, -0.7987,  2.6898,\n",
       "          -2.2242]])]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can train and test model on the generated data\n",
    "synthentic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert array of tensor to tensors\n",
    "temp = torch.Tensor(99)\n",
    "synthentic_data = torch.cat(synthentic_data, out=temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.878369</td>\n",
       "      <td>1.166729</td>\n",
       "      <td>1.267083</td>\n",
       "      <td>-1.719918</td>\n",
       "      <td>-2.501470</td>\n",
       "      <td>-1.223442</td>\n",
       "      <td>-2.420864</td>\n",
       "      <td>-1.035397</td>\n",
       "      <td>-3.435341</td>\n",
       "      <td>0.930589</td>\n",
       "      <td>...</td>\n",
       "      <td>1.513909</td>\n",
       "      <td>1.257708</td>\n",
       "      <td>0.833932</td>\n",
       "      <td>2.686128</td>\n",
       "      <td>2.138771</td>\n",
       "      <td>1.554884</td>\n",
       "      <td>-3.656888</td>\n",
       "      <td>-2.150606</td>\n",
       "      <td>-0.307752</td>\n",
       "      <td>1.369795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.286283</td>\n",
       "      <td>1.777114</td>\n",
       "      <td>-1.621855</td>\n",
       "      <td>0.694037</td>\n",
       "      <td>-1.939506</td>\n",
       "      <td>1.865593</td>\n",
       "      <td>2.548972</td>\n",
       "      <td>2.667264</td>\n",
       "      <td>0.510648</td>\n",
       "      <td>-2.824627</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.671729</td>\n",
       "      <td>-0.038235</td>\n",
       "      <td>1.528785</td>\n",
       "      <td>1.532033</td>\n",
       "      <td>3.736961</td>\n",
       "      <td>-1.838274</td>\n",
       "      <td>1.075052</td>\n",
       "      <td>-1.698538</td>\n",
       "      <td>-0.101544</td>\n",
       "      <td>0.965199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.745649</td>\n",
       "      <td>1.226942</td>\n",
       "      <td>-2.287163</td>\n",
       "      <td>1.729961</td>\n",
       "      <td>-0.681605</td>\n",
       "      <td>-2.184601</td>\n",
       "      <td>1.033023</td>\n",
       "      <td>-0.904403</td>\n",
       "      <td>-3.397020</td>\n",
       "      <td>2.065704</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.114003</td>\n",
       "      <td>2.532003</td>\n",
       "      <td>3.173552</td>\n",
       "      <td>1.961705</td>\n",
       "      <td>-1.091195</td>\n",
       "      <td>-1.456010</td>\n",
       "      <td>-1.217667</td>\n",
       "      <td>-0.711793</td>\n",
       "      <td>-2.361647</td>\n",
       "      <td>-1.645972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.111560</td>\n",
       "      <td>-1.217564</td>\n",
       "      <td>-1.886643</td>\n",
       "      <td>2.569919</td>\n",
       "      <td>-2.636978</td>\n",
       "      <td>2.728637</td>\n",
       "      <td>-3.263298</td>\n",
       "      <td>-0.754449</td>\n",
       "      <td>-1.224119</td>\n",
       "      <td>3.502965</td>\n",
       "      <td>...</td>\n",
       "      <td>2.461310</td>\n",
       "      <td>0.080884</td>\n",
       "      <td>-1.185353</td>\n",
       "      <td>-2.380249</td>\n",
       "      <td>-1.344216</td>\n",
       "      <td>-2.075672</td>\n",
       "      <td>2.546557</td>\n",
       "      <td>1.747474</td>\n",
       "      <td>-1.464104</td>\n",
       "      <td>0.197886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.514295</td>\n",
       "      <td>-0.932959</td>\n",
       "      <td>2.505473</td>\n",
       "      <td>2.137690</td>\n",
       "      <td>-0.854857</td>\n",
       "      <td>-2.237430</td>\n",
       "      <td>-0.544429</td>\n",
       "      <td>1.898578</td>\n",
       "      <td>-2.834359</td>\n",
       "      <td>0.554754</td>\n",
       "      <td>...</td>\n",
       "      <td>2.827179</td>\n",
       "      <td>-1.917193</td>\n",
       "      <td>0.445887</td>\n",
       "      <td>1.917602</td>\n",
       "      <td>-3.724737</td>\n",
       "      <td>0.642127</td>\n",
       "      <td>0.980936</td>\n",
       "      <td>0.357832</td>\n",
       "      <td>-0.678436</td>\n",
       "      <td>-0.075131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  1.878369  1.166729  1.267083 -1.719918 -2.501470 -1.223442 -2.420864   \n",
       "1  3.286283  1.777114 -1.621855  0.694037 -1.939506  1.865593  2.548972   \n",
       "2 -1.745649  1.226942 -2.287163  1.729961 -0.681605 -2.184601  1.033023   \n",
       "3  2.111560 -1.217564 -1.886643  2.569919 -2.636978  2.728637 -3.263298   \n",
       "4  1.514295 -0.932959  2.505473  2.137690 -0.854857 -2.237430 -0.544429   \n",
       "\n",
       "         7         8         9     ...           19        20        21  \\\n",
       "0 -1.035397 -3.435341  0.930589    ...     1.513909  1.257708  0.833932   \n",
       "1  2.667264  0.510648 -2.824627    ...    -1.671729 -0.038235  1.528785   \n",
       "2 -0.904403 -3.397020  2.065704    ...    -4.114003  2.532003  3.173552   \n",
       "3 -0.754449 -1.224119  3.502965    ...     2.461310  0.080884 -1.185353   \n",
       "4  1.898578 -2.834359  0.554754    ...     2.827179 -1.917193  0.445887   \n",
       "\n",
       "         22        23        24        25        26        27        28  \n",
       "0  2.686128  2.138771  1.554884 -3.656888 -2.150606 -0.307752  1.369795  \n",
       "1  1.532033  3.736961 -1.838274  1.075052 -1.698538 -0.101544  0.965199  \n",
       "2  1.961705 -1.091195 -1.456010 -1.217667 -0.711793 -2.361647 -1.645972  \n",
       "3 -2.380249 -1.344216 -2.075672  2.546557  1.747474 -1.464104  0.197886  \n",
       "4  1.917602 -3.724737  0.642127  0.980936  0.357832 -0.678436 -0.075131  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tensor to PD data frame\n",
    "synthentic_data_df = pd.DataFrame(data=synthentic_data.data.numpy());\n",
    "synthentic_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns of the synthentic dataset\n",
    "cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "        'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount']\n",
    "synthentic_data_df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.878369</td>\n",
       "      <td>1.166729</td>\n",
       "      <td>1.267083</td>\n",
       "      <td>-1.719918</td>\n",
       "      <td>-2.501470</td>\n",
       "      <td>-1.223442</td>\n",
       "      <td>-2.420864</td>\n",
       "      <td>-1.035397</td>\n",
       "      <td>-3.435341</td>\n",
       "      <td>0.930589</td>\n",
       "      <td>...</td>\n",
       "      <td>1.513909</td>\n",
       "      <td>1.257708</td>\n",
       "      <td>0.833932</td>\n",
       "      <td>2.686128</td>\n",
       "      <td>2.138771</td>\n",
       "      <td>1.554884</td>\n",
       "      <td>-3.656888</td>\n",
       "      <td>-2.150606</td>\n",
       "      <td>-0.307752</td>\n",
       "      <td>1.369795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.286283</td>\n",
       "      <td>1.777114</td>\n",
       "      <td>-1.621855</td>\n",
       "      <td>0.694037</td>\n",
       "      <td>-1.939506</td>\n",
       "      <td>1.865593</td>\n",
       "      <td>2.548972</td>\n",
       "      <td>2.667264</td>\n",
       "      <td>0.510648</td>\n",
       "      <td>-2.824627</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.671729</td>\n",
       "      <td>-0.038235</td>\n",
       "      <td>1.528785</td>\n",
       "      <td>1.532033</td>\n",
       "      <td>3.736961</td>\n",
       "      <td>-1.838274</td>\n",
       "      <td>1.075052</td>\n",
       "      <td>-1.698538</td>\n",
       "      <td>-0.101544</td>\n",
       "      <td>0.965199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.745649</td>\n",
       "      <td>1.226942</td>\n",
       "      <td>-2.287163</td>\n",
       "      <td>1.729961</td>\n",
       "      <td>-0.681605</td>\n",
       "      <td>-2.184601</td>\n",
       "      <td>1.033023</td>\n",
       "      <td>-0.904403</td>\n",
       "      <td>-3.397020</td>\n",
       "      <td>2.065704</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.114003</td>\n",
       "      <td>2.532003</td>\n",
       "      <td>3.173552</td>\n",
       "      <td>1.961705</td>\n",
       "      <td>-1.091195</td>\n",
       "      <td>-1.456010</td>\n",
       "      <td>-1.217667</td>\n",
       "      <td>-0.711793</td>\n",
       "      <td>-2.361647</td>\n",
       "      <td>-1.645972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.111560</td>\n",
       "      <td>-1.217564</td>\n",
       "      <td>-1.886643</td>\n",
       "      <td>2.569919</td>\n",
       "      <td>-2.636978</td>\n",
       "      <td>2.728637</td>\n",
       "      <td>-3.263298</td>\n",
       "      <td>-0.754449</td>\n",
       "      <td>-1.224119</td>\n",
       "      <td>3.502965</td>\n",
       "      <td>...</td>\n",
       "      <td>2.461310</td>\n",
       "      <td>0.080884</td>\n",
       "      <td>-1.185353</td>\n",
       "      <td>-2.380249</td>\n",
       "      <td>-1.344216</td>\n",
       "      <td>-2.075672</td>\n",
       "      <td>2.546557</td>\n",
       "      <td>1.747474</td>\n",
       "      <td>-1.464104</td>\n",
       "      <td>0.197886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.514295</td>\n",
       "      <td>-0.932959</td>\n",
       "      <td>2.505473</td>\n",
       "      <td>2.137690</td>\n",
       "      <td>-0.854857</td>\n",
       "      <td>-2.237430</td>\n",
       "      <td>-0.544429</td>\n",
       "      <td>1.898578</td>\n",
       "      <td>-2.834359</td>\n",
       "      <td>0.554754</td>\n",
       "      <td>...</td>\n",
       "      <td>2.827179</td>\n",
       "      <td>-1.917193</td>\n",
       "      <td>0.445887</td>\n",
       "      <td>1.917602</td>\n",
       "      <td>-3.724737</td>\n",
       "      <td>0.642127</td>\n",
       "      <td>0.980936</td>\n",
       "      <td>0.357832</td>\n",
       "      <td>-0.678436</td>\n",
       "      <td>-0.075131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  1.878369  1.166729  1.267083 -1.719918 -2.501470 -1.223442 -2.420864   \n",
       "1  3.286283  1.777114 -1.621855  0.694037 -1.939506  1.865593  2.548972   \n",
       "2 -1.745649  1.226942 -2.287163  1.729961 -0.681605 -2.184601  1.033023   \n",
       "3  2.111560 -1.217564 -1.886643  2.569919 -2.636978  2.728637 -3.263298   \n",
       "4  1.514295 -0.932959  2.505473  2.137690 -0.854857 -2.237430 -0.544429   \n",
       "\n",
       "         V8        V9       V10     ...           V20       V21       V22  \\\n",
       "0 -1.035397 -3.435341  0.930589     ...      1.513909  1.257708  0.833932   \n",
       "1  2.667264  0.510648 -2.824627     ...     -1.671729 -0.038235  1.528785   \n",
       "2 -0.904403 -3.397020  2.065704     ...     -4.114003  2.532003  3.173552   \n",
       "3 -0.754449 -1.224119  3.502965     ...      2.461310  0.080884 -1.185353   \n",
       "4  1.898578 -2.834359  0.554754     ...      2.827179 -1.917193  0.445887   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  normAmount  \n",
       "0  2.686128  2.138771  1.554884 -3.656888 -2.150606 -0.307752    1.369795  \n",
       "1  1.532033  3.736961 -1.838274  1.075052 -1.698538 -0.101544    0.965199  \n",
       "2  1.961705 -1.091195 -1.456010 -1.217667 -0.711793 -2.361647   -1.645972  \n",
       "3 -2.380249 -1.344216 -2.075672  2.546557  1.747474 -1.464104    0.197886  \n",
       "4  1.917602 -3.724737  0.642127  0.980936  0.357832 -0.678436   -0.075131  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tensor to PD data frame\n",
    "#synthentic_data_df = pd.DataFrame(data=synthentic_data.data.numpy());\n",
    "synthentic_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 1 to Class column since they're all synthetic generated fraud data\n",
    "synthentic_data_df['Class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normAmount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.878369</td>\n",
       "      <td>1.166729</td>\n",
       "      <td>1.267083</td>\n",
       "      <td>-1.719918</td>\n",
       "      <td>-2.501470</td>\n",
       "      <td>-1.223442</td>\n",
       "      <td>-2.420864</td>\n",
       "      <td>-1.035397</td>\n",
       "      <td>-3.435341</td>\n",
       "      <td>0.930589</td>\n",
       "      <td>...</td>\n",
       "      <td>1.257708</td>\n",
       "      <td>0.833932</td>\n",
       "      <td>2.686128</td>\n",
       "      <td>2.138771</td>\n",
       "      <td>1.554884</td>\n",
       "      <td>-3.656888</td>\n",
       "      <td>-2.150606</td>\n",
       "      <td>-0.307752</td>\n",
       "      <td>1.369795</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.286283</td>\n",
       "      <td>1.777114</td>\n",
       "      <td>-1.621855</td>\n",
       "      <td>0.694037</td>\n",
       "      <td>-1.939506</td>\n",
       "      <td>1.865593</td>\n",
       "      <td>2.548972</td>\n",
       "      <td>2.667264</td>\n",
       "      <td>0.510648</td>\n",
       "      <td>-2.824627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038235</td>\n",
       "      <td>1.528785</td>\n",
       "      <td>1.532033</td>\n",
       "      <td>3.736961</td>\n",
       "      <td>-1.838274</td>\n",
       "      <td>1.075052</td>\n",
       "      <td>-1.698538</td>\n",
       "      <td>-0.101544</td>\n",
       "      <td>0.965199</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.745649</td>\n",
       "      <td>1.226942</td>\n",
       "      <td>-2.287163</td>\n",
       "      <td>1.729961</td>\n",
       "      <td>-0.681605</td>\n",
       "      <td>-2.184601</td>\n",
       "      <td>1.033023</td>\n",
       "      <td>-0.904403</td>\n",
       "      <td>-3.397020</td>\n",
       "      <td>2.065704</td>\n",
       "      <td>...</td>\n",
       "      <td>2.532003</td>\n",
       "      <td>3.173552</td>\n",
       "      <td>1.961705</td>\n",
       "      <td>-1.091195</td>\n",
       "      <td>-1.456010</td>\n",
       "      <td>-1.217667</td>\n",
       "      <td>-0.711793</td>\n",
       "      <td>-2.361647</td>\n",
       "      <td>-1.645972</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.111560</td>\n",
       "      <td>-1.217564</td>\n",
       "      <td>-1.886643</td>\n",
       "      <td>2.569919</td>\n",
       "      <td>-2.636978</td>\n",
       "      <td>2.728637</td>\n",
       "      <td>-3.263298</td>\n",
       "      <td>-0.754449</td>\n",
       "      <td>-1.224119</td>\n",
       "      <td>3.502965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080884</td>\n",
       "      <td>-1.185353</td>\n",
       "      <td>-2.380249</td>\n",
       "      <td>-1.344216</td>\n",
       "      <td>-2.075672</td>\n",
       "      <td>2.546557</td>\n",
       "      <td>1.747474</td>\n",
       "      <td>-1.464104</td>\n",
       "      <td>0.197886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.514295</td>\n",
       "      <td>-0.932959</td>\n",
       "      <td>2.505473</td>\n",
       "      <td>2.137690</td>\n",
       "      <td>-0.854857</td>\n",
       "      <td>-2.237430</td>\n",
       "      <td>-0.544429</td>\n",
       "      <td>1.898578</td>\n",
       "      <td>-2.834359</td>\n",
       "      <td>0.554754</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.917193</td>\n",
       "      <td>0.445887</td>\n",
       "      <td>1.917602</td>\n",
       "      <td>-3.724737</td>\n",
       "      <td>0.642127</td>\n",
       "      <td>0.980936</td>\n",
       "      <td>0.357832</td>\n",
       "      <td>-0.678436</td>\n",
       "      <td>-0.075131</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  1.878369  1.166729  1.267083 -1.719918 -2.501470 -1.223442 -2.420864   \n",
       "1  3.286283  1.777114 -1.621855  0.694037 -1.939506  1.865593  2.548972   \n",
       "2 -1.745649  1.226942 -2.287163  1.729961 -0.681605 -2.184601  1.033023   \n",
       "3  2.111560 -1.217564 -1.886643  2.569919 -2.636978  2.728637 -3.263298   \n",
       "4  1.514295 -0.932959  2.505473  2.137690 -0.854857 -2.237430 -0.544429   \n",
       "\n",
       "         V8        V9       V10  ...         V21       V22       V23  \\\n",
       "0 -1.035397 -3.435341  0.930589  ...    1.257708  0.833932  2.686128   \n",
       "1  2.667264  0.510648 -2.824627  ...   -0.038235  1.528785  1.532033   \n",
       "2 -0.904403 -3.397020  2.065704  ...    2.532003  3.173552  1.961705   \n",
       "3 -0.754449 -1.224119  3.502965  ...    0.080884 -1.185353 -2.380249   \n",
       "4  1.898578 -2.834359  0.554754  ...   -1.917193  0.445887  1.917602   \n",
       "\n",
       "        V24       V25       V26       V27       V28  normAmount  Class  \n",
       "0  2.138771  1.554884 -3.656888 -2.150606 -0.307752    1.369795      1  \n",
       "1  3.736961 -1.838274  1.075052 -1.698538 -0.101544    0.965199      1  \n",
       "2 -1.091195 -1.456010 -1.217667 -0.711793 -2.361647   -1.645972      1  \n",
       "3 -1.344216 -2.075672  2.546557  1.747474 -1.464104    0.197886      1  \n",
       "4 -3.724737  0.642127  0.980936  0.357832 -0.678436   -0.075131      1  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthentic_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "data = data.drop(['Time','Amount'],axis=1)\n",
    "\n",
    "# Rearrange columns to the right order\n",
    "cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "        'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount', 'Class']\n",
    "real_fraud_data = data.loc[data['Class']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>-2.312227</td>\n",
       "      <td>1.951992</td>\n",
       "      <td>-1.609851</td>\n",
       "      <td>3.997906</td>\n",
       "      <td>-0.522188</td>\n",
       "      <td>-1.426545</td>\n",
       "      <td>-2.537387</td>\n",
       "      <td>1.391657</td>\n",
       "      <td>-2.770089</td>\n",
       "      <td>-2.772272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517232</td>\n",
       "      <td>-0.035049</td>\n",
       "      <td>-0.465211</td>\n",
       "      <td>0.320198</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>0.177840</td>\n",
       "      <td>0.261145</td>\n",
       "      <td>-0.143276</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.353229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>-0.838587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>1</td>\n",
       "      <td>1.761758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>-2.303350</td>\n",
       "      <td>1.759247</td>\n",
       "      <td>-0.359745</td>\n",
       "      <td>2.330243</td>\n",
       "      <td>-0.821628</td>\n",
       "      <td>-0.075788</td>\n",
       "      <td>0.562320</td>\n",
       "      <td>-0.399147</td>\n",
       "      <td>-0.238253</td>\n",
       "      <td>-1.525412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294166</td>\n",
       "      <td>-0.932391</td>\n",
       "      <td>0.172726</td>\n",
       "      <td>-0.087330</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>-0.542628</td>\n",
       "      <td>0.039566</td>\n",
       "      <td>-0.153029</td>\n",
       "      <td>1</td>\n",
       "      <td>0.606031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6108</th>\n",
       "      <td>-4.397974</td>\n",
       "      <td>1.358367</td>\n",
       "      <td>-2.592844</td>\n",
       "      <td>2.679787</td>\n",
       "      <td>-1.128131</td>\n",
       "      <td>-1.706536</td>\n",
       "      <td>-3.496197</td>\n",
       "      <td>-0.248778</td>\n",
       "      <td>-0.247768</td>\n",
       "      <td>-4.801637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573574</td>\n",
       "      <td>0.176968</td>\n",
       "      <td>-0.436207</td>\n",
       "      <td>-0.053502</td>\n",
       "      <td>0.252405</td>\n",
       "      <td>-0.657488</td>\n",
       "      <td>-0.827136</td>\n",
       "      <td>0.849573</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.117342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6329</th>\n",
       "      <td>1.234235</td>\n",
       "      <td>3.019740</td>\n",
       "      <td>-4.304597</td>\n",
       "      <td>4.732795</td>\n",
       "      <td>3.624201</td>\n",
       "      <td>-1.357746</td>\n",
       "      <td>1.713445</td>\n",
       "      <td>-0.496358</td>\n",
       "      <td>-1.282858</td>\n",
       "      <td>-2.447469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379068</td>\n",
       "      <td>-0.704181</td>\n",
       "      <td>-0.656805</td>\n",
       "      <td>-1.632653</td>\n",
       "      <td>1.488901</td>\n",
       "      <td>0.566797</td>\n",
       "      <td>-0.010016</td>\n",
       "      <td>0.146793</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.349231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            V1        V2        V3        V4        V5        V6        V7  \\\n",
       "541  -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545 -2.537387   \n",
       "623  -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823  0.325574   \n",
       "4920 -2.303350  1.759247 -0.359745  2.330243 -0.821628 -0.075788  0.562320   \n",
       "6108 -4.397974  1.358367 -2.592844  2.679787 -1.128131 -1.706536 -3.496197   \n",
       "6329  1.234235  3.019740 -4.304597  4.732795  3.624201 -1.357746  1.713445   \n",
       "\n",
       "            V8        V9       V10     ...           V21       V22       V23  \\\n",
       "541   1.391657 -2.770089 -2.772272     ...      0.517232 -0.035049 -0.465211   \n",
       "623  -0.067794 -0.270953 -0.838587     ...      0.661696  0.435477  1.375966   \n",
       "4920 -0.399147 -0.238253 -1.525412     ...     -0.294166 -0.932391  0.172726   \n",
       "6108 -0.248778 -0.247768 -4.801637     ...      0.573574  0.176968 -0.436207   \n",
       "6329 -0.496358 -1.282858 -2.447469     ...     -0.379068 -0.704181 -0.656805   \n",
       "\n",
       "           V24       V25       V26       V27       V28  Class  normAmount  \n",
       "541   0.320198  0.044519  0.177840  0.261145 -0.143276      1   -0.353229  \n",
       "623  -0.293803  0.279798 -0.145362 -0.252773  0.035764      1    1.761758  \n",
       "4920 -0.087330 -0.156114 -0.542628  0.039566 -0.153029      1    0.606031  \n",
       "6108 -0.053502  0.252405 -0.657488 -0.827136  0.849573      1   -0.117342  \n",
       "6329 -1.632653  1.488901  0.566797 -0.010016  0.146793      1   -0.349231  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_fraud_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat old fraud data and newly generated fraud data\n",
    "new_data = pd.concat([real_fraud_data, synthentic_data_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
