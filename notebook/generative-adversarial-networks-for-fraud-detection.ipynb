{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import recall_score\n",
    "import torch\n",
    "#from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quadro P5000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if Cuda is running\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomize the data, just to be sure not to get any pathological ordering effects that might harm the performane of Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124011</th>\n",
       "      <td>77147.0</td>\n",
       "      <td>-1.437314</td>\n",
       "      <td>-0.672735</td>\n",
       "      <td>0.925002</td>\n",
       "      <td>-0.728117</td>\n",
       "      <td>-0.498919</td>\n",
       "      <td>-1.088232</td>\n",
       "      <td>0.098312</td>\n",
       "      <td>0.294362</td>\n",
       "      <td>-1.763211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032172</td>\n",
       "      <td>-0.254662</td>\n",
       "      <td>0.569343</td>\n",
       "      <td>0.599137</td>\n",
       "      <td>-0.909906</td>\n",
       "      <td>0.070153</td>\n",
       "      <td>0.017838</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>149.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182926</th>\n",
       "      <td>125586.0</td>\n",
       "      <td>-1.188026</td>\n",
       "      <td>0.648003</td>\n",
       "      <td>0.195708</td>\n",
       "      <td>-2.300317</td>\n",
       "      <td>-1.876858</td>\n",
       "      <td>0.483442</td>\n",
       "      <td>-0.257970</td>\n",
       "      <td>-4.314653</td>\n",
       "      <td>1.394378</td>\n",
       "      <td>...</td>\n",
       "      <td>3.773234</td>\n",
       "      <td>-1.129681</td>\n",
       "      <td>-1.190132</td>\n",
       "      <td>0.066215</td>\n",
       "      <td>0.806729</td>\n",
       "      <td>0.669240</td>\n",
       "      <td>0.580496</td>\n",
       "      <td>0.090244</td>\n",
       "      <td>392.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>2364.0</td>\n",
       "      <td>1.571567</td>\n",
       "      <td>-0.852696</td>\n",
       "      <td>-0.411170</td>\n",
       "      <td>-1.735533</td>\n",
       "      <td>-0.574247</td>\n",
       "      <td>-0.298426</td>\n",
       "      <td>-0.606201</td>\n",
       "      <td>-0.165091</td>\n",
       "      <td>-2.384797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.546954</td>\n",
       "      <td>-1.269700</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>-0.896239</td>\n",
       "      <td>0.433003</td>\n",
       "      <td>-0.425964</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>-0.003500</td>\n",
       "      <td>19.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10848</th>\n",
       "      <td>18552.0</td>\n",
       "      <td>-0.265284</td>\n",
       "      <td>0.081207</td>\n",
       "      <td>1.595559</td>\n",
       "      <td>-1.671107</td>\n",
       "      <td>-0.359368</td>\n",
       "      <td>-0.158713</td>\n",
       "      <td>-0.188054</td>\n",
       "      <td>0.271057</td>\n",
       "      <td>2.710338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102560</td>\n",
       "      <td>0.673973</td>\n",
       "      <td>0.126516</td>\n",
       "      <td>-0.065758</td>\n",
       "      <td>-1.159539</td>\n",
       "      <td>-1.066871</td>\n",
       "      <td>0.283226</td>\n",
       "      <td>0.230268</td>\n",
       "      <td>11.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250966</th>\n",
       "      <td>155148.0</td>\n",
       "      <td>-1.071928</td>\n",
       "      <td>0.594211</td>\n",
       "      <td>-0.352817</td>\n",
       "      <td>-0.819417</td>\n",
       "      <td>1.439513</td>\n",
       "      <td>0.755137</td>\n",
       "      <td>0.594540</td>\n",
       "      <td>0.448423</td>\n",
       "      <td>0.063034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085680</td>\n",
       "      <td>0.088365</td>\n",
       "      <td>0.361190</td>\n",
       "      <td>-0.336159</td>\n",
       "      <td>-1.006584</td>\n",
       "      <td>0.176926</td>\n",
       "      <td>-0.098098</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>14.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "124011   77147.0 -1.437314 -0.672735  0.925002 -0.728117 -0.498919 -1.088232   \n",
       "182926  125586.0 -1.188026  0.648003  0.195708 -2.300317 -1.876858  0.483442   \n",
       "2807      2364.0  1.571567 -0.852696 -0.411170 -1.735533 -0.574247 -0.298426   \n",
       "10848    18552.0 -0.265284  0.081207  1.595559 -1.671107 -0.359368 -0.158713   \n",
       "250966  155148.0 -1.071928  0.594211 -0.352817 -0.819417  1.439513  0.755137   \n",
       "\n",
       "              V7        V8        V9  ...         V21       V22       V23  \\\n",
       "124011  0.098312  0.294362 -1.763211  ...   -0.032172 -0.254662  0.569343   \n",
       "182926 -0.257970 -4.314653  1.394378  ...    3.773234 -1.129681 -1.190132   \n",
       "2807   -0.606201 -0.165091 -2.384797  ...   -0.546954 -1.269700  0.009546   \n",
       "10848  -0.188054  0.271057  2.710338  ...    0.102560  0.673973  0.126516   \n",
       "250966  0.594540  0.448423  0.063034  ...   -0.085680  0.088365  0.361190   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "124011  0.599137 -0.909906  0.070153  0.017838  0.001861  149.20      0  \n",
       "182926  0.066215  0.806729  0.669240  0.580496  0.090244  392.00      0  \n",
       "2807   -0.896239  0.433003 -0.425964  0.000977 -0.003500   19.75      0  \n",
       "10848  -0.065758 -1.159539 -1.066871  0.283226  0.230268   11.85      0  \n",
       "250966 -0.336159 -1.006584  0.176926 -0.098098  0.001136   14.75      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.reindex(np.random.permutation(data.index))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.174225e-15</td>\n",
       "      <td>3.429687e-16</td>\n",
       "      <td>-1.386421e-15</td>\n",
       "      <td>2.073779e-15</td>\n",
       "      <td>9.939598e-16</td>\n",
       "      <td>1.493625e-15</td>\n",
       "      <td>-5.931037e-16</td>\n",
       "      <td>1.318317e-16</td>\n",
       "      <td>-2.414318e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.416845e-16</td>\n",
       "      <td>-3.515296e-16</td>\n",
       "      <td>2.727492e-16</td>\n",
       "      <td>4.482012e-15</td>\n",
       "      <td>5.203181e-16</td>\n",
       "      <td>1.689590e-15</td>\n",
       "      <td>-3.712632e-16</td>\n",
       "      <td>-1.159267e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.174225e-15  3.429687e-16 -1.386421e-15  2.073779e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.939598e-16  1.493625e-15 -5.931037e-16  1.318317e-16 -2.414318e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...        1.416845e-16 -3.515296e-16  2.727492e-16  4.482012e-15   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.203181e-16  1.689590e-15 -3.712632e-16 -1.159267e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Frequency')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = pd.value_counts(data['Class'], sort = True).sort_index()\n",
    "classes.plot(kind = 'bar')\n",
    "plt.title(\"Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Class')['Class'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Data is hihgly imbalance. 284315 Normal transaction vs 492 Fraud transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run with Normalising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns != 'Class']\n",
    "y = data.loc[:, data.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape\n",
      "(199364, 29)\n",
      "xtest shape\n",
      "(85443, 29)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "print('xtrain shape')\n",
    "print(X_train.shape)\n",
    "print('xtest shape')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the training data and test data\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit classifier to a model\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85291,     5],\n",
       "       [   39,   108]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     85296\n",
      "          1       0.96      0.73      0.83       147\n",
      "\n",
      "avg / total       1.00      1.00      1.00     85443\n",
      "\n",
      "Accuracy : 0.999485\n",
      "Area under the curve : 0.867318\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print('Accuracy : %f' % (metrics.accuracy_score(y_test, y_pred)))\n",
    "print('Area under the curve : %f' % (metrics.roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run with Over Sampling data using SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10     ...           V21       V22       V23  \\\n",
       "0  0.098698  0.363787  0.090794     ...     -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425 -0.166974     ...     -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654  0.207643     ...      0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024 -0.054952     ...     -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739  0.753074     ...     -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  normAmount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0    0.244964  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   -0.342475  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0    1.160686  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0    0.140534  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0   -0.073403  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "data = data.drop(['Time','Amount'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (284807, 29)\n",
      "Shape of y: (284807, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(data.ix[:, data.columns != 'Class'])\n",
    "y = np.array(data.ix[:, data.columns == 'Class'])\n",
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape\n",
      "(199364, 29)\n",
      "xtest shape\n",
      "(85443, 29)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print('xtrain shape')\n",
    "print(X_train.shape)\n",
    "print('xtest shape')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over Sampling data using SMOTE\n",
    "smote = SMOTE(random_state=2)\n",
    "X_train_resample, y_train_resample = smote.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train_resample, y_train_resample.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85283,    13],\n",
       "       [   31,   116]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a Confision Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     85296\n",
      "          1       0.90      0.79      0.84       147\n",
      "\n",
      "avg / total       1.00      1.00      1.00     85443\n",
      "\n",
      "Accuracy : 0.999485\n",
      "Area under the curve : 0.894482\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print('Accuracy : %f' % (metrics.accuracy_score(y_test, y_pred)))\n",
    "print('Area under the curve : %f' % (metrics.roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run with GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Custom DataLoader\n",
    "class FraudDataset(Dataset):\n",
    "    \n",
    "    # Initialize the data\n",
    "    def __init__(self):\n",
    "        data = pd.read_csv(\"creditcard.csv\")\n",
    "        data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "        data = data.drop(['Time','Amount'],axis=1)\n",
    "        \n",
    "        # Rearrange columns to the right order\n",
    "        cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "        'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount', 'Class']\n",
    "        data = data[cols]\n",
    "        \n",
    "        fraud_data = data.loc[data['Class']==1]\n",
    "        fraud_data = fraud_data.drop('Class', 1)\n",
    "        self.len = fraud_data.shape[0]\n",
    "        \n",
    "        self.fraud_data = torch.FloatTensor(np.array(fraud_data))\n",
    "        \n",
    "        #self.X = np.array(data.loc[:, data.columns != 'Class'])\n",
    "        #self.y = np.array(data.loc[:, data.columns == 'Class'])\n",
    "        \n",
    "        #self.X = torch.FloatTensor(self.X)\n",
    "        #self.y = torch.FloatTensor(self.y)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.fraud_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FraudDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=2,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator's paragrams\n",
    "g_input_size = 29     # Random noise dimension\n",
    "g_hidden_size = 50   # Generator complexity\n",
    "g_output_size = 1   \n",
    "g_learning_rate = 0.0002\n",
    "\n",
    "#Discriminator's paragrams\n",
    "d_input_size = 29   # Minibatch size\n",
    "d_hidden_size = 50  # Discriminator complexity\n",
    "d_output_size = 1   # Single dimension for 'real' vs. 'fake'\n",
    "d_learning_rate = 0.0002\n",
    "\n",
    "minibatch_size = d_input_size\n",
    "\n",
    "num_epochs = 2000\n",
    "print_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ELU (Exponential Linear Unit) function tends to converge cost to zero faster and produce more accurate results\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.sigmoid(self.map2(x))\n",
    "        return self.map3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.map2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.map3(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "discriminator = Discriminator(input_size=d_input_size, hidden_size=d_hidden_size, output_size=d_output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these 2 lines to run on GPU\n",
    "#generator.cuda()\n",
    "#discriminator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(disc):\n",
    "    h=0.1\n",
    "    x_min, x_max = data[:, 0].min() - .5, data[:, 0].max() + .5\n",
    "    y_min, y_max = data[:, 1].min() - .5, data[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    x_test = np.c_[xx.ravel(), yy.ravel()]\n",
    "    y_hat_test = disc.forward_with_sigmoid(Variable(torch.from_numpy(x_test).float()))\n",
    "\n",
    "    plt.pcolormesh(xx, yy, y_hat_test.data.numpy().reshape(xx.shape), cmap=plt.cm.Paired)\n",
    "    plt.colorbar()\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=y*20, alpha=0.1, cmap=plt.cm.flag, s=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Binary Cross Entropy loss\n",
    "BCE_loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizers\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=d_learning_rate/2, betas=(beta_1, beta_2))\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=g_learning_rate, betas=(beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiem/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1474: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/Users/kiem/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/Users/kiem/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Discriminator Loss: 1.024, Generator Loss: 0.721\n",
      "Epoch 11 - Discriminator Loss: 0.150, Generator Loss: 3.226\n",
      "Epoch 21 - Discriminator Loss: 0.305, Generator Loss: 3.627\n",
      "Epoch 31 - Discriminator Loss: 0.291, Generator Loss: 3.521\n",
      "Epoch 41 - Discriminator Loss: 0.222, Generator Loss: 3.673\n",
      "Epoch 51 - Discriminator Loss: 0.191, Generator Loss: 3.820\n",
      "Epoch 61 - Discriminator Loss: 0.206, Generator Loss: 3.830\n",
      "Epoch 71 - Discriminator Loss: 0.239, Generator Loss: 3.914\n",
      "Epoch 81 - Discriminator Loss: 0.195, Generator Loss: 3.968\n",
      "Epoch 91 - Discriminator Loss: 0.170, Generator Loss: 4.132\n",
      "Epoch 101 - Discriminator Loss: 0.143, Generator Loss: 4.296\n",
      "Epoch 111 - Discriminator Loss: 0.180, Generator Loss: 4.030\n",
      "Epoch 121 - Discriminator Loss: 0.156, Generator Loss: 4.047\n",
      "Epoch 131 - Discriminator Loss: 0.162, Generator Loss: 4.516\n",
      "Epoch 141 - Discriminator Loss: 0.129, Generator Loss: 4.515\n",
      "Epoch 151 - Discriminator Loss: 0.110, Generator Loss: 4.697\n",
      "Epoch 161 - Discriminator Loss: 0.165, Generator Loss: 4.637\n",
      "Epoch 171 - Discriminator Loss: 0.104, Generator Loss: 4.691\n",
      "Epoch 181 - Discriminator Loss: 0.086, Generator Loss: 4.985\n",
      "Epoch 191 - Discriminator Loss: 0.092, Generator Loss: 5.078\n",
      "Epoch 201 - Discriminator Loss: 0.074, Generator Loss: 5.537\n",
      "Epoch 211 - Discriminator Loss: 0.061, Generator Loss: 5.579\n",
      "Epoch 221 - Discriminator Loss: 0.059, Generator Loss: 5.804\n",
      "Epoch 231 - Discriminator Loss: 0.037, Generator Loss: 6.477\n",
      "Epoch 241 - Discriminator Loss: 0.042, Generator Loss: 6.535\n",
      "Epoch 251 - Discriminator Loss: 0.030, Generator Loss: 6.880\n",
      "Epoch 261 - Discriminator Loss: 0.049, Generator Loss: 7.155\n",
      "Epoch 271 - Discriminator Loss: 0.041, Generator Loss: 7.468\n",
      "Epoch 281 - Discriminator Loss: 0.016, Generator Loss: 7.437\n",
      "Epoch 291 - Discriminator Loss: 0.028, Generator Loss: 8.030\n",
      "Epoch 301 - Discriminator Loss: 0.031, Generator Loss: 8.272\n",
      "Epoch 311 - Discriminator Loss: 0.017, Generator Loss: 8.692\n",
      "Epoch 321 - Discriminator Loss: 0.019, Generator Loss: 8.730\n",
      "Epoch 331 - Discriminator Loss: 0.013, Generator Loss: 9.096\n",
      "Epoch 341 - Discriminator Loss: 0.014, Generator Loss: 9.964\n",
      "Epoch 351 - Discriminator Loss: 0.019, Generator Loss: 9.963\n",
      "Epoch 361 - Discriminator Loss: 0.010, Generator Loss: 10.329\n",
      "Epoch 371 - Discriminator Loss: 0.005, Generator Loss: 10.902\n",
      "Epoch 381 - Discriminator Loss: 0.008, Generator Loss: 10.591\n",
      "Epoch 391 - Discriminator Loss: 0.017, Generator Loss: 11.312\n",
      "Epoch 401 - Discriminator Loss: 0.016, Generator Loss: 11.826\n",
      "Epoch 411 - Discriminator Loss: 0.010, Generator Loss: 10.958\n",
      "Epoch 421 - Discriminator Loss: 0.011, Generator Loss: 11.386\n",
      "Epoch 431 - Discriminator Loss: 0.005, Generator Loss: 12.012\n",
      "Epoch 441 - Discriminator Loss: 0.014, Generator Loss: 12.038\n",
      "Epoch 451 - Discriminator Loss: 0.006, Generator Loss: 12.446\n",
      "Epoch 461 - Discriminator Loss: 0.003, Generator Loss: 12.459\n",
      "Epoch 471 - Discriminator Loss: 0.003, Generator Loss: 12.716\n",
      "Epoch 481 - Discriminator Loss: 0.005, Generator Loss: 13.214\n",
      "Epoch 491 - Discriminator Loss: 0.004, Generator Loss: 13.564\n",
      "Epoch 501 - Discriminator Loss: 0.013, Generator Loss: 14.477\n",
      "Epoch 511 - Discriminator Loss: 0.002, Generator Loss: 13.969\n",
      "Epoch 521 - Discriminator Loss: 0.002, Generator Loss: 13.390\n",
      "Epoch 531 - Discriminator Loss: 0.017, Generator Loss: 14.613\n",
      "Epoch 541 - Discriminator Loss: 0.002, Generator Loss: 14.044\n",
      "Epoch 551 - Discriminator Loss: 0.009, Generator Loss: 15.139\n",
      "Epoch 561 - Discriminator Loss: 0.003, Generator Loss: 15.297\n",
      "Epoch 571 - Discriminator Loss: 0.003, Generator Loss: 15.048\n",
      "Epoch 581 - Discriminator Loss: 0.003, Generator Loss: 15.225\n",
      "Epoch 591 - Discriminator Loss: 0.002, Generator Loss: 15.496\n",
      "Epoch 601 - Discriminator Loss: 0.009, Generator Loss: 15.487\n",
      "Epoch 611 - Discriminator Loss: 0.002, Generator Loss: 14.974\n",
      "Epoch 621 - Discriminator Loss: 0.003, Generator Loss: 15.687\n",
      "Epoch 631 - Discriminator Loss: 0.004, Generator Loss: 16.004\n",
      "Epoch 641 - Discriminator Loss: 0.038, Generator Loss: 16.720\n",
      "Epoch 651 - Discriminator Loss: 0.003, Generator Loss: 16.323\n",
      "Epoch 661 - Discriminator Loss: 0.001, Generator Loss: 16.416\n",
      "Epoch 671 - Discriminator Loss: 0.001, Generator Loss: 16.284\n",
      "Epoch 681 - Discriminator Loss: 0.000, Generator Loss: 16.439\n",
      "Epoch 691 - Discriminator Loss: 0.004, Generator Loss: 16.905\n",
      "Epoch 701 - Discriminator Loss: 0.004, Generator Loss: 17.727\n",
      "Epoch 711 - Discriminator Loss: 0.001, Generator Loss: 17.764\n",
      "Epoch 721 - Discriminator Loss: 0.004, Generator Loss: 17.822\n",
      "Epoch 731 - Discriminator Loss: 0.004, Generator Loss: 17.912\n",
      "Epoch 741 - Discriminator Loss: 0.001, Generator Loss: 16.888\n",
      "Epoch 751 - Discriminator Loss: 0.001, Generator Loss: 18.534\n",
      "Epoch 761 - Discriminator Loss: 0.005, Generator Loss: 18.659\n",
      "Epoch 771 - Discriminator Loss: 0.001, Generator Loss: 18.453\n",
      "Epoch 781 - Discriminator Loss: 0.002, Generator Loss: 18.686\n",
      "Epoch 791 - Discriminator Loss: 0.001, Generator Loss: 17.609\n",
      "Epoch 801 - Discriminator Loss: 0.002, Generator Loss: 19.218\n",
      "Epoch 811 - Discriminator Loss: 0.008, Generator Loss: 19.556\n",
      "Epoch 821 - Discriminator Loss: 0.002, Generator Loss: 19.194\n",
      "Epoch 831 - Discriminator Loss: 0.008, Generator Loss: 19.617\n",
      "Epoch 841 - Discriminator Loss: 0.004, Generator Loss: 20.125\n",
      "Epoch 851 - Discriminator Loss: 0.003, Generator Loss: 19.652\n",
      "Epoch 861 - Discriminator Loss: 0.001, Generator Loss: 20.190\n",
      "Epoch 871 - Discriminator Loss: 0.002, Generator Loss: 19.240\n",
      "Epoch 881 - Discriminator Loss: 0.003, Generator Loss: 19.985\n",
      "Epoch 891 - Discriminator Loss: 0.001, Generator Loss: 20.042\n",
      "Epoch 901 - Discriminator Loss: 0.005, Generator Loss: 19.461\n",
      "Epoch 911 - Discriminator Loss: 0.005, Generator Loss: 20.087\n",
      "Epoch 921 - Discriminator Loss: 0.002, Generator Loss: 20.460\n",
      "Epoch 931 - Discriminator Loss: 0.009, Generator Loss: 18.920\n",
      "Epoch 941 - Discriminator Loss: 0.002, Generator Loss: 20.277\n",
      "Epoch 951 - Discriminator Loss: 0.016, Generator Loss: 19.837\n",
      "Epoch 961 - Discriminator Loss: 0.008, Generator Loss: 20.783\n",
      "Epoch 971 - Discriminator Loss: 0.002, Generator Loss: 20.614\n",
      "Epoch 981 - Discriminator Loss: 0.026, Generator Loss: 21.574\n",
      "Epoch 991 - Discriminator Loss: 0.004, Generator Loss: 21.278\n",
      "Epoch 1001 - Discriminator Loss: 0.002, Generator Loss: 20.904\n",
      "Epoch 1011 - Discriminator Loss: 0.001, Generator Loss: 20.333\n",
      "Epoch 1021 - Discriminator Loss: 0.000, Generator Loss: 19.532\n",
      "Epoch 1031 - Discriminator Loss: 0.000, Generator Loss: 18.309\n",
      "Epoch 1041 - Discriminator Loss: 0.000, Generator Loss: 18.996\n",
      "Epoch 1051 - Discriminator Loss: 0.000, Generator Loss: 18.346\n",
      "Epoch 1061 - Discriminator Loss: 0.000, Generator Loss: 18.973\n",
      "Epoch 1071 - Discriminator Loss: 0.000, Generator Loss: 20.997\n",
      "Epoch 1081 - Discriminator Loss: 0.025, Generator Loss: 22.091\n",
      "Epoch 1091 - Discriminator Loss: 0.113, Generator Loss: 22.951\n",
      "Epoch 1101 - Discriminator Loss: 0.016, Generator Loss: 23.501\n",
      "Epoch 1111 - Discriminator Loss: 0.004, Generator Loss: 22.974\n",
      "Epoch 1121 - Discriminator Loss: 0.003, Generator Loss: 22.137\n",
      "Epoch 1131 - Discriminator Loss: 0.000, Generator Loss: 20.921\n",
      "Epoch 1141 - Discriminator Loss: 0.000, Generator Loss: 20.255\n",
      "Epoch 1151 - Discriminator Loss: 0.000, Generator Loss: 19.286\n",
      "Epoch 1161 - Discriminator Loss: 0.000, Generator Loss: 18.707\n",
      "Epoch 1171 - Discriminator Loss: 0.000, Generator Loss: 17.933\n",
      "Epoch 1181 - Discriminator Loss: 0.000, Generator Loss: 17.122\n",
      "Epoch 1191 - Discriminator Loss: 0.000, Generator Loss: 16.748\n",
      "Epoch 1201 - Discriminator Loss: 0.000, Generator Loss: 16.914\n",
      "Epoch 1211 - Discriminator Loss: 0.000, Generator Loss: 18.402\n",
      "Epoch 1221 - Discriminator Loss: 0.000, Generator Loss: 22.828\n",
      "Epoch 1231 - Discriminator Loss: 0.145, Generator Loss: 24.413\n",
      "Epoch 1241 - Discriminator Loss: 0.211, Generator Loss: 24.842\n",
      "Epoch 1251 - Discriminator Loss: 0.091, Generator Loss: 24.402\n",
      "Epoch 1261 - Discriminator Loss: 0.010, Generator Loss: 24.718\n",
      "Epoch 1271 - Discriminator Loss: 0.008, Generator Loss: 24.299\n",
      "Epoch 1281 - Discriminator Loss: 0.004, Generator Loss: 24.203\n",
      "Epoch 1291 - Discriminator Loss: 0.001, Generator Loss: 23.446\n",
      "Epoch 1301 - Discriminator Loss: 0.000, Generator Loss: 22.535\n",
      "Epoch 1311 - Discriminator Loss: 0.000, Generator Loss: 21.827\n",
      "Epoch 1321 - Discriminator Loss: 0.000, Generator Loss: 20.900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1331 - Discriminator Loss: 0.000, Generator Loss: 20.092\n",
      "Epoch 1341 - Discriminator Loss: 0.000, Generator Loss: 19.438\n",
      "Epoch 1351 - Discriminator Loss: 0.000, Generator Loss: 18.589\n",
      "Epoch 1361 - Discriminator Loss: 0.000, Generator Loss: 17.957\n",
      "Epoch 1371 - Discriminator Loss: 0.000, Generator Loss: 17.807\n",
      "Epoch 1381 - Discriminator Loss: 0.000, Generator Loss: 18.706\n",
      "Epoch 1391 - Discriminator Loss: 0.000, Generator Loss: 20.359\n",
      "Epoch 1401 - Discriminator Loss: 0.000, Generator Loss: 23.940\n",
      "Epoch 1411 - Discriminator Loss: 0.140, Generator Loss: 25.466\n",
      "Epoch 1421 - Discriminator Loss: 0.249, Generator Loss: 25.234\n",
      "Epoch 1431 - Discriminator Loss: 0.174, Generator Loss: 25.378\n",
      "Epoch 1441 - Discriminator Loss: 0.005, Generator Loss: 25.254\n",
      "Epoch 1451 - Discriminator Loss: 0.147, Generator Loss: 24.742\n",
      "Epoch 1461 - Discriminator Loss: 0.003, Generator Loss: 25.216\n",
      "Epoch 1471 - Discriminator Loss: 0.067, Generator Loss: 24.663\n",
      "Epoch 1481 - Discriminator Loss: 0.004, Generator Loss: 24.289\n",
      "Epoch 1491 - Discriminator Loss: 0.013, Generator Loss: 24.617\n",
      "Epoch 1501 - Discriminator Loss: 0.016, Generator Loss: 23.841\n",
      "Epoch 1511 - Discriminator Loss: 0.015, Generator Loss: 23.992\n",
      "Epoch 1521 - Discriminator Loss: 0.001, Generator Loss: 22.863\n",
      "Epoch 1531 - Discriminator Loss: 0.000, Generator Loss: 22.005\n",
      "Epoch 1541 - Discriminator Loss: 0.000, Generator Loss: 21.705\n",
      "Epoch 1551 - Discriminator Loss: 0.000, Generator Loss: 22.884\n",
      "Epoch 1561 - Discriminator Loss: 0.000, Generator Loss: 21.445\n",
      "Epoch 1571 - Discriminator Loss: 0.000, Generator Loss: 21.129\n",
      "Epoch 1581 - Discriminator Loss: 0.000, Generator Loss: 20.873\n",
      "Epoch 1591 - Discriminator Loss: 0.000, Generator Loss: 20.416\n",
      "Epoch 1601 - Discriminator Loss: 0.000, Generator Loss: 19.769\n",
      "Epoch 1611 - Discriminator Loss: 0.000, Generator Loss: 19.018\n",
      "Epoch 1621 - Discriminator Loss: 0.000, Generator Loss: 18.309\n",
      "Epoch 1631 - Discriminator Loss: 0.000, Generator Loss: 18.156\n",
      "Epoch 1641 - Discriminator Loss: 0.000, Generator Loss: 18.769\n",
      "Epoch 1651 - Discriminator Loss: 0.000, Generator Loss: 19.419\n",
      "Epoch 1661 - Discriminator Loss: 0.000, Generator Loss: 20.387\n",
      "Epoch 1671 - Discriminator Loss: 0.000, Generator Loss: 21.367\n",
      "Epoch 1681 - Discriminator Loss: 0.000, Generator Loss: 22.307\n",
      "Epoch 1691 - Discriminator Loss: 0.000, Generator Loss: 22.676\n",
      "Epoch 1701 - Discriminator Loss: 0.174, Generator Loss: 22.677\n",
      "Epoch 1711 - Discriminator Loss: 0.172, Generator Loss: 24.753\n",
      "Epoch 1721 - Discriminator Loss: 0.067, Generator Loss: 25.824\n",
      "Epoch 1731 - Discriminator Loss: 0.005, Generator Loss: 25.700\n",
      "Epoch 1741 - Discriminator Loss: 0.011, Generator Loss: 24.395\n",
      "Epoch 1751 - Discriminator Loss: 0.124, Generator Loss: 24.894\n",
      "Epoch 1761 - Discriminator Loss: 0.039, Generator Loss: 25.608\n",
      "Epoch 1771 - Discriminator Loss: 0.110, Generator Loss: 25.916\n",
      "Epoch 1781 - Discriminator Loss: 0.167, Generator Loss: 24.483\n",
      "Epoch 1791 - Discriminator Loss: 0.168, Generator Loss: 25.487\n",
      "Epoch 1801 - Discriminator Loss: 0.074, Generator Loss: 24.520\n",
      "Epoch 1811 - Discriminator Loss: 0.041, Generator Loss: 25.357\n",
      "Epoch 1821 - Discriminator Loss: 0.024, Generator Loss: 24.707\n",
      "Epoch 1831 - Discriminator Loss: 0.004, Generator Loss: 24.894\n",
      "Epoch 1841 - Discriminator Loss: 0.126, Generator Loss: 24.622\n",
      "Epoch 1851 - Discriminator Loss: 0.051, Generator Loss: 24.749\n",
      "Epoch 1861 - Discriminator Loss: 0.022, Generator Loss: 25.093\n",
      "Epoch 1871 - Discriminator Loss: 0.022, Generator Loss: 24.715\n",
      "Epoch 1881 - Discriminator Loss: 0.040, Generator Loss: 24.683\n",
      "Epoch 1891 - Discriminator Loss: 0.046, Generator Loss: 24.736\n",
      "Epoch 1901 - Discriminator Loss: 0.002, Generator Loss: 24.595\n",
      "Epoch 1911 - Discriminator Loss: 0.001, Generator Loss: 24.075\n",
      "Epoch 1921 - Discriminator Loss: 0.001, Generator Loss: 24.039\n",
      "Epoch 1931 - Discriminator Loss: 0.113, Generator Loss: 23.916\n",
      "Epoch 1941 - Discriminator Loss: 0.001, Generator Loss: 24.073\n",
      "Epoch 1951 - Discriminator Loss: 0.008, Generator Loss: 23.996\n",
      "Epoch 1961 - Discriminator Loss: 0.001, Generator Loss: 24.020\n",
      "Epoch 1971 - Discriminator Loss: 0.001, Generator Loss: 24.302\n",
      "Epoch 1981 - Discriminator Loss: 0.026, Generator Loss: 24.770\n",
      "Epoch 1991 - Discriminator Loss: 0.002, Generator Loss: 25.464\n"
     ]
    }
   ],
   "source": [
    "# Training DCGANs\n",
    "for epoch in range(num_epochs):\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    synthentic_data = []\n",
    "    for i, fraud_data in enumerate(train_loader):\n",
    "        # Updating the weights of the Discriminator\n",
    "        discriminator.zero_grad() # Initialize gradients of the Discriminator to 0\n",
    "        \n",
    "        mini_batch = fraud_data.size()[0]\n",
    "        \n",
    "        # Wrap data in PyTorch Variable\n",
    "        d_real_data = Variable(fraud_data[0])\n",
    "        y_real = Variable(torch.ones(1))\n",
    "        y_fake = Variable(torch.zeros(1))\n",
    "\n",
    "        # Training the Discriminator with real data\n",
    "        d_real_result = discriminator(d_real_data) # Forward propagate this real data into the neural network\n",
    "        d_real_loss = BCE_loss(d_real_result, y_real) # Compute the loss between the prediction and actual\n",
    "        d_real_loss.backward()\n",
    "    \n",
    "        # Inject fake data to the generator\n",
    "        d_gen_input = Variable(torch.randn(minibatch_size, g_input_size))\n",
    "        d_fake_data = generator(d_gen_input).detach()  # detach to avoid training G on these labels\n",
    "        \n",
    "        # Train the Discriminator with a fake data generated by the Generator\n",
    "        d_fake_result = discriminator(d_fake_data.t())\n",
    "        d_fake_loss = BCE_loss(d_fake_result, y_fake)  # zeros = fake\n",
    "        d_fake_loss.backward()\n",
    "        \n",
    "        # Combine discriminator loss from real data and fake data\n",
    "        d_train_loss = d_real_loss + d_fake_loss\n",
    "        \n",
    "        #d_train_loss.backward()\n",
    "        d_optimizer.step()     # Apply SGD to update the weight\n",
    "        d_losses.append(d_train_loss.data[0])\n",
    "        \n",
    "        # Update the weight of the Generator \n",
    "        generator.zero_grad()\n",
    "        gen_input = Variable(torch.randn(minibatch_size, g_input_size))  \n",
    "        g_fake_data = generator(gen_input)\n",
    "        \n",
    "        dg_fake_result = discriminator(g_fake_data.t())\n",
    "        g_loss = BCE_loss(dg_fake_result, y_real)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        g_losses.append(g_loss.data[0])\n",
    "        \n",
    "        synthentic_data.append(d_fake_data.t())\n",
    "        \n",
    "    if epoch % print_interval == 0:       \n",
    "        print('Epoch {} - Discriminator Loss: {:.3f}, Generator Loss: {:.3f}'.format((epoch + 1), \n",
    "                          torch.mean(torch.FloatTensor(d_losses)), torch.mean(torch.FloatTensor(g_losses))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x104bf0b70>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(d_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x104c4a438>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.0366,  0.0663, -3.0860,  0.1141,  0.0644,  0.0298,  0.0936,\n",
       "           0.0639,  0.0947, -3.8126,  0.1399,  0.0651,  0.1022,  0.0791,\n",
       "          -2.3816,  0.0451,  0.0674, -3.9379,  0.0051, -0.4626,  0.0703,\n",
       "           0.0803, -0.1061,  0.0589,  0.5689,  0.0748,  0.0192,  0.0581,\n",
       "           0.0435]]),\n",
       " tensor([[ 0.1882,  0.0720,  0.0654,  0.0629, -0.0147, -0.2652,  0.0470,\n",
       "          -0.0004,  0.0377,  0.0783,  0.1826,  1.0644,  0.0543,  0.0092,\n",
       "           0.0739,  0.0194,  0.0985,  0.0612,  0.0474,  0.0414,  0.0864,\n",
       "           0.0574, -0.0425,  0.0503,  0.0540,  0.0226,  0.1295,  0.0667,\n",
       "           0.0498]]),\n",
       " tensor([[-0.5751, -1.0632,  0.0984,  0.1432,  0.0624,  0.0539, -0.8770,\n",
       "          -0.4738, -0.6287,  0.0503,  0.0244,  0.0500,  0.0492,  0.0371,\n",
       "           0.0972,  0.0396,  0.0662,  0.0883,  0.0389,  0.0515,  0.0543,\n",
       "          -0.3729,  0.1379,  0.0429,  0.0409,  0.0454,  0.0211,  0.0497,\n",
       "           4.5468]]),\n",
       " tensor([[-0.7600,  0.0802,  4.9151,  0.0681,  0.0616, -1.3595,  0.0748,\n",
       "           0.1173,  0.1296,  3.0410,  0.0342,  0.0765,  0.0700,  0.1311,\n",
       "           0.2582,  0.1265, -0.0140, -0.2020,  0.0249,  0.0463,  0.0592,\n",
       "           0.0603, -4.1696,  0.0528,  0.0592,  0.0933,  0.0669, -0.8618,\n",
       "           0.0808]]),\n",
       " tensor([[-1.4547, -2.9036,  0.0246,  0.0688,  0.0982,  0.0591,  0.0507,\n",
       "           0.1138,  0.1485,  0.0594,  0.0681, -0.0457,  0.0267,  0.0434,\n",
       "           0.1253, -2.6256,  0.0779,  0.0753,  0.0764,  0.0614,  0.0540,\n",
       "           0.0496,  0.0660,  0.0416,  0.1540,  0.0569,  0.7221, -0.0042,\n",
       "           0.2450]]),\n",
       " tensor([[ 0.0470,  0.0123,  0.0683,  0.4984,  0.1647,  0.0757,  0.0424,\n",
       "           0.0736,  0.0555, -2.3365,  0.1159,  0.0534,  0.0560,  0.1419,\n",
       "           0.0666,  1.9096,  0.0482,  0.0441,  0.1390,  0.0272, -0.6478,\n",
       "           0.0613,  0.0714, -0.7049, -3.6650,  0.0879,  0.0907,  0.0718,\n",
       "          -1.1184]]),\n",
       " tensor([[ 0.0528,  0.0785, -0.0398,  0.0960,  0.0595,  0.0741,  0.1242,\n",
       "           0.0695, -0.2317,  0.4184,  0.0540,  0.1073, -2.5253, -0.6404,\n",
       "           0.0469,  0.0547,  0.0211, -1.2101,  0.1553, -4.4013,  0.0273,\n",
       "           0.0656,  0.0466,  0.0574,  0.0512, -1.3212,  0.0498,  0.0745,\n",
       "           0.0756]]),\n",
       " tensor([[ 0.0327,  0.1416,  0.0412,  0.0040,  0.0658,  0.0087,  0.1008,\n",
       "           0.0480,  0.0546,  0.1021,  0.1366,  0.0592, -1.3306,  2.6238,\n",
       "           0.0583,  0.0566,  0.0683,  0.2372,  0.0436,  0.3689,  0.0856,\n",
       "           0.0898,  0.0558,  0.1361,  0.0185, -0.7322,  0.0440,  0.0767,\n",
       "          -0.0093]]),\n",
       " tensor([[ 0.0364,  0.0568,  0.0401,  0.0409,  0.1066,  0.0629,  0.0550,\n",
       "           0.0300,  0.0428,  0.0210,  0.1531, -0.6300, -0.5419,  0.0600,\n",
       "           0.0727,  0.0730,  0.0482,  0.0797,  1.5216, -1.1893, -3.1369,\n",
       "           0.0763, -0.0706,  0.0532,  0.0500,  0.0345,  0.0828, -1.7837,\n",
       "           0.0362]]),\n",
       " tensor([[ 0.0579,  0.0556, -0.3838,  0.0145,  0.0355,  0.1186,  0.1197,\n",
       "           0.0566,  0.0856,  0.0584,  0.0515,  0.0463, -0.0818,  0.0300,\n",
       "           0.0433,  0.0739,  0.0950,  0.0769, -3.3291,  0.0374,  3.7654,\n",
       "           0.0586,  0.1013,  0.2026,  0.1571,  0.0629,  0.0882,  0.0808,\n",
       "          -0.3529]]),\n",
       " tensor([[ 0.0431,  0.1138,  0.0755,  0.0811,  0.0790,  0.0391,  0.0420,\n",
       "           0.0282,  0.0267,  0.0722,  0.0792,  0.1035,  0.1387, -1.3833,\n",
       "          -0.6678,  0.0945,  0.0827,  0.0587,  0.0768,  0.0514,  0.0565,\n",
       "           0.0522,  0.0676,  0.0583,  0.0703,  0.0294,  0.0904,  0.0571,\n",
       "           0.0598]]),\n",
       " tensor([[ 0.1132,  5.7510,  0.1938,  0.0479,  0.0546, -0.0041, -0.8885,\n",
       "           0.0471,  0.0883,  0.0171,  0.1011,  0.0817,  0.0465,  0.0533,\n",
       "           0.0363,  0.0493,  0.1144,  0.1128,  0.0623,  0.1245,  0.0596,\n",
       "           0.0575,  2.7996,  0.1130, -4.9075,  0.0953,  0.0566,  0.0689,\n",
       "           0.1491]]),\n",
       " tensor([[ 0.0513,  0.1475,  0.0531, -2.7326, -2.0476, -0.0046, -2.0715,\n",
       "           0.0483,  0.0537,  3.8683, -0.0080,  0.0457,  0.0287, -4.1446,\n",
       "           0.0140,  0.0652, -0.0731,  0.0639,  0.0697,  0.0500, -3.5598,\n",
       "           0.0509,  0.0744,  1.5550,  0.0736,  0.0541,  0.0087, -4.1991,\n",
       "           0.0501]]),\n",
       " tensor([[ 5.4858e-02, -4.1205e-01,  1.5610e-02, -6.9812e-01,  4.9937e-02,\n",
       "           3.4705e-02,  9.9974e-02,  4.7116e-02,  1.4408e-05, -4.8086e+00,\n",
       "          -4.5274e+00,  5.9366e-02,  1.0511e-01,  2.4615e-02,  6.2260e-02,\n",
       "           5.6582e-02,  9.9450e-02,  1.3475e-01,  5.8393e-02,  5.6351e-02,\n",
       "          -4.0402e-01,  6.1430e-01,  8.1536e-02,  8.3574e-03,  5.9731e-02,\n",
       "           6.4695e-02,  2.5992e-01,  4.6324e-02,  1.5295e-02]]),\n",
       " tensor([[ 0.0510,  0.0979,  0.0572,  0.0510,  0.0581,  0.0498,  0.0336,\n",
       "          -1.4398,  0.0800,  0.1257,  0.0516,  0.0465,  0.0197,  0.0396,\n",
       "           0.0807,  0.0247, -0.0178, -0.0250, -2.0842,  0.0503,  0.0231,\n",
       "          -0.6927,  0.0479,  0.0504,  0.0848,  0.1348,  0.0174,  0.1277,\n",
       "           0.0452]]),\n",
       " tensor([[ 0.0535, -1.6509, -4.6469,  0.0789,  0.0305,  0.0732,  0.0453,\n",
       "           0.0644,  0.0671,  0.0925, -0.4290,  0.0441,  0.0417,  2.3071,\n",
       "           0.0556, -0.0223, -0.3964,  0.0503,  0.0547,  0.0347,  0.0974,\n",
       "           5.8468,  0.0325,  0.0557,  0.0554,  0.0421,  0.0725,  0.0918,\n",
       "           0.1139]]),\n",
       " tensor([[ 0.0595,  0.0167,  0.0785,  0.0262,  0.0808,  0.0507,  0.0384,\n",
       "           0.0766,  0.0486, -0.0267, -0.0824,  0.0517,  0.0174, -0.0197,\n",
       "           0.0307,  0.0897,  0.0476, -0.3390,  0.0714,  0.0361,  0.0076,\n",
       "           0.0374,  0.0709,  0.0598,  0.1500,  0.0567,  0.1134,  0.0460,\n",
       "          -0.9665]]),\n",
       " tensor([[ 0.0538,  0.1413,  0.0496,  0.0517, -1.5806,  0.0492, -4.9596,\n",
       "           0.0462, -1.9560,  0.0476,  0.0439,  0.1030,  0.1113,  1.9579,\n",
       "           0.0993,  0.0467,  0.1463,  0.1611,  0.0651,  0.0462,  0.0567,\n",
       "           0.1060, -1.0135,  0.0657,  0.0038,  0.1656,  0.0579, -0.2918,\n",
       "           0.0552]]),\n",
       " tensor([[ 0.0446,  0.0754,  0.1213,  0.0605, -0.0832,  0.0590,  0.0085,\n",
       "           0.0761,  0.0405,  0.0376,  0.0682, -3.4465,  0.1499,  0.0717,\n",
       "           0.0327, -0.2290,  0.0853,  0.0378,  0.0483,  0.0575,  0.2032,\n",
       "          -1.6437,  0.0511,  0.0473,  0.0555,  0.0787,  0.0486,  0.0345,\n",
       "           0.0425]]),\n",
       " tensor([[-0.1621,  0.0502, -1.2045,  0.0246,  0.0567,  0.0425, -0.1306,\n",
       "          -0.0005, -0.0302,  0.0421,  0.0447,  0.0529, -4.7503, -4.8604,\n",
       "           0.0494,  0.9742,  0.0405,  0.0614,  0.0627,  0.0812,  0.0134,\n",
       "           0.0475,  0.0433,  0.0149,  0.0186, -1.2882, -0.9916,  0.0771,\n",
       "           3.1377]]),\n",
       " tensor([[ 0.1047,  0.0708,  0.0468,  0.0154,  0.0110, -3.7683,  0.0070,\n",
       "           0.0487, -1.7307,  0.0476,  0.0453,  0.0432, -1.1290,  0.0480,\n",
       "          -3.5118,  0.0221,  0.0388,  0.0240,  0.0524, -0.2239,  0.0239,\n",
       "           0.0278,  0.0508,  0.0623,  0.0416,  0.1057,  0.0666,  0.0435,\n",
       "           0.5529]]),\n",
       " tensor([[-4.1451,  0.0434,  0.0462,  0.0479,  0.0567,  1.9932,  0.1038,\n",
       "           0.0674,  0.0218,  0.0484,  0.0528,  0.0288,  0.0734,  0.0080,\n",
       "           0.0471, -0.0165,  0.0490, -0.2256,  0.0511,  0.0457,  0.1029,\n",
       "           0.0269,  0.0432, -0.0639,  0.0440, -4.6212,  0.0192, -1.6164,\n",
       "          -0.0209]]),\n",
       " tensor([[ 0.1072,  0.0532,  0.0431,  0.0503, -0.4449,  0.0095,  0.0874,\n",
       "           0.0421,  0.0398,  0.5059,  0.0401, -2.8351,  0.0709,  0.0480,\n",
       "           0.0926, -0.8834,  0.0367,  0.0386,  0.0539,  0.0509,  0.0450,\n",
       "           0.0624,  0.0445,  0.0422,  4.7609,  0.0441,  0.1921,  0.0190,\n",
       "           0.0478]]),\n",
       " tensor([[ 0.0466,  2.1247,  0.0583,  0.0875,  0.0445,  1.6705, -1.6138,\n",
       "           0.1008,  1.3122,  0.0463,  0.0738,  0.0654,  0.0490,  0.0481,\n",
       "          -1.9361,  0.1503,  0.0783,  0.0452, -0.2260, -2.9915,  0.1519,\n",
       "           0.3617,  0.0325,  0.0170,  0.0417, -0.1057,  0.0656,  0.0949,\n",
       "           0.0961]]),\n",
       " tensor([[ 0.0331,  0.1282,  0.0535,  0.0955,  0.0354, -0.0074,  0.0339,\n",
       "           0.0531, -0.8231, -0.0213,  0.0823, -2.8022, -0.0164,  0.0534,\n",
       "           0.0229,  0.0570,  0.0453,  0.0420,  0.0415,  0.0422,  0.0641,\n",
       "           0.0488,  0.0247,  0.0654,  0.0462,  0.0468,  0.0469, -0.1061,\n",
       "           0.0606]]),\n",
       " tensor([[ 0.0410,  0.0767,  0.0626, -2.7265,  0.0519, -0.0234,  1.8238,\n",
       "           0.0800,  0.0392,  0.0497,  0.1067,  0.0564,  0.6444,  0.1247,\n",
       "           0.0266,  0.1342, -0.2742, -4.2662,  0.0564, -0.8164,  0.0327,\n",
       "           5.8442,  0.0443,  0.5993,  0.0481,  0.0493, -0.0137,  0.0250,\n",
       "           0.0513]]),\n",
       " tensor([[ 0.0992,  0.1121, -2.4429,  0.0504, -1.3380, -1.2626,  0.0359,\n",
       "           0.0536,  0.0622, -2.7304,  0.0489,  0.0628,  0.1493,  0.0305,\n",
       "           0.0126,  0.0391, -0.0933,  0.5904,  0.0568,  0.0778, -3.6670,\n",
       "           0.0733, -1.3328,  0.0334, -2.8374,  0.0447,  0.0647,  0.1005,\n",
       "           0.1062]]),\n",
       " tensor([[ 0.0560, -1.6794,  0.0433,  0.0533,  0.0536,  0.0702,  0.0481,\n",
       "           0.0642,  0.0775,  0.0484,  0.0200,  0.0510,  0.1658,  0.0499,\n",
       "           3.5857,  0.0548,  0.0441,  0.0098,  0.0535,  0.0298,  0.0427,\n",
       "           0.0379,  0.8387,  0.0514,  0.0483,  0.0298,  0.0206,  0.0680,\n",
       "           0.1364]]),\n",
       " tensor([[ 6.4008e-02,  5.6077e-02, -3.8879e-01,  9.6994e-02,  5.7562e-02,\n",
       "           5.1618e-02,  3.5836e-02,  5.3772e-02, -6.3131e-05,  6.1225e-02,\n",
       "           7.1118e-02,  3.7637e-02,  7.9086e-02,  2.8956e-02,  4.8814e-02,\n",
       "           8.7516e-02,  1.8680e+00,  4.6659e-02,  8.3989e-02,  5.7763e-02,\n",
       "           7.6162e-02,  6.0658e-02,  4.6208e-03,  8.8887e-02,  4.4706e-02,\n",
       "           1.0376e-01,  1.2893e-02, -1.4616e-01,  6.2349e-02]]),\n",
       " tensor([[ 0.0067,  0.0191,  0.0460,  0.0734,  0.0465,  0.0453, -1.1626,\n",
       "          -0.0223,  1.6809,  0.0395,  0.0463,  0.0592,  0.0457, -0.0536,\n",
       "           0.0404,  0.0677, -0.1651,  0.0384,  0.0556,  0.1254,  0.0469,\n",
       "           0.0477,  0.0495,  0.0524, -3.2547, -0.0918,  0.1017,  0.0322,\n",
       "           0.1838]]),\n",
       " tensor([[ 0.0459,  0.0218,  0.0588,  0.0402,  0.0765,  0.0377,  0.0677,\n",
       "           0.0611, -2.6841,  0.0456, -2.8895,  0.0237,  0.0451,  0.0575,\n",
       "           0.0541,  0.0479,  0.0487,  0.0386,  0.0587,  0.1164,  0.0956,\n",
       "          -0.0003,  0.0541,  0.0467,  0.0099, -3.4538, -0.0194, -3.0264,\n",
       "           0.0867]]),\n",
       " tensor([[ 0.0370,  0.0650,  0.1231,  0.0055,  0.1932,  0.0561, -0.0308,\n",
       "          -0.0069,  0.0310, -3.1294,  0.1503,  0.0916, -3.4535,  0.0851,\n",
       "           0.0616,  0.0367, -0.9732, -1.9268,  0.0463, -1.5053,  0.1426,\n",
       "           0.0463,  0.0680,  0.0761,  0.0582, -0.0125, -0.6308,  0.1268,\n",
       "           0.0475]]),\n",
       " tensor([[ 0.0483,  0.0402,  0.0419,  0.0319, -0.0083,  0.1229,  0.0657,\n",
       "           0.0407, -0.0022,  0.0425,  0.1115,  0.0289, -3.3006, -1.7193,\n",
       "           0.1216, -0.0036,  0.2116,  0.0774,  0.0444,  0.0577,  0.1087,\n",
       "           0.0945,  0.0448,  0.0286,  0.0682,  0.0562,  0.1266, -0.5005,\n",
       "          -1.9966]]),\n",
       " tensor([[ 0.0471, -0.0060,  0.0503,  0.0523,  0.0937,  0.0416,  0.1039,\n",
       "           0.0603,  0.0412,  0.0560,  0.0426,  0.0449,  0.1650,  2.8871,\n",
       "           0.0398,  0.0471,  5.3237,  0.0884,  0.0170, -0.0121,  0.0717,\n",
       "          -0.0772,  0.0091,  0.0537, -0.5315,  0.0248,  0.0456,  0.0428,\n",
       "           0.0465]]),\n",
       " tensor([[ 0.0460,  0.0522,  0.0644,  0.0420,  0.0336, -0.5258,  0.0472,\n",
       "           0.0585,  0.0474,  0.0649,  0.0461,  0.0256, -0.1295,  0.0826,\n",
       "          -4.5108,  0.0423,  0.0442,  0.0386, -0.5706,  0.0394,  0.0475,\n",
       "           0.0352, -0.0605, -1.2526,  3.8208,  0.0249,  0.0916,  0.0438,\n",
       "           0.2412]]),\n",
       " tensor([[-3.1043,  0.0564,  0.6413,  0.0497,  0.0786,  0.1502,  0.0450,\n",
       "           0.1038,  0.0436,  0.0360,  0.0297,  0.0456,  0.0547,  0.0441,\n",
       "           0.0749, -0.2781, -0.1730,  0.0273,  0.0810,  0.0498,  0.0621,\n",
       "           0.0826,  0.0719,  0.0517,  0.0739, -0.0086,  0.0527,  0.0809,\n",
       "           0.0410]]),\n",
       " tensor([[ 0.0440,  0.0390, -0.3941,  0.0432,  0.0564,  0.0601,  1.1423,\n",
       "          -1.1457,  0.0271,  0.0456, -0.0061,  0.0721,  0.1055,  0.0495,\n",
       "           0.0327,  0.0426, -0.9792,  0.0292,  0.0149,  0.0360, -1.8381,\n",
       "           0.0850,  0.1132,  0.0604,  0.0712,  0.0227,  0.0048,  0.0757,\n",
       "           0.0532]]),\n",
       " tensor([[ 0.4606, -1.7847,  0.0872,  0.0333,  0.0775,  0.0085,  0.0270,\n",
       "           0.0832,  0.0277, -4.5179,  0.1040,  0.0393,  0.1582,  0.0276,\n",
       "           0.0935,  0.0115,  1.2853,  0.0147,  0.0397, -3.4618,  0.0877,\n",
       "           0.0119,  0.0715,  0.0414, -2.8615, -2.9530,  0.0552, -0.0482,\n",
       "           0.0534]]),\n",
       " tensor([[ 9.8633e-02,  1.0583e-02,  4.7605e-02,  7.1267e-02, -1.2044e-01,\n",
       "          -1.1437e+00,  9.9368e-02,  4.8433e-02,  8.3084e-02,  2.0095e-02,\n",
       "           4.7157e-02, -9.0317e-02,  4.8751e-02,  5.7170e-03,  8.6024e-03,\n",
       "           5.0039e-02,  4.7406e-02,  2.3033e-02,  2.4933e-02,  6.1360e-02,\n",
       "           5.9187e-02,  1.0723e-01,  1.7174e-02,  2.5487e-05, -3.1941e-03,\n",
       "           6.0204e-02,  2.1347e-01,  4.3115e-02,  8.7150e-02]]),\n",
       " tensor([[ 0.0363,  0.0499,  0.0656, -4.8345,  0.0472,  0.0513, -0.0114,\n",
       "           0.0280,  0.0529, -0.7541, -0.0012,  0.1531,  0.0485, -0.0148,\n",
       "           0.0217,  4.4102,  0.0493, -1.5671,  0.0441,  0.0457,  0.1382,\n",
       "           0.0447,  0.0514, -1.2993,  0.0459,  0.0208,  0.1748,  0.0357,\n",
       "          -4.3665]]),\n",
       " tensor([[ 0.0345, -0.0244, -0.0310,  0.0152,  0.0867,  0.0860,  0.0643,\n",
       "           0.0006,  0.0286,  0.0283, -0.2969,  0.0339,  0.0494,  0.0615,\n",
       "           0.0415,  0.0009, -3.3850,  0.0401,  0.0621, -4.4514, -1.6495,\n",
       "           0.0356, -1.2348,  0.1056, -1.7841,  0.0457,  0.0456,  0.0547,\n",
       "           0.0456]]),\n",
       " tensor([[ 0.0731, -0.1947,  0.0672, -0.9564,  0.0301,  0.0472,  0.0463,\n",
       "           0.0719,  0.0276, -3.8422,  0.0430,  0.0177,  0.0844,  0.0895,\n",
       "           0.0677,  0.0343,  0.1746,  0.0324, -0.3487,  0.0959,  0.1466,\n",
       "           0.0381,  0.0096,  0.0131,  0.0404, -1.6630, -0.0069,  0.0629,\n",
       "           0.0177]]),\n",
       " tensor([[ 0.0524,  0.0224,  0.0431, -0.0024,  0.0457,  0.0372,  0.0526,\n",
       "           0.0388,  0.1783,  0.0502,  0.0625,  0.0466,  0.0450,  0.0725,\n",
       "           0.0586,  2.1609,  0.1360, -0.0268,  0.0472, -0.0583,  0.0598,\n",
       "           0.1051,  0.0362,  0.3559,  0.0913, -0.9214,  0.0470,  0.0435,\n",
       "           0.0447]]),\n",
       " tensor([[ 0.0449, -0.0269,  0.0832,  0.0582,  0.0487,  0.0173,  0.0428,\n",
       "           0.0471,  0.1656,  0.0630,  0.0546,  0.0020,  0.3441,  0.0482,\n",
       "           0.1231,  0.0122,  0.0235,  0.0946,  0.1421, -0.4613,  0.0609,\n",
       "           0.0359,  0.0272,  0.0357, -2.1135, -1.2131,  0.0378,  0.0580,\n",
       "           0.0437]]),\n",
       " tensor([[-0.0631,  0.0259, -0.0037,  0.0775,  0.0500, -1.1163,  0.0730,\n",
       "           0.0192,  0.0404,  1.4420,  0.0578,  0.0642,  0.0924, -4.1822,\n",
       "           0.0344, -0.0109,  0.0489,  0.0540,  0.0449,  0.0450,  0.0400,\n",
       "           0.0739,  0.0635, -0.8981, -0.3152,  0.0414,  0.0447, -1.0023,\n",
       "           0.0894]]),\n",
       " tensor([[-0.1250,  0.1658,  0.0972,  0.0502, -0.6292,  0.0382,  0.0384,\n",
       "           0.0433,  0.0005,  4.7485,  0.0471, -0.3866,  0.0599,  0.0478,\n",
       "           0.0255, -1.2040,  0.1242,  0.0433,  0.0877,  0.0130,  0.0607,\n",
       "           0.0904,  0.0422,  0.0409,  0.0310, -0.2035,  0.0012,  0.0405,\n",
       "           0.0189]]),\n",
       " tensor([[ 0.0562,  0.0601, -0.0163,  0.0463,  0.0647,  0.0553, -0.0073,\n",
       "           0.0476,  0.0300, -0.1928,  0.0638, -0.1583,  2.0301,  0.0702,\n",
       "          -0.0160,  0.0449,  0.0498,  0.0680,  0.0415, -0.0074,  0.1248,\n",
       "           0.0796,  0.0690,  0.0391,  0.0706,  0.1831,  0.0232,  0.0883,\n",
       "           0.0165]]),\n",
       " tensor([[ 0.0519,  0.1067, -0.5864,  7.2350,  0.0414, -2.6035,  0.0432,\n",
       "           0.0041,  0.0183,  0.0096,  0.0419,  0.1762, -0.0031,  0.0459,\n",
       "           0.0436,  0.0773, -3.5455, -0.0172, -0.6548,  0.0404,  0.0547,\n",
       "           0.0548,  0.0423,  0.0408,  0.0450,  0.1589, -2.8414,  0.0424,\n",
       "           0.0758]]),\n",
       " tensor([[ 0.0474,  0.0623,  0.0956,  0.0423,  0.0254,  0.0249, -0.5828,\n",
       "           0.0451, -0.2563, -0.0001,  0.0400,  0.0493,  0.0527,  0.1147,\n",
       "           0.0375,  0.0375,  0.1787,  0.0547, -0.2518,  0.0259,  0.0571,\n",
       "           0.0227, -4.4406, -0.5551,  0.0580,  0.0357,  0.0372,  0.0417,\n",
       "           0.0398]]),\n",
       " tensor([[ 0.0496,  0.0308, -0.0269,  0.0464,  0.0337,  0.0449,  0.0380,\n",
       "           0.0548,  0.0506,  0.0329,  0.0246,  0.0167,  0.0404,  0.0309,\n",
       "           0.0434,  0.0875,  0.0051, -0.7182,  0.0379,  0.1972,  0.0772,\n",
       "           0.0343,  0.0559,  0.0261,  0.0368,  0.0475,  0.0398,  0.0567,\n",
       "           0.0396]]),\n",
       " tensor([[ 0.8471,  0.0710,  0.0540, -0.8336, -0.0046,  0.0107,  0.0731,\n",
       "           0.0346,  0.0484, -0.1767,  0.0762,  0.0354,  0.0671,  0.0474,\n",
       "           0.0443,  1.2765,  0.0385,  0.0556, -0.7982,  0.0217, -1.5941,\n",
       "           0.0441,  0.0523, -0.0624,  0.0444,  0.1196,  0.0393,  0.0097,\n",
       "           0.0664]]),\n",
       " tensor([[ 0.0429,  0.0335,  1.4969,  0.0346,  0.0243,  0.1072,  0.0396,\n",
       "           0.0354, -0.2716,  0.0873,  0.0289,  0.0404,  0.0508, -0.2497,\n",
       "           0.0422,  0.0569,  0.7645,  0.0417,  0.0412,  0.0196, -0.7845,\n",
       "           0.1335,  0.0214, -0.3052,  0.0229,  0.0416,  0.0269,  0.0472,\n",
       "           0.1191]]),\n",
       " tensor([[ 0.0501,  0.0389, -4.8201,  0.0044,  0.0385,  0.0352,  0.1028,\n",
       "           0.0668,  0.0272,  0.0551,  0.0104,  0.0977,  0.0503, -3.5037,\n",
       "          -0.2254,  0.0430, -0.8581,  0.0556, -0.0142,  4.5710,  0.0842,\n",
       "           0.0154,  6.8960,  0.1540,  0.0515,  0.0476,  0.0554, -0.4762,\n",
       "           0.1005]]),\n",
       " tensor([[ 0.0385,  0.0235,  0.0616, -0.0017,  0.0414,  0.0151,  0.0493,\n",
       "           0.0762,  0.0578,  0.1012,  0.0754,  0.0212,  0.0192,  0.0677,\n",
       "           0.1078,  0.0534, -0.2071,  0.0316,  0.1731,  0.0277,  1.0117,\n",
       "           0.0328,  0.0570,  0.0570,  0.0682, -1.0671,  0.0407, -1.0518,\n",
       "           0.0507]]),\n",
       " tensor([[ 0.0261,  0.8466,  0.0691,  0.0628,  0.0810, -0.7693,  1.7840,\n",
       "           0.1008,  0.0418,  0.0346,  0.0348,  0.2249,  0.8905,  0.0716,\n",
       "          -0.0559,  0.0611,  0.0527,  0.0485,  0.0946,  0.0407, -0.0812,\n",
       "           0.0484,  4.0901,  0.0249,  0.0245, -0.0069,  0.0479,  0.0435,\n",
       "           0.0284]]),\n",
       " tensor([[-2.9914,  0.0205,  0.0250, -1.6861,  0.0452,  0.0500,  0.0588,\n",
       "           6.5829, -4.6550, -0.0543,  0.0907, -0.0300,  0.0242,  0.0490,\n",
       "           0.0578,  0.0443,  0.2384,  0.0458, -0.1779,  0.0315,  0.0581,\n",
       "           0.0636,  0.0341,  0.0360,  0.0591,  0.1304,  0.0435,  0.0273,\n",
       "          -2.3699]]),\n",
       " tensor([[ 0.8282,  0.0441, -1.4820,  0.0908,  0.0427,  0.0478,  0.0888,\n",
       "           0.1271,  0.0341,  0.0014, -0.0015,  0.0359,  0.0626,  0.0603,\n",
       "           0.0479,  0.0315,  0.0507,  0.0396,  0.0901,  0.0525, -3.5471,\n",
       "           0.0373,  0.0453,  0.0444,  0.0553,  0.0203, -0.0846,  0.0210,\n",
       "          -0.0083]]),\n",
       " tensor([[ 0.0303,  1.5544,  0.0313,  0.0346,  0.0726, -3.7082,  0.0647,\n",
       "          -1.3138,  0.0151,  0.1057,  0.0460, -0.7390,  0.0476,  0.0362,\n",
       "           0.1098,  0.0424,  0.0512,  0.0231,  0.0331,  0.0276,  0.0413,\n",
       "           0.0604,  0.0466,  0.0377,  0.1360,  0.0557, -0.4501,  0.0456,\n",
       "           0.0235]]),\n",
       " tensor([[ 0.1060,  0.0433,  0.0434,  0.0434,  0.0459,  0.0435,  0.0520,\n",
       "           0.0550,  0.1148,  4.5161,  0.0446, -0.1067, -0.0205,  0.0600,\n",
       "           0.0530,  0.0388, -0.3804,  0.0457,  0.0407,  0.0554,  0.0513,\n",
       "           0.0602,  0.0063,  0.0490,  0.1463, -0.0064,  0.0555,  1.2042,\n",
       "          -3.6963]]),\n",
       " tensor([[ 0.0633,  0.0272,  0.0468,  0.0438,  0.0210,  0.0525,  0.0507,\n",
       "           0.1265,  0.0349,  0.0626, -0.0829,  0.0266,  0.0413,  0.0427,\n",
       "           0.0439, -3.8693,  0.0599,  0.0451,  0.1579, -1.3909, -0.0311,\n",
       "           0.0397,  0.0437,  3.7657, -4.6104,  0.0472,  0.0201,  0.0267,\n",
       "           0.0408]]),\n",
       " tensor([[ 0.0410,  0.0557,  0.0446,  0.0415, -0.4580, -4.7711,  0.0881,\n",
       "          -3.3822,  0.0389, -3.2049,  0.0250, -0.0661,  0.9134, -3.7353,\n",
       "           0.0015, -0.4471, -0.0391,  0.4061,  0.0462, -0.0752,  0.0201,\n",
       "           0.1116,  0.0637,  0.0347,  0.0390,  0.0426,  0.0524, -0.0043,\n",
       "           0.1449]]),\n",
       " tensor([[ 0.0504,  0.0183,  0.0524,  0.0426,  0.0457,  0.0438,  0.0564,\n",
       "           0.0848,  0.0489,  0.0638,  0.0576,  0.1132,  0.0077,  0.0024,\n",
       "           0.0592,  0.1073,  0.0383,  0.0169,  0.0666,  0.1101,  0.0033,\n",
       "           0.1573,  0.0415,  0.0873,  0.0475,  0.0330, -0.1815,  0.0406,\n",
       "           0.0555]]),\n",
       " tensor([[-0.3003,  2.3971,  0.0566,  0.0534,  0.0858,  0.0430,  0.0613,\n",
       "           0.0587,  0.0306,  0.0415,  0.0893, -0.0013,  0.0419,  0.0719,\n",
       "           0.0460,  0.0393,  0.0455,  0.0440,  0.0517,  0.0225,  0.0458,\n",
       "           0.0227, -2.6810,  0.0483, -1.5071,  0.0300,  0.0455,  0.0441,\n",
       "          -2.8894]]),\n",
       " tensor([[ 0.1453,  0.1076,  0.0530,  0.0540, -0.0550,  0.0180,  0.0669,\n",
       "           0.0494,  0.0160,  0.0029, -0.2402, -2.7873,  0.0399,  0.0359,\n",
       "           0.0583,  0.0432,  0.0168,  0.0377,  0.0427,  0.5662,  0.0855,\n",
       "           0.0521,  0.0608,  0.0503,  0.0468,  0.0146,  0.0451,  0.0441,\n",
       "           0.0582]]),\n",
       " tensor([[ 0.2125,  0.0405,  0.0426,  0.0713, -0.0522,  2.3928,  0.0154,\n",
       "           0.0741,  0.0460,  0.1094,  0.0421,  0.0519,  0.0497,  0.0569,\n",
       "           0.0459,  0.0459,  0.0292,  0.0425,  0.0453, -0.0977,  0.0013,\n",
       "           0.0712,  0.0341,  0.0496,  0.0102,  0.0405,  0.1163,  0.0476,\n",
       "           0.0502]]),\n",
       " tensor([[-0.0022,  0.0539,  1.4693,  0.0418,  0.0452,  0.0466,  4.2332,\n",
       "           0.0618,  0.0528,  0.4146,  0.0750,  0.0242, -0.0046,  0.0423,\n",
       "           0.0554,  0.0146, -0.0113,  0.0430,  0.0213,  0.0641, -4.4852,\n",
       "           0.0417,  0.1227,  0.0550,  0.0452, -2.5301, -0.2289,  0.1194,\n",
       "           0.1130]]),\n",
       " tensor([[ 0.0217, -0.9301,  0.0268,  1.7468,  0.0217,  0.1022, -0.8686,\n",
       "           0.0420,  0.0523,  1.0669,  0.1093,  0.0068,  0.0352,  0.0720,\n",
       "           0.0199,  0.0484,  0.0486,  0.0556,  0.0352,  0.0101,  0.0222,\n",
       "           0.1140,  0.0590,  0.0405,  0.0122, -0.8837,  0.0111, -1.6298,\n",
       "           0.0831]]),\n",
       " tensor([[ 0.0167,  0.0708,  0.8060,  0.0820,  0.0450,  0.0474,  0.0027,\n",
       "           0.0562,  0.0930,  0.0541,  0.0973,  0.0610, -0.6197,  0.0618,\n",
       "           0.0702, -2.7006,  0.0445,  0.0359,  0.0538,  0.0335,  0.0520,\n",
       "           0.0592, -4.7842,  0.0948,  0.0482,  0.1163,  2.2830,  0.0030,\n",
       "           2.5252]]),\n",
       " tensor([[ 0.0566,  0.0227,  0.1328, -0.4116,  0.0193,  0.0752,  0.0394,\n",
       "          -0.0078,  0.0916,  0.2804,  0.0417,  0.0822, -0.8777, -1.4352,\n",
       "           0.0689,  0.0562,  0.0523,  0.0684,  0.0167,  0.0507,  0.1032,\n",
       "           0.0567,  0.0482, -0.7488, -0.0592,  0.0601,  0.0730,  0.0465,\n",
       "           0.0623]]),\n",
       " tensor([[ 0.0086,  0.1260,  0.0267,  0.0521,  0.1161,  0.0505,  4.1764,\n",
       "           0.2290,  0.0346,  0.0486,  0.0570,  0.0644,  0.0546,  0.0823,\n",
       "           0.0729,  0.0354, -0.3487,  0.0617, -0.3735,  0.0807,  0.0665,\n",
       "           0.0549,  0.0348,  0.0601,  1.0452,  0.0717,  0.0247,  0.0425,\n",
       "           0.0356]]),\n",
       " tensor([[ 0.0530,  0.0968,  0.0596,  0.0936,  0.0683,  0.0501,  0.1275,\n",
       "          -4.4056,  0.1164, -4.1410,  0.0472,  0.0571,  0.1228,  0.0990,\n",
       "           0.0465,  0.0607,  0.0740,  0.0510,  0.0475,  0.0901,  0.0470,\n",
       "           0.0464,  0.2104,  0.0523,  0.0517,  0.1493,  0.0440,  0.0761,\n",
       "           0.0809]]),\n",
       " tensor([[ 0.0447,  0.0538,  0.3475,  0.1069,  0.0790,  0.5067, -1.3061,\n",
       "           0.0394,  0.1012, -1.6616,  0.0820,  0.0610,  0.0368,  0.0530,\n",
       "           0.0801,  0.0127,  0.1429,  0.0252,  0.1039,  0.0485, -3.2694,\n",
       "          -0.2891,  0.1047,  0.0924,  0.0947,  0.0713,  0.0407,  0.0577,\n",
       "           0.0950]]),\n",
       " tensor([[ 0.0303,  0.0705,  0.0511,  0.0946,  0.0556,  0.0540,  0.0510,\n",
       "           0.1654, -0.8421,  0.0347,  0.0352,  0.0493,  0.0423,  0.0682,\n",
       "          -4.6911,  0.0386,  0.1074,  0.0429,  0.0383,  0.0353,  0.0551,\n",
       "           0.0361,  0.0735,  0.1398, -1.9178,  0.0853, -0.7565,  0.0539,\n",
       "           0.0168]]),\n",
       " tensor([[ 0.0469,  0.0511,  0.0924,  0.0203,  0.1062, -1.2456, -0.0315,\n",
       "           0.0015,  0.0371,  0.0210, -0.0411,  0.0294,  0.0315, -0.8657,\n",
       "           0.0545, -0.1544,  0.0433,  0.0877,  0.0480,  0.0945,  0.0332,\n",
       "           0.0197,  0.0338,  0.0306, -0.9155,  0.0492,  0.1010,  0.0528,\n",
       "           0.0611]]),\n",
       " tensor([[ 0.0535,  0.0697,  0.0643,  0.0488,  0.0233,  0.0409,  0.1026,\n",
       "           0.0462,  0.0473,  0.0574,  0.0865,  1.1868, -0.4871,  0.1033,\n",
       "           0.0487,  0.1589,  0.0591, -0.4483,  0.0476, -0.2632,  0.0139,\n",
       "           0.0170,  0.0530,  0.0625,  0.0956,  0.0345,  0.0475,  0.0607,\n",
       "          -0.2174]]),\n",
       " tensor([[ 0.0305,  0.1167,  0.0879,  0.0532,  1.2549,  0.0523,  0.0418,\n",
       "           0.0773,  0.0403,  0.0476,  2.3383,  0.0421,  0.0661, -0.1415,\n",
       "           0.0357,  0.0301,  0.0376,  0.0497,  0.0214,  0.1410,  0.0548,\n",
       "           0.0796,  0.1599,  0.0194,  0.0552,  0.0603,  0.0536,  0.0354,\n",
       "          -2.1017]]),\n",
       " tensor([[-0.3032, -0.0442,  0.1962,  0.0685,  0.0646, -3.3570,  0.0367,\n",
       "           0.0539, -0.0417,  0.0584,  0.0475, -0.2066,  0.0431, -0.7887,\n",
       "           0.1131,  0.0226,  0.0516,  0.0202, -3.8545, -3.1672,  0.1076,\n",
       "           0.0785,  5.6061,  0.0391, -0.0737,  0.0263,  0.0342, -0.0024,\n",
       "           0.0394]]),\n",
       " tensor([[ 0.0096,  0.1548,  0.0425,  0.0578,  0.0601,  0.0334,  0.0405,\n",
       "           0.0659,  0.0395,  0.0282,  0.1046,  0.0556,  0.0406,  0.0885,\n",
       "          -1.7403,  0.0407,  0.0941,  0.7608,  0.0392,  0.0077, -4.5083,\n",
       "          -0.0061,  0.0361,  0.0520, -4.5314,  0.0336,  0.0813, -0.0704,\n",
       "          -0.5695]]),\n",
       " tensor([[-0.0363,  0.0456,  0.0362,  0.0356,  0.0451,  5.3917,  0.0404,\n",
       "           0.0242,  0.0095,  0.0593,  2.3656,  0.0047, -0.3434,  0.0701,\n",
       "           0.0377,  0.3467,  0.0311,  0.0504, -2.5132,  0.0306,  0.0386,\n",
       "           0.0313,  0.2022,  0.0782,  0.0250,  0.1128, -0.0119,  3.3760,\n",
       "           0.0417]]),\n",
       " tensor([[ 0.0308,  0.0302,  0.0388,  0.0345, -0.0011,  0.0304,  0.0025,\n",
       "           0.0357,  0.0291,  0.1218,  0.0394, -1.0983,  0.0670,  0.0239,\n",
       "           0.0386, -0.2704, -2.8342,  0.0314, -4.1363,  0.0413,  2.7759,\n",
       "           0.1027,  0.1232,  0.0342,  0.1275,  0.0017, -0.4714,  0.0895,\n",
       "           0.0277]]),\n",
       " tensor([[ 0.0313,  0.0305,  0.1332,  0.0383,  0.0466,  1.3696,  0.0224,\n",
       "          -0.0555,  0.0373,  0.0988,  0.1600,  0.0339,  0.0533,  0.0686,\n",
       "          -0.2001,  0.0920,  0.0369,  0.0279,  0.0422,  0.0224,  0.0058,\n",
       "           0.0349,  0.0213,  0.0248, -1.0947,  0.0528,  0.1029,  0.0561,\n",
       "           0.0411]]),\n",
       " tensor([[-0.9944,  0.0489,  0.0356, -0.1235,  0.0193,  0.0259,  0.0104,\n",
       "           0.1018,  0.0461,  0.0522,  0.0433,  0.0796,  0.0192,  0.0599,\n",
       "          -0.7315,  0.0747,  0.3053, -0.5194, -3.7236,  0.0623,  0.1162,\n",
       "          -4.2848, -0.0399, -0.4393, -0.0051,  0.0228,  0.0681, -0.0071,\n",
       "           0.0059]]),\n",
       " tensor([[ 0.0860, -0.8217,  0.0301,  0.0613,  0.0416,  0.0360,  0.0315,\n",
       "           0.0426,  0.2470,  5.3698, -0.0090,  0.0193,  0.0650,  0.0268,\n",
       "           0.0580, -0.0026, -1.0293,  0.0518,  0.0531,  0.0471,  0.0461,\n",
       "           0.0530,  0.0364,  0.0805,  0.0765, -3.8329,  0.1224,  0.1443,\n",
       "           0.0180]]),\n",
       " tensor([[ 0.0526,  0.0282,  0.0435,  0.0535,  0.3061,  0.1364,  0.1788,\n",
       "          -0.7219,  0.0442, -3.7875,  0.0272,  0.0459,  0.0911,  0.0323,\n",
       "           0.0393,  0.0421, -0.0091,  0.1626,  0.0432,  0.0611,  0.0441,\n",
       "           0.0628,  0.0482,  0.1844,  0.0531,  0.0359,  0.0448, -1.8489,\n",
       "          -0.8567]]),\n",
       " tensor([[ 0.0333,  0.0729, -2.3267,  0.0357, -0.0049,  0.0322, -4.3223,\n",
       "           0.0094,  0.0194, -0.0022,  0.0644,  1.1779,  0.0826, -0.0108,\n",
       "          -0.0382,  0.0395,  0.0339,  0.0247,  0.0680,  0.1219, -2.0126,\n",
       "           0.0683,  0.0354,  0.0234,  0.0544,  0.0986,  0.0267,  0.0376,\n",
       "           0.0563]]),\n",
       " tensor([[ 0.0250,  0.0004,  0.0340,  0.0362,  0.0115, -3.3290,  0.1218,\n",
       "          -0.0114,  0.0369,  0.0655, -0.2909, -0.0017, -0.3239,  0.1270,\n",
       "           0.0379,  0.0082,  0.0376,  0.0087,  0.0824,  0.0018, -0.0990,\n",
       "           0.4214,  0.0227,  0.0740,  0.1283,  0.0018, -0.2754,  0.1362,\n",
       "           0.0338]]),\n",
       " tensor([[ 0.0432,  0.0340,  0.0322,  0.0365,  0.0425,  0.0654, -0.3602,\n",
       "          -2.3307, -0.7722, -1.4202,  0.0324,  0.1339,  0.0855,  0.0443,\n",
       "           0.0587,  0.0124,  0.0657,  1.2643,  0.0511,  0.0534,  0.0055,\n",
       "          -3.6252,  0.0581, -0.0090, -3.5962,  0.0368,  0.0363,  0.0402,\n",
       "           0.0274]]),\n",
       " tensor([[ 0.0364, -0.4808,  0.1735,  0.0467, -4.8589,  0.0954,  0.0770,\n",
       "           5.7833,  0.1470,  0.0290,  0.0449, -0.0054,  0.0605,  0.5501,\n",
       "           0.0898,  0.0563,  0.0788,  0.0575,  0.0586,  0.0266, -0.1339,\n",
       "           0.0352,  1.4601,  0.0375, -1.4869,  0.0485,  0.0623,  0.2452,\n",
       "           0.0407]]),\n",
       " tensor([[ 0.1090,  0.0636,  0.0375, -0.0073,  0.0396,  0.0429,  0.0954,\n",
       "           0.0225,  0.0252,  0.0616,  0.0153,  0.0567,  0.0846,  0.0417,\n",
       "           0.0840,  0.0252, -2.6684,  0.0124,  0.0394,  0.1451,  0.0368,\n",
       "           0.0411,  0.0733,  0.0390,  0.0382,  0.0441,  0.0304,  0.0412,\n",
       "           0.0351]]),\n",
       " tensor([[ 0.0402,  0.0390,  0.0362,  0.0289, -4.5597,  0.0294,  0.8560,\n",
       "           0.0398,  0.0613,  0.0346,  0.0439,  0.0575,  0.0337,  0.0099,\n",
       "           0.0610,  0.0975,  0.0378,  3.2204, -0.0005,  0.0457,  0.0371,\n",
       "           0.0217,  0.0546,  0.0785,  0.0684,  0.0528,  0.0458,  0.0312,\n",
       "           0.0392]]),\n",
       " tensor([[ 0.0300,  0.0225,  0.0297,  0.0366,  0.0477,  0.0803,  0.0364,\n",
       "           0.0814,  0.0200, -0.0126,  0.1345,  0.0283,  0.0373, -0.5444,\n",
       "           0.0198,  0.0098,  0.0416,  0.0177,  0.0682,  0.0328,  0.0579,\n",
       "           0.0763,  0.1265,  0.0408,  0.0292,  0.0042,  0.0190,  0.0072,\n",
       "           0.0488]]),\n",
       " tensor([[ 0.0687,  3.6314,  0.0479,  0.0165,  0.0645,  0.0422,  0.0516,\n",
       "           0.0554,  0.0327,  0.0449,  0.0463,  0.0657,  0.0698,  0.0320,\n",
       "           0.0402,  0.0120,  0.0672,  0.0302,  0.0349,  0.1860,  0.0378,\n",
       "           0.0057,  0.0366,  0.0548,  0.0267,  0.0503,  0.1314,  0.0695,\n",
       "           0.0541]]),\n",
       " tensor([[ 0.0415, -0.0149,  0.0584, -4.8110,  0.1480,  0.0379,  0.0357,\n",
       "          -4.4754,  0.0355,  0.0246,  0.0033, -0.0002,  0.0856,  0.0892,\n",
       "           0.0407,  0.0371,  0.0932,  0.0542,  0.0079,  0.0396,  0.0621,\n",
       "           0.0061,  0.0183,  0.0882, -0.1786,  0.0384,  0.0180,  0.0272,\n",
       "           0.0740]]),\n",
       " tensor([[ 0.0572,  0.0137,  0.0324,  0.0718,  0.0387,  0.0527,  0.1154,\n",
       "          -0.0798,  0.0413,  0.0291, -0.0169,  0.0501, -0.2757,  0.0943,\n",
       "           0.0436, -0.0595,  0.0544,  0.0160, -0.0774,  0.0933,  0.0596,\n",
       "           0.0243,  0.1280,  0.0735,  0.1227,  0.1043,  0.0308,  0.0586,\n",
       "           0.0502]]),\n",
       " tensor([[ 0.0583,  0.0443,  0.0305,  0.0454,  0.0396,  0.0146,  0.0848,\n",
       "           0.0050,  0.0926, -1.5300,  0.0497,  0.0221,  0.0473,  0.0178,\n",
       "           0.0296,  0.1195,  0.0458,  0.1793,  0.0278,  0.0336, -0.0596,\n",
       "           0.1176,  0.0352,  0.0107,  0.0845,  0.0345, -1.9010, -0.8924,\n",
       "           5.0591]]),\n",
       " tensor([[ 2.8117,  0.0414,  0.0436,  0.0321,  0.0316, -0.5309,  0.0387,\n",
       "           0.0436,  0.0940, -4.5293,  0.0487,  0.0085, -0.1991,  0.0441,\n",
       "           0.0675, -1.0684, -0.5042,  0.1501, -0.2877, -0.0088, -0.8634,\n",
       "           0.0257,  0.0938,  0.0354,  0.0447,  0.0425,  0.0363, -0.2058,\n",
       "           0.0533]]),\n",
       " tensor([[ 0.0922,  0.0293, -0.9890,  0.0296, -0.4066,  0.0389,  0.0762,\n",
       "           0.0602,  0.0822,  6.4305, -2.3054,  0.0466,  0.0446,  0.0347,\n",
       "          -1.6794,  0.0314,  0.0178,  0.0972,  0.0687,  0.0517,  0.0107,\n",
       "           0.0519,  0.0334,  0.0344,  0.1393,  0.0122, -0.3033,  0.0345,\n",
       "          -0.7898]]),\n",
       " tensor([[ 0.0508, -0.5739,  0.0106,  0.0093,  0.0120,  0.0222,  0.0378,\n",
       "           0.0338,  0.0239,  0.0694,  2.1432,  1.9315,  0.0457,  0.0792,\n",
       "           0.0384, -4.2115,  0.0254,  0.0644,  0.0322,  0.0289,  0.0313,\n",
       "           0.1389, -0.0998,  0.0343,  0.0341,  0.0401,  0.0530,  0.0342,\n",
       "          -2.2176]]),\n",
       " tensor([[ 0.0301,  0.0938, -4.2161,  0.0754,  0.0253,  0.0316,  0.0307,\n",
       "           0.0214,  0.0304,  0.4021,  0.0485, -0.0057,  0.0255,  0.0097,\n",
       "           5.8749, -0.3243, -1.2652,  0.1008,  0.0235,  0.0339,  0.0503,\n",
       "           0.0231, -2.2735,  0.0437,  0.0353,  0.0361,  0.5673,  0.0731,\n",
       "          -0.9949]]),\n",
       " tensor([[ 0.0201,  0.0448,  0.0573,  0.0602,  0.0147,  0.0305,  0.0328,\n",
       "           0.0421, -0.2153, -3.6937, -3.3009, -2.5899,  0.2737, -2.0628,\n",
       "           0.0328, -0.2695, -4.5483,  0.0242,  0.1183,  0.0082,  0.0378,\n",
       "           0.1130,  0.0784,  0.0225,  0.0321,  0.0548,  0.0755, -0.0334,\n",
       "           0.0777]]),\n",
       " tensor([[ 0.0371,  0.0305,  0.0735,  0.0513,  0.0788,  0.0137,  0.0499,\n",
       "          -0.3959,  0.0336,  0.1493,  0.0381,  0.0355,  0.0882,  2.7209,\n",
       "           0.0413,  0.0152, -0.0234,  0.0255,  0.0339,  0.0406,  0.0177,\n",
       "           1.9870,  0.0361,  0.0251,  0.0408,  0.0278,  0.0369,  0.0059,\n",
       "          -1.1218]]),\n",
       " tensor([[ 0.0256,  0.0380,  0.0078, -2.2566,  0.1367,  0.0360, -0.5070,\n",
       "           0.0185,  0.0742,  0.0379,  0.0307, -3.6154,  0.0414,  0.0802,\n",
       "           0.1408, -0.8497, -0.1096,  0.0793,  0.0414,  0.0438,  0.1691,\n",
       "           0.0399,  0.0391,  0.0384,  0.0237,  0.0161,  0.0495,  0.0579,\n",
       "           0.0233]]),\n",
       " tensor([[ 0.0305, -1.0028,  0.0614,  0.0299, -0.6349,  0.0517,  0.0789,\n",
       "           0.0991, -0.0195,  0.0580,  0.0307,  0.0967,  0.0351, -0.5972,\n",
       "           0.0380,  0.0589, -0.1667, -0.1563,  0.0033,  3.0830, -0.0227,\n",
       "           0.1633,  0.1322,  0.0816,  0.0371,  0.0876,  0.0339,  0.0416,\n",
       "           0.0382]]),\n",
       " tensor([[-3.2844,  0.0463,  0.0926, -0.2339,  0.0285,  0.1704,  0.0295,\n",
       "           0.0228, -0.3175,  0.0328, -0.6035, -0.0966,  0.0482,  0.1548,\n",
       "           0.0457,  0.0338,  0.0283,  0.3689, -1.6756,  0.0239, -1.1889,\n",
       "          -0.2856,  0.0320, -0.2828, -0.5865, -3.7273,  0.0628,  0.0373,\n",
       "           0.1419]]),\n",
       " tensor([[ 0.0482,  0.0365,  0.0327,  0.0145,  0.0276,  0.6938,  0.1083,\n",
       "           0.6233,  0.1270,  0.0635,  0.0361,  0.0412,  0.0525,  0.0322,\n",
       "           0.9174,  0.0299,  0.0409,  0.1782,  0.0168,  0.0098,  0.0329,\n",
       "           0.0554,  0.0264, -0.2343,  0.0358,  6.8761, -0.2303,  0.0524,\n",
       "           0.0531]]),\n",
       " tensor([[ 0.0282,  0.0995,  0.0158, -2.0505, -3.5125,  0.0709,  0.0617,\n",
       "           0.0352,  0.0070,  0.0657,  0.0300,  0.0355,  0.0292, -0.0056,\n",
       "          -0.2158,  0.0369,  0.0524, -0.0003,  0.1628,  0.0201,  0.0515,\n",
       "           0.0461,  0.0035,  0.1143,  0.0143, -0.0502, -3.0270,  0.0710,\n",
       "           0.0276]]),\n",
       " tensor([[ 9.9336e-03,  3.6295e-02,  4.6367e-02, -4.5907e-02,  9.1642e-02,\n",
       "           3.6616e-02,  3.3443e-02,  1.6907e-02,  6.8322e-02,  1.1776e-01,\n",
       "           4.1256e-02,  1.1387e-02,  3.9983e-02,  8.6993e-05,  6.6553e-02,\n",
       "           1.2063e-01,  2.9413e-02,  3.5239e-02, -3.4009e+00, -9.6609e-03,\n",
       "           7.1542e-02,  6.0458e-02,  1.2675e-01,  2.7419e-02,  1.4639e-01,\n",
       "           3.3261e-02,  3.6011e-02,  4.6476e-02,  6.6990e-02]]),\n",
       " tensor([[ 0.0319,  0.0395,  0.0828,  0.0319,  0.0209,  0.0398,  0.0043,\n",
       "          -0.4568, -0.1383,  0.0318,  0.0365,  0.0346, -0.0498,  0.0373,\n",
       "           0.0443,  0.0742,  0.0359,  3.9018,  0.0646, -1.2452, -4.2767,\n",
       "           0.0283,  0.0137,  0.0289,  0.0331,  0.1715, -0.1414,  0.0736,\n",
       "           0.0278]]),\n",
       " tensor([[ 0.0303,  0.1145,  0.0351,  0.0876,  0.0781,  3.5542,  2.6450,\n",
       "           0.0416,  1.4140,  0.0105,  0.0787,  0.0012,  0.0505, -0.4815,\n",
       "           0.0367,  0.1171,  0.0349,  0.0363,  0.0407,  0.0289,  0.0347,\n",
       "          -0.0287,  0.0690, -0.1672, -0.0046,  0.0047,  0.0099, -4.4328,\n",
       "          -0.0513]]),\n",
       " tensor([[-4.3049,  0.0322,  0.1043,  0.0455,  0.0549, -0.0023,  0.8013,\n",
       "           0.1824,  0.0557,  0.1784,  0.0362,  0.1080,  0.0969,  0.0264,\n",
       "           0.0913,  0.0234, -1.1109,  0.0724,  0.0857, -2.3319,  0.0325,\n",
       "          -1.2653,  0.0178,  0.0377,  0.0377,  0.0192,  0.0222,  0.0399,\n",
       "           0.0361]]),\n",
       " tensor([[ 2.3644e-02,  9.4555e-02,  1.2366e-02,  5.4148e-02,  3.3515e-02,\n",
       "           1.4489e-02, -1.8994e+00,  2.7912e-02,  1.3028e-02,  2.5237e-02,\n",
       "           1.1221e-05,  3.4975e-02,  1.2872e-01,  2.2264e-02, -8.0788e-01,\n",
       "           5.5339e-02,  3.4063e-02,  2.1718e-02,  6.1017e-02,  4.9289e-02,\n",
       "           5.2144e-03, -7.4395e-02,  2.6817e-02, -3.7555e+00,  7.0421e-02,\n",
       "           1.6410e-02,  4.1181e-02, -1.7785e-01,  7.9773e-02]]),\n",
       " tensor([[ 0.0295,  1.4574,  0.0042,  0.0594, -3.6519, -0.1319,  0.0479,\n",
       "           0.0196,  0.0202,  0.0198,  0.0510,  0.0855,  0.0061,  0.0292,\n",
       "           0.0299,  0.0345, -0.1299,  0.0211,  0.0349,  0.0248,  0.0310,\n",
       "           0.7187,  0.0181,  0.1230,  0.0214,  0.0093,  0.0536, -0.8530,\n",
       "           0.0630]]),\n",
       " tensor([[ 0.7600,  0.0119, -0.1830, -0.3055, -0.1129,  0.0212,  0.0301,\n",
       "           0.1160,  0.0430,  0.0595,  0.0292,  0.0591, -3.7053,  0.0316,\n",
       "           0.0342, -0.7267,  0.0564,  0.0416,  0.0283,  0.0276,  0.0423,\n",
       "           0.0276, -0.5353,  0.1069,  0.0385,  0.0612,  0.0513,  0.0435,\n",
       "           0.0711]]),\n",
       " tensor([[-4.1707, -0.0825,  0.0510,  0.0481, -0.2853,  0.0209, -0.4914,\n",
       "           0.0092, -0.1425,  0.0426,  0.1457,  0.0375,  0.0317,  0.0354,\n",
       "           0.0319,  0.0652, -0.4485,  0.0365,  0.0377, -0.0260,  0.0359,\n",
       "           0.0043,  0.0921,  0.0471,  0.1056,  0.0833,  0.1054,  0.1433,\n",
       "           0.0379]]),\n",
       " tensor([[ 0.0179,  0.0393,  0.1467,  0.0551,  0.2418,  0.1616,  0.0446,\n",
       "           0.0778,  0.0256,  0.0355,  0.0524,  0.0281,  0.1352,  0.0671,\n",
       "           0.0909,  0.0359,  0.0289,  0.0832,  0.0152,  0.0556,  0.0206,\n",
       "           0.0477,  0.0346,  0.0346, -0.0026,  0.0360, -3.6075,  0.0700,\n",
       "           0.0280]]),\n",
       " tensor([[ 0.0133,  0.0406,  0.0732,  0.0507,  1.0329,  0.0226,  0.0634,\n",
       "           0.0331, -0.6130,  0.0242,  0.0692,  0.0381,  0.0275,  0.0644,\n",
       "          -0.1191,  0.0265, -0.7882, -0.0836,  0.0468,  0.0168, -1.4101,\n",
       "           0.0835,  0.0226,  0.0250,  0.4088,  0.0187,  0.0196, -0.1688,\n",
       "           0.0203]]),\n",
       " tensor([[-0.1251,  0.0434, -1.2884,  0.0579,  0.0366, -0.0004,  0.0688,\n",
       "           0.0212,  0.0144,  0.0666,  0.0378,  0.0343,  0.0245,  0.0336,\n",
       "          -0.2952,  0.0601,  0.0179,  0.1165,  0.0491,  0.0282,  0.0865,\n",
       "          -2.6287,  0.0271,  0.0508,  0.0290,  0.0314,  0.0053,  0.0438,\n",
       "          -0.0829]]),\n",
       " tensor([[ 0.1006,  0.0319, -0.0012,  0.0296, -0.0011,  0.0685,  0.0546,\n",
       "          -0.0250,  0.0353, -1.0459,  0.1077,  0.0458,  0.0016,  0.0628,\n",
       "           0.0308,  0.0376,  0.0704,  0.0282,  0.0345,  0.0578, -1.1432,\n",
       "           0.1153, -2.1564, -0.5028,  0.0300,  0.0401, -0.2988,  0.0295,\n",
       "          -1.8232]]),\n",
       " tensor([[ 0.1109,  0.1137, -2.1976,  0.0253, -0.4075, -0.0064,  0.0243,\n",
       "           0.0190, -0.0487,  0.0844,  0.0381,  0.0181,  0.0520,  0.0632,\n",
       "          -3.0862,  0.0811,  0.0260,  0.0504, -2.5063,  0.0192,  1.4750,\n",
       "           0.0148,  0.3545,  0.0251,  0.0362,  0.0307,  0.0444, -1.9036,\n",
       "          -3.3646]]),\n",
       " tensor([[ 0.1154,  0.1402,  0.0154,  0.0292,  0.1377,  0.0418,  0.0326,\n",
       "           0.0434,  0.0290,  0.1177,  0.0370, -2.5051,  0.0265,  0.0819,\n",
       "           0.0198,  0.0282,  0.0395,  0.0880,  0.0029,  0.0836,  0.0444,\n",
       "           0.0554, -0.0206,  0.0153,  0.1001, -4.5122,  0.0349, -0.0157,\n",
       "          -0.0390]]),\n",
       " tensor([[-0.0261,  0.0026,  0.0557,  0.0949,  0.1427,  0.0125,  0.0383,\n",
       "           1.0288,  0.0232,  0.0289,  0.0584,  0.0101,  0.0318, -0.0267,\n",
       "           0.0565, -0.0233, -0.0014,  0.0301,  0.0264,  0.0624,  0.0082,\n",
       "          -0.4722,  0.0609,  3.7096,  0.0502,  0.0310,  0.0083,  0.1079,\n",
       "           0.0522]]),\n",
       " tensor([[ 0.0322,  0.0125,  0.0194,  0.0213, -2.5728, -0.0400,  0.0597,\n",
       "           0.1591,  0.0268, -5.0892,  0.0870,  0.0597,  0.0360,  0.0357,\n",
       "           0.0255,  0.0716,  0.0344,  0.0779, -0.1554,  0.0301,  0.0863,\n",
       "          -0.3852,  0.0253,  0.0783,  0.0304,  0.0846,  0.0213,  0.0874,\n",
       "           0.0419]]),\n",
       " tensor([[ 0.0238,  0.0606,  0.0266, -0.0119,  0.0589, -0.0156,  0.0369,\n",
       "           0.0357,  0.0222,  0.0331,  0.0103,  0.0188,  0.0302,  0.0489,\n",
       "          -0.0716,  0.0377,  0.0623,  0.0063, -0.0829,  0.0231,  0.0149,\n",
       "           2.6701,  0.0274,  4.9071,  0.0490,  0.0483,  0.0278, -0.0104,\n",
       "           0.0507]]),\n",
       " tensor([[ 2.9397, -1.7409,  0.0405,  0.0643,  0.0301,  0.0402,  0.0242,\n",
       "           0.0317, -0.0021, -1.1034,  0.0386,  0.0510,  0.1002,  0.0518,\n",
       "          -0.6922,  0.0312, -4.4787,  0.2313,  0.1344,  0.0385,  0.0767,\n",
       "           0.0158,  0.0284,  0.0300,  0.0258, -0.6553,  0.0670,  0.0505,\n",
       "           0.0229]]),\n",
       " tensor([[ 0.0452,  0.0119,  0.0178,  0.0314, -0.3251, -0.6226,  0.0867,\n",
       "          -0.6566,  0.0720,  0.0186,  5.5116,  0.0644,  0.0801,  0.3628,\n",
       "           0.0191,  0.0309,  2.3195,  0.0129,  0.0280,  0.0314,  0.0320,\n",
       "           0.0379,  0.0800,  0.0392,  0.0678,  0.0238,  0.0310,  0.0366,\n",
       "           0.0333]]),\n",
       " tensor([[-0.0157,  0.0745,  0.0369,  0.0284,  0.0859,  3.8386,  0.0187,\n",
       "           0.0085,  0.0370,  0.0556,  0.0428,  0.0347,  1.5163, -0.0551,\n",
       "           0.0827,  0.0688,  0.1457,  0.0207,  0.0350,  0.0356,  0.0407,\n",
       "           0.0031, -0.1492,  0.0343,  0.0348,  0.0780,  0.0116,  0.0804,\n",
       "           0.0163]]),\n",
       " tensor([[ 0.0003,  0.0495,  0.0504,  0.0394,  0.0410,  0.0352,  0.0392,\n",
       "           0.3735,  0.0634,  0.0578,  0.0525, -0.0098, -2.3480,  0.0262,\n",
       "           0.0097, -1.0215,  0.0399,  0.0396,  0.0913,  0.0072,  0.0981,\n",
       "           0.0535,  0.0119,  0.1006,  0.0225,  0.0498,  0.0294,  0.0213,\n",
       "          -0.1373]]),\n",
       " tensor([[ 0.0175,  0.0083,  0.1579,  0.0061,  0.0480,  0.0569,  0.0231,\n",
       "           0.0050,  0.0497,  4.6631,  0.0340, -3.6596, -0.0087,  0.0313,\n",
       "           0.0526,  0.0417,  0.0379,  0.0077,  0.0319,  0.0361,  0.0302,\n",
       "           0.5278, -0.4264,  0.1253,  0.0612, -0.0242,  0.0852,  0.6764,\n",
       "          -2.4443]]),\n",
       " tensor([[ 0.0129,  0.0606,  0.0340,  0.0349,  0.1812,  0.0402,  1.0510,\n",
       "           0.0354,  0.0209,  0.0612,  0.0378,  0.0532, -0.0383,  0.0775,\n",
       "           0.0472,  0.0592,  0.1266, -0.2497,  0.0252, -0.0520,  0.0366,\n",
       "          -0.1670,  0.0329,  0.0445,  0.0319,  0.0411,  0.1245,  0.0418,\n",
       "           0.0377]]),\n",
       " tensor([[ 0.0444,  0.0436,  0.0480, -0.4868,  0.0404,  0.0487, -1.9349,\n",
       "           0.0410,  0.0173,  0.0389,  0.0417,  0.0360,  0.0426,  0.4545,\n",
       "           0.1281, -1.0346, -0.1104,  0.0448,  0.0407,  0.0231, -2.2844,\n",
       "           0.0218,  0.0361,  0.0578,  0.0485,  0.0369,  0.0547,  0.0075,\n",
       "           0.0517]]),\n",
       " tensor([[ 0.0441,  0.0659,  0.0362,  0.1075,  0.0564, -1.2525,  0.0105,\n",
       "           0.0447,  0.0380,  0.0618,  0.0451,  0.9990, -0.0320,  0.0328,\n",
       "           0.0683, -0.5392,  0.0168,  0.0373, -0.6224,  0.0390,  0.0872,\n",
       "           0.0114,  0.0289,  0.0483,  0.0409, -0.5070,  0.0311,  0.0029,\n",
       "           0.0096]]),\n",
       " tensor([[-0.0005,  0.0345,  0.0440,  0.1024, -0.5933,  0.0856,  0.0457,\n",
       "           2.2474,  0.0411,  0.0363,  0.0408,  0.0292, -3.5314,  0.1250,\n",
       "           0.0227,  1.9495,  0.0389,  0.0283,  0.0189,  0.0544,  0.0824,\n",
       "          -2.4242,  0.1218,  0.0021,  0.0621,  0.2962,  0.0375,  0.0401,\n",
       "           0.0437]]),\n",
       " tensor([[ 0.0423,  0.0844,  0.0460,  0.0231,  0.0468,  0.0454,  0.0415,\n",
       "           0.0929,  1.4791,  0.0328,  0.0885,  0.0048,  0.0348,  0.0817,\n",
       "           0.0618,  0.0560,  0.1625,  0.0311,  0.0640,  0.0598,  0.1112,\n",
       "           0.1047,  0.0353,  0.0457,  0.0885, -0.3520,  0.0548, -0.0069,\n",
       "           0.1324]]),\n",
       " tensor([[ 0.0461,  0.0251,  0.1061,  0.0430,  0.1766,  0.0340,  0.0362,\n",
       "           0.0200,  0.8893,  0.0362,  0.0425, -0.0926, -3.8382,  0.0949,\n",
       "           0.0166,  0.0310,  0.0565,  0.0624,  0.0402,  0.0928,  6.5138,\n",
       "          -0.5104,  0.0265, -0.2060,  0.0518, -1.8856,  0.0327,  0.0405,\n",
       "          -0.1735]]),\n",
       " tensor([[-1.5520,  0.0544,  0.0884,  0.0187,  0.0409, -1.4988,  0.0529,\n",
       "           0.1034,  0.0389,  0.0256,  0.0514,  0.0474,  0.0316,  0.0627,\n",
       "           0.0471, -1.5120,  0.0291,  0.0245,  0.0361, -0.0834,  0.0319,\n",
       "           0.3867,  0.0640,  0.5320,  0.0170,  0.0357,  0.0315, -1.6056,\n",
       "           0.0287]]),\n",
       " tensor([[ 0.0385,  0.0098,  0.0722,  0.0345,  0.0783, -3.0782,  0.0211,\n",
       "           0.0359,  0.0115,  0.0763,  0.0362,  0.0323,  0.0136,  0.0862,\n",
       "           0.1060,  0.3579,  0.0978,  0.0432,  0.0183, -0.0049,  0.0111,\n",
       "          -0.0036,  0.0382,  0.0391,  0.1347,  0.0345,  0.0653,  0.0683,\n",
       "          -2.4934]]),\n",
       " tensor([[ 0.1349,  0.0987,  0.0548,  0.0292,  2.1973,  0.0640,  0.0468,\n",
       "           0.0833,  0.0452,  0.0471,  0.0275, -0.8942,  0.0395,  0.4055,\n",
       "           0.0628,  2.3008, -0.3292,  0.0404,  0.0392,  0.0164,  0.0325,\n",
       "           0.0638,  0.0570, -4.6060, -1.4208,  0.0377,  0.0114,  0.0214,\n",
       "          -0.1482]]),\n",
       " tensor([[ 0.0508, -0.0018,  0.0605,  0.0480,  0.0717,  0.0358,  0.0626,\n",
       "           0.0421,  0.0406,  0.0367, -0.5871,  0.0619,  0.0473,  0.0451,\n",
       "           0.1068,  0.0196,  0.0589,  0.0734,  0.0537,  0.0341,  0.1142,\n",
       "           0.0858,  0.0371,  0.0672,  0.0532,  0.0310,  0.1650,  0.0252,\n",
       "           0.0239]]),\n",
       " tensor([[-2.0319,  0.0453,  0.0392,  0.1131,  0.0367,  0.0729,  0.0500,\n",
       "           0.0887,  0.0426, -4.2592,  0.0293,  0.0267,  0.0340,  0.0907,\n",
       "           0.0753,  0.6880,  0.0358,  0.0529,  0.1073,  0.1293, -0.3003,\n",
       "           0.0926,  0.0493,  0.0580,  0.0761,  0.0347,  0.0288,  0.0472,\n",
       "           0.0395]]),\n",
       " tensor([[ 0.0490,  0.0390,  0.0855,  0.0333,  0.0393, -1.2004,  0.0652,\n",
       "           0.0328,  0.0230,  0.0372,  0.0610, -0.0008,  0.0379, -0.0237,\n",
       "          -0.1016,  0.1470,  0.0679,  0.0127,  2.8940,  0.0254,  0.0445,\n",
       "           0.0139,  0.0340,  0.0287,  0.0176, -1.0361,  0.0424,  0.0389,\n",
       "           0.0742]]),\n",
       " tensor([[ 0.0516,  0.0393,  3.6030,  0.0208, -0.6156,  0.0316,  0.0646,\n",
       "           0.0674,  0.0596, -0.2435,  0.0240,  0.0255,  0.0155,  0.0791,\n",
       "           0.0431, -0.9501,  0.0386,  0.1127,  0.0357, -0.6074,  0.0416,\n",
       "           0.0640, -1.1081,  0.0341,  0.6719,  0.3728,  0.0209,  0.0631,\n",
       "           0.0413]]),\n",
       " tensor([[-4.0215,  0.0084,  0.0213,  0.0383,  0.6244,  0.0361, -0.1187,\n",
       "           0.1187,  0.0663, -1.1960,  0.0670,  0.0347,  0.0398, -0.0138,\n",
       "           0.0739,  0.2122,  0.0101,  0.0451,  0.0401,  0.0799,  0.0495,\n",
       "           0.0362,  0.0048,  0.0299,  0.0279,  0.0416,  0.0362,  0.0565,\n",
       "          -2.1418]]),\n",
       " tensor([[ 0.0411,  0.0332,  0.0394,  0.0802,  0.1112, -4.4941,  0.0533,\n",
       "           0.0401,  0.0355,  0.0773, -0.6530,  0.0444,  0.0687, -1.4859,\n",
       "           5.4854, -0.0063, -0.4147,  0.0503,  0.0222,  0.0223,  0.0796,\n",
       "           0.0325,  0.0144,  0.0486,  0.0360,  0.0533,  0.0608,  0.0776,\n",
       "           0.0427]]),\n",
       " tensor([[ 0.0438,  0.0586,  0.0636,  0.0738, -0.9286,  0.0316, -0.0520,\n",
       "           0.1059,  1.5637,  0.0294,  0.0709,  0.0016,  0.0370,  0.0362,\n",
       "          -0.2046,  0.0364,  0.0213, -0.8884,  0.0333,  0.0595,  0.3886,\n",
       "           0.0341,  0.0322,  0.0360,  0.0918,  0.0659,  0.0342,  0.0741,\n",
       "           0.0522]]),\n",
       " tensor([[ 0.0438,  0.0305,  0.0487,  0.0251,  0.0535,  0.0402,  0.0352,\n",
       "           0.0447,  0.0354,  0.0318,  0.0279,  0.0235,  0.0426,  0.0157,\n",
       "           0.0357, -4.2148,  0.0352,  0.0979,  0.0636,  0.0356,  0.1133,\n",
       "           0.1185,  0.0127,  0.0415,  0.1123,  0.0388, -0.4252,  0.0358,\n",
       "           0.1022]]),\n",
       " tensor([[ 0.0548, -1.7156,  0.0963, -0.7082,  0.0469,  0.0150, -1.4015,\n",
       "           0.0934, -1.7277,  0.0417,  0.0352,  0.1419,  0.0351,  0.0336,\n",
       "           0.0138,  0.0549, -0.0094, -3.6324,  0.0180,  0.0713,  0.0313,\n",
       "           0.1214,  0.5161,  0.0298,  6.1851,  0.0660,  0.1174,  0.0277,\n",
       "          -1.6698]]),\n",
       " tensor([[ 0.0699,  0.0345, -1.9251,  0.1028,  0.1049, -1.1805,  3.1654,\n",
       "           0.0135,  0.0104,  0.0548,  0.0402,  0.0305,  0.0461, -1.4791,\n",
       "           0.0290,  0.0290, -2.5195,  0.0505,  0.0607,  0.0802,  0.0333,\n",
       "           0.0635,  0.0366,  0.1015,  0.0402,  0.1585, -0.3247,  0.0278,\n",
       "           0.0812]]),\n",
       " tensor([[ 0.0323,  0.0135,  0.0677, -1.9993,  0.1303, -0.3101,  0.0414,\n",
       "           0.0335,  0.0312,  0.0589, -0.9357, -1.3056,  0.0089, -0.0177,\n",
       "           0.1413, -1.5758,  0.0274,  0.0941, -1.0627, -0.1391,  0.0343,\n",
       "           0.0558, -4.5882,  0.0273, -1.0899,  0.0406,  0.0440,  0.0199,\n",
       "           0.0415]]),\n",
       " tensor([[ 0.0393, -0.4563,  0.0218,  0.0255,  0.0458,  0.0571, -2.5944,\n",
       "           0.0470,  0.0422,  0.0717, -0.8441,  0.0461,  0.0281,  0.0426,\n",
       "          -0.3453,  0.0451,  0.0584,  0.0315,  0.0544,  0.0464,  0.0529,\n",
       "           0.1517,  0.0388, -3.0414, -0.0050,  0.0156,  0.0589,  0.0375,\n",
       "           0.0312]]),\n",
       " tensor([[ 0.0267,  0.0241, -0.3006,  0.0418,  0.0362,  0.0403,  0.0554,\n",
       "           0.0607,  0.0435,  0.0535,  0.0435,  0.0893,  0.0179, -0.1595,\n",
       "           0.0271,  0.0415,  0.0352,  0.0303,  0.0969, -0.0142,  0.0504,\n",
       "          -0.4856,  0.0417, -0.0910, -0.2943,  0.0379,  0.0135,  0.3362,\n",
       "           0.0582]]),\n",
       " tensor([[-0.0152,  0.0618,  0.0188,  0.1226,  0.0405,  0.0363, -0.0020,\n",
       "           0.0525,  0.0518,  0.0279, -0.6964,  0.0381,  0.0382,  0.0397,\n",
       "          -0.0157,  0.2845, -1.2327,  0.0488,  0.0280, -2.3763,  0.1235,\n",
       "          -1.1163,  0.1100,  0.0793,  3.9038,  0.0307,  0.0056, -3.2468,\n",
       "           0.1057]]),\n",
       " tensor([[ 0.0530,  0.0300,  0.0175,  0.0101,  0.0366,  0.0402, -4.0234,\n",
       "          -0.3321, -0.0092,  0.0692,  1.0969, -3.6398, -1.7273,  2.0550,\n",
       "           0.0753,  0.0573,  0.0171,  0.0867,  0.0368, -0.0105,  0.0677,\n",
       "           0.0473, -0.0137, -4.8150,  3.9694,  0.1600,  0.0472,  4.9501,\n",
       "          -4.5806]]),\n",
       " tensor([[ 0.0321,  0.1124,  0.0318,  0.1303,  0.0438,  0.5246,  0.0247,\n",
       "           0.0362,  0.0149,  0.0384,  0.0268,  0.0408,  0.0489,  0.0325,\n",
       "           0.0263, -0.7715,  1.6735,  0.0477,  0.0338,  0.0344,  0.0299,\n",
       "           0.0333,  0.0208,  0.0316,  0.0266, -3.2201,  0.0279,  0.0446,\n",
       "          -0.5900]]),\n",
       " tensor([[ 0.0332, -1.7351,  0.0322,  0.1800,  0.0466, -2.1252,  0.0970,\n",
       "           0.0078, -0.0117,  0.0471,  0.0152,  1.3177,  0.0408, -0.4555,\n",
       "           0.0619, -0.0116,  0.0330, -4.3592,  0.0249, -4.0238,  0.0180,\n",
       "           0.0374,  0.0246, -0.4105,  0.0347, -0.4415,  5.5528,  0.7297,\n",
       "           0.0196]]),\n",
       " tensor([[ 0.0164,  0.0297,  0.0290,  0.0576,  0.0363,  0.0169,  0.0829,\n",
       "           0.0329,  0.0652,  0.0155,  0.0329, -0.5848,  0.0538,  0.0305,\n",
       "           0.0319,  0.0721,  0.0057,  0.0157,  0.0478,  0.0576,  1.9565,\n",
       "          -0.0235,  0.0654,  0.0729,  0.3154,  0.0825, -1.0382,  0.0317,\n",
       "           0.0628]]),\n",
       " tensor([[-0.0070,  0.0410,  0.0428, -1.2201,  0.0996, -0.1636,  0.0598,\n",
       "           0.0255, -0.6588,  1.4330,  0.0103,  0.0310,  0.0325,  0.0289,\n",
       "           0.0067,  0.1004,  0.0704,  0.0790,  1.3643,  0.0280,  0.0599,\n",
       "           0.0265,  1.7300,  0.0510,  0.0082,  0.3636,  0.0313,  0.0389,\n",
       "           0.0450]]),\n",
       " tensor([[ 0.0410,  0.0350,  0.0656,  0.2258,  0.1172,  0.0522,  0.0923,\n",
       "          -0.7897,  1.1670,  0.0521,  0.0683, -3.1340,  0.0175, -1.7591,\n",
       "          -0.0360,  0.0649,  0.0348,  0.0377,  0.0088, -0.0173, -0.1541,\n",
       "           0.0744,  0.0395,  0.0289,  0.0457,  0.0121,  0.0294,  0.0335,\n",
       "          -2.8371]]),\n",
       " tensor([[ 0.0189, -2.2865, -1.5586,  0.0663,  0.0259,  0.0366,  0.0438,\n",
       "           0.0282,  0.0686,  0.0203,  0.0577,  0.0668,  0.0458,  0.0342,\n",
       "           0.1258,  0.0379, -0.0071,  0.0776,  0.0574,  0.0379,  0.0436,\n",
       "           0.0346,  0.0506,  0.0286, -0.0062,  0.0607,  0.0425,  0.0323,\n",
       "           0.0998]]),\n",
       " tensor([[ 0.0434, -0.1873,  0.0213,  0.0598,  0.0601,  0.0400,  0.0348,\n",
       "           0.0362,  0.1011,  0.0481, -2.9499, -0.1144,  0.0344,  0.0313,\n",
       "           0.0483,  0.0546,  0.0394,  0.0216,  0.0314,  0.1704,  0.0989,\n",
       "          -2.7367, -0.0471, -1.2821, -0.0516,  0.0301,  0.1157,  0.7427,\n",
       "           0.0229]]),\n",
       " tensor([[ 0.0387,  0.0392,  0.0086,  0.0192,  1.4929,  0.4724,  0.0595,\n",
       "           0.0425,  0.0163,  0.0321,  0.0208,  0.0385, -1.7090,  0.0471,\n",
       "           0.0249,  0.0337,  0.0337,  0.0291,  0.0564,  0.0290,  0.0395,\n",
       "          -0.1364,  0.0390,  0.0661,  0.4383, -0.0914,  0.0203,  0.3945,\n",
       "           0.1013]]),\n",
       " tensor([[ 0.0471, -0.8944,  0.0342,  0.0197,  0.0308,  0.0128,  0.0852,\n",
       "           0.1347,  0.1117, -0.7435,  0.0576, -0.2794,  0.0355,  0.0479,\n",
       "           0.2313,  0.0964,  0.0362,  0.0056, -0.2107,  0.0211, -0.2723,\n",
       "           0.0550,  0.1050,  0.1399,  0.0580, -1.8983,  0.0088, -0.1147,\n",
       "          -0.3155]]),\n",
       " tensor([[ 0.0297,  0.0047,  0.0268,  0.0811,  0.0343,  0.1152,  0.0774,\n",
       "           0.0453, -0.0123,  0.0343,  0.0859,  0.0260,  0.1328,  0.0375,\n",
       "          -0.0047,  0.0337,  0.0580,  0.1387,  0.0325,  0.0307,  0.0326,\n",
       "           0.0394,  0.0447,  0.0472, -0.0078,  0.0739,  0.0833,  0.0502,\n",
       "           0.0370]]),\n",
       " tensor([[ 0.0583,  0.0369,  0.0297,  0.0423,  0.0286,  0.0152,  0.0079,\n",
       "           0.0396, -0.1834,  0.2664,  0.0123,  0.1656,  0.0069,  2.9232,\n",
       "           0.0338, -0.3748,  0.3562,  0.0367,  0.0758,  2.9089, -2.4005,\n",
       "           0.0310,  0.1008,  0.0217,  0.2006,  0.0378, -1.4959,  0.1064,\n",
       "           0.0369]]),\n",
       " tensor([[-0.9506,  0.0396,  0.0400,  0.0307, -0.0024,  0.0291, -0.1014,\n",
       "           0.0353,  0.0406,  0.0272,  0.0324, -4.0704, -4.8862,  0.0457,\n",
       "           0.0292,  0.0961,  0.0323, -0.0990,  0.0232, -1.5142,  0.0711,\n",
       "           0.0872,  0.0737,  0.0304, -2.0242, -0.3461,  0.0446,  0.0307,\n",
       "          -0.2420]]),\n",
       " tensor([[ 4.3871e-02, -3.9372e-01,  3.8558e-02,  2.5071e-02,  3.4740e-02,\n",
       "           3.2960e-02,  5.3865e-02,  2.2993e-02, -4.7407e+00,  4.9448e-02,\n",
       "           1.2824e-01,  4.0513e-02,  4.4463e-02,  2.0625e-01,  3.4525e-02,\n",
       "           5.5141e-02,  4.9662e-02,  4.3895e-02, -1.0287e-01,  2.4423e-02,\n",
       "          -1.1690e-05,  3.4421e-02, -2.0073e-01, -2.5615e+00,  6.0485e-02,\n",
       "           3.6498e-02,  1.2792e-02,  1.2633e-01,  4.9708e-02]]),\n",
       " tensor([[ 0.0359,  0.1146,  0.0412,  0.0534,  0.0449,  0.0193, -0.2548,\n",
       "          -0.1156,  0.0552,  0.0547, -0.0662, -1.5451,  0.1089, -0.0495,\n",
       "          -1.8323,  0.0102,  0.0206,  0.0065,  0.0348,  5.5429,  0.0406,\n",
       "           0.0240,  0.0050,  0.0159,  0.0279,  0.0265,  0.0743,  0.0986,\n",
       "           0.4466]]),\n",
       " tensor([[-0.1105,  0.0954,  0.0271,  0.0715,  0.0715,  0.0390, -0.0415,\n",
       "          -0.1003,  0.0123,  0.0527,  0.0721,  0.0467,  0.0897,  0.0527,\n",
       "          -0.7288,  0.0335,  0.0640,  0.1046, -1.6769,  0.0431,  0.0466,\n",
       "          -0.0255,  0.0489,  0.0653,  0.0340,  0.0011,  0.0342, -0.8390,\n",
       "           0.0321]]),\n",
       " tensor([[ 0.0400,  0.0279,  0.0487, -0.0306,  0.0362,  0.2002, -0.2065,\n",
       "           0.0307,  0.0319,  0.0109, -0.3003,  0.0092,  0.0325,  0.0429,\n",
       "           0.0969,  0.0379, -0.9517,  0.0125,  0.0416,  0.0316, -2.5662,\n",
       "           0.0443, -3.2007,  0.1006,  0.7989,  0.0372,  0.0404,  0.0323,\n",
       "           0.0569]]),\n",
       " tensor([[ 0.0852, -0.0813,  2.2432,  0.0402,  0.1549,  0.0067,  0.0611,\n",
       "           0.0349,  0.0619,  0.0147,  0.0827,  0.0449,  0.0398, -1.6249,\n",
       "           2.2038,  0.0589,  0.0231,  0.0491,  0.0382,  0.0534,  0.0790,\n",
       "           0.0374,  0.0311,  0.0312,  0.0920,  0.0331,  0.0797, -0.2542,\n",
       "           0.5858]]),\n",
       " tensor([[ 0.0308,  0.0407,  0.0798,  0.0733,  0.0516,  0.0735,  0.0347,\n",
       "           0.0297, -0.0027,  0.0356,  0.0332,  0.0913,  0.0345, -0.5871,\n",
       "           0.0296, -1.8526, -0.0348,  0.0222, -2.7751,  0.0300,  0.0578,\n",
       "          -0.0139,  0.0254,  0.0346,  0.0335,  0.0375,  0.0219,  0.0388,\n",
       "           0.0294]]),\n",
       " tensor([[-0.0030,  0.0273,  0.0167,  0.0288, -0.0073,  0.0312,  0.1124,\n",
       "           0.0379,  0.0269,  0.0473,  0.0358,  0.0836,  0.0404,  0.0539,\n",
       "           0.0177,  0.0859,  0.1201, -2.9480,  0.0181,  0.0670,  0.0368,\n",
       "           0.0822,  0.0308,  0.0375,  0.0252,  0.0554,  0.0178,  0.1413,\n",
       "           0.0346]]),\n",
       " tensor([[-0.7686,  0.0598,  0.0533,  0.0361,  0.0534,  0.0370,  0.0405,\n",
       "           0.0432,  0.0477,  0.0305,  0.0266,  0.0133,  0.0483, -1.2828,\n",
       "          -0.1889,  0.0514,  0.0362,  0.0518,  0.0550,  0.1013,  0.0321,\n",
       "           0.0706,  2.7086,  0.0344,  0.0993,  0.0319,  0.0668, -0.0106,\n",
       "           0.1178]]),\n",
       " tensor([[ 0.0184,  0.0457, -0.8008,  0.0353,  0.0795, -0.0174,  0.0358,\n",
       "           0.0671,  0.0346,  0.0417,  0.0349, -0.1155,  0.0688,  0.0240,\n",
       "           0.0494,  0.0324,  0.0109,  0.1721, -0.7851,  0.0498,  0.1375,\n",
       "           0.0312,  0.0450,  0.0520,  0.0447, -3.0583,  0.0279,  0.0403,\n",
       "           0.0095]]),\n",
       " tensor([[ 0.0925, -2.9323, -0.1143, -3.8211,  0.0353,  0.0065,  0.0496,\n",
       "           0.0224,  0.1179,  0.0113,  0.0750,  0.0239,  0.0145,  0.1310,\n",
       "           0.0390, -0.8930,  0.0299,  0.0274, -1.4410,  0.0762,  0.0388,\n",
       "          -4.5599,  0.0305,  0.0389,  0.0557,  0.0336,  0.0331,  0.0227,\n",
       "           0.1460]]),\n",
       " tensor([[ 0.0770,  0.0346,  0.0357,  0.1301,  0.0202,  0.0379,  0.0533,\n",
       "           0.0738,  0.0106,  0.0135,  0.0755,  0.0489,  0.1402,  0.0472,\n",
       "           1.9759,  0.0753,  0.1329,  0.1170,  0.0086,  0.0266,  0.0138,\n",
       "           0.0598,  0.0372, -0.0018,  0.0528,  0.0359,  0.0415,  0.0643,\n",
       "           0.1033]]),\n",
       " tensor([[ 0.0236,  0.0341, -0.0114,  0.0906,  0.0178,  0.0356,  0.0118,\n",
       "           0.0524,  0.0025,  0.0294,  0.0417,  0.0134, -0.0195,  0.0373,\n",
       "           0.0183,  0.0295, -0.1944,  0.1077,  0.0686,  0.0499,  0.0769,\n",
       "           0.0544, -0.0970,  0.0186,  0.0331,  0.0373, -0.0061,  0.0609,\n",
       "          -3.4585]]),\n",
       " tensor([[ 0.0360,  0.0248, -2.3916,  0.1386,  0.0270,  0.0870, -0.0012,\n",
       "           0.0367,  0.0634,  0.0507,  0.0174,  0.1677,  0.0904,  0.0594,\n",
       "           0.0364,  0.0328,  0.0381,  0.0280, -0.1266,  0.0314,  0.0166,\n",
       "           0.0809,  0.0607, -0.0341,  0.0283,  0.1543,  0.0361,  0.0679,\n",
       "           0.0748]]),\n",
       " tensor([[ 0.0905,  0.0538,  0.0447,  0.0446,  0.0219,  0.0535,  0.0549,\n",
       "           0.0292,  0.1036,  0.1165,  0.0358,  0.0518,  0.0331,  0.0477,\n",
       "           0.0596, -0.0624,  0.0806,  0.0440, -0.8302,  0.0487,  0.0179,\n",
       "           1.2275,  0.0494,  0.0618, -0.2308, -0.0936,  0.0210,  0.0390,\n",
       "           0.0335]]),\n",
       " tensor([[ 0.0724,  0.0030,  0.0943,  0.0715,  0.0237,  0.0385,  0.0327,\n",
       "          -0.0195,  0.0417,  0.0316,  0.0819,  0.0266,  0.3740,  0.0628,\n",
       "           0.0323,  0.2119,  0.0993,  0.0374,  0.0569,  0.1019,  0.0384,\n",
       "           0.0475,  0.0371,  0.0210,  0.1131,  0.0138,  0.0612,  0.0324,\n",
       "           0.2590]]),\n",
       " tensor([[-0.1290,  0.0416,  0.0460,  0.0367, -3.5940,  0.0377,  0.0241,\n",
       "          -2.4186,  0.0552,  0.1053,  0.7350,  0.0549,  0.0287, -0.0004,\n",
       "           0.0176,  0.0374,  0.0327,  0.0495,  0.0168,  0.1182, -0.3629,\n",
       "           0.0423, -1.0446,  0.1420,  0.0226,  0.0246, -0.5712,  0.0373,\n",
       "           0.1090]]),\n",
       " tensor([[-0.3941,  5.8718,  0.0252,  0.0445,  0.0444,  0.0490,  0.0529,\n",
       "          -0.1973,  0.1407,  0.0382,  0.1124, -0.2746,  0.0223,  0.0476,\n",
       "           0.1127,  0.0009,  0.1001,  0.1453,  0.0499, -1.2573,  0.0440,\n",
       "           0.0586,  0.0653,  0.9392,  0.0562,  0.0941,  0.0417, -0.4931,\n",
       "           0.0245]]),\n",
       " tensor([[ 0.0113,  0.0299,  0.0463, -0.0056,  0.0504, -2.7279,  0.7121,\n",
       "           0.1693,  0.0522,  0.0468,  0.0428,  0.0462,  0.0470, -1.3926,\n",
       "           0.0217,  0.0181,  0.0390,  0.0186,  0.0043, -0.0154,  0.0435,\n",
       "           0.0299,  0.0416,  0.0408,  0.0212,  0.0347,  0.0468,  0.1538,\n",
       "           0.0102]]),\n",
       " tensor([[ 0.0408,  0.0422,  0.1052,  0.0419,  0.0454,  0.3663,  0.4457,\n",
       "           0.0603, -4.5289,  0.0190, -4.1469,  0.0575,  0.1208,  0.0345,\n",
       "           0.3096,  0.0461,  0.1319, -0.4110,  0.0450,  0.0485,  0.0552,\n",
       "           0.0511,  0.1114,  0.0779,  0.8031,  0.0326,  0.0579,  0.0051,\n",
       "           0.1494]]),\n",
       " tensor([[ 0.0406,  0.0537,  0.0515,  0.0370,  0.0441,  0.1452,  0.1074,\n",
       "           0.0476,  0.3532,  0.0843,  0.0525, -0.7807, -0.0145,  0.0314,\n",
       "          -0.4489,  0.0093,  0.0320,  0.0378,  0.0648,  0.0449,  0.0345,\n",
       "           0.0401,  0.0011,  0.0749,  0.0682,  0.0785,  0.0303,  0.1305,\n",
       "           0.0420]]),\n",
       " tensor([[ 0.0244,  0.0429,  1.2224,  0.1834,  0.0373,  0.0744,  0.1214,\n",
       "          -1.2353,  0.0809,  0.1111, -0.0529,  0.0418,  0.0216,  0.1277,\n",
       "           0.0378,  1.3156,  0.0325,  0.0478,  0.0257,  0.0578,  0.0771,\n",
       "           0.0478,  0.0408,  0.1026,  0.0459,  0.0366,  0.0257,  0.0394,\n",
       "           0.0197]]),\n",
       " tensor([[ 0.0320, -3.1189,  0.1300,  0.0366,  0.0645,  0.0367,  6.3278,\n",
       "           0.0289,  0.0562,  0.0303,  0.0951,  0.0749,  5.9180,  0.0496,\n",
       "           0.0619,  0.0781,  0.0261,  0.0996,  0.0307,  0.0426,  0.0696,\n",
       "           0.0402,  2.4167,  0.0407,  0.0470,  0.1435,  0.0224,  0.0350,\n",
       "           0.0483]]),\n",
       " tensor([[ 0.1073, -0.2706,  0.0438,  0.0464,  0.0222,  0.0404,  0.0697,\n",
       "           0.1356,  0.0448,  0.0938,  0.0444,  0.0436,  0.0645, -1.0874,\n",
       "          -1.1486,  0.1029,  0.0215,  0.0411,  0.0469,  0.1378, -0.3468,\n",
       "          -0.2055,  0.0434,  0.1202,  0.0417, -1.7879, -0.0109,  0.0268,\n",
       "           0.0282]]),\n",
       " tensor([[ 0.0880,  0.0487,  0.0314,  0.0800,  0.0642,  0.0438,  0.0450,\n",
       "           0.0321,  0.0770,  0.0482,  0.0587,  0.0464,  0.0640,  0.0436,\n",
       "           0.0807,  0.0406,  0.1463,  0.9511,  0.0151, -0.1169, -0.2356,\n",
       "           0.2402,  0.0565,  0.0274,  0.1198,  0.1232,  0.0988, -1.6091,\n",
       "           0.0606]]),\n",
       " tensor([[ 0.1029,  0.0369,  0.0458, -0.0490,  0.0593, -0.5328,  0.1390,\n",
       "           0.0472,  0.0442,  0.0698,  0.0351,  0.1069, -4.7810,  0.0465,\n",
       "          -0.6886,  0.0402,  0.0409,  0.0364,  0.0798,  0.0833,  0.0974,\n",
       "           0.0391,  0.0580,  0.0350,  0.1001, -0.3369,  0.0349,  0.0692,\n",
       "          -2.0779]]),\n",
       " tensor([[ 0.0207,  0.0501,  0.0300,  0.0913,  0.0515,  0.0412,  0.0402,\n",
       "           0.0477,  0.1261,  0.0787,  0.0004,  0.0973,  0.1095,  0.0365,\n",
       "           0.0400,  0.0439, -0.1454,  0.0416,  0.2776,  0.0496, -0.1744,\n",
       "           0.0367, -0.0268,  0.0441,  0.0134, -0.1604,  0.0467,  0.0287,\n",
       "           0.0231]]),\n",
       " tensor([[ 0.0390,  0.0696,  0.0405,  0.0687,  0.0487,  0.0499, -2.7437,\n",
       "           0.0957,  0.0698,  0.0349,  0.0147,  0.0565,  0.0312,  0.0382,\n",
       "           1.0555,  0.0373, -0.4769,  0.1319,  0.0086,  0.0513,  0.0752,\n",
       "           0.8375,  0.4188,  0.0603,  0.0424,  0.0241, -0.0124,  0.0317,\n",
       "           0.0581]]),\n",
       " tensor([[ 0.0628,  0.0263,  0.1107, -0.0973,  0.3988,  0.0390,  0.0436,\n",
       "           0.0672,  0.0379,  1.1142,  0.1002,  0.0554,  0.0770,  0.0613,\n",
       "           0.0391,  0.0394, -1.2977, -1.2500, -0.0424,  0.0368,  0.0382,\n",
       "          -0.0117,  0.0437,  0.0453, -0.8402,  0.0209,  0.0975, -0.1293,\n",
       "           0.0626]]),\n",
       " tensor([[ 0.1172, -0.3468,  0.1297,  0.0412,  0.0201, -0.3067,  0.0373,\n",
       "          -0.0290,  0.0343,  0.0541,  0.0465,  0.0007,  0.0885,  0.1150,\n",
       "           0.0385,  0.0454, -0.1544,  0.1387,  0.0387,  0.0926,  0.0402,\n",
       "           0.0440,  0.1259,  0.0278, -0.0040,  0.0627,  0.0268, -0.2575,\n",
       "          -0.0024]]),\n",
       " tensor([[ 0.5686, -0.7734, -1.0039,  0.0347,  0.0452, -0.0187,  0.0415,\n",
       "           0.0426,  0.0519,  0.0221,  0.0229,  0.0325,  0.0375,  0.1054,\n",
       "           0.0751,  0.0943,  0.0466,  0.0148,  0.0530,  0.1387,  0.0813,\n",
       "           0.0435,  0.0991,  0.0289,  0.1088,  0.0392,  0.0407,  0.0466,\n",
       "           0.0748]]),\n",
       " tensor([[ 0.1363,  0.0372,  0.0385,  0.0432,  0.1015,  0.0341,  0.0821,\n",
       "           0.0338,  0.0415, -0.2145,  3.4566,  0.1702,  0.0267,  0.0090,\n",
       "           0.0036,  0.0442,  0.0389,  0.0560,  0.0314,  0.0541,  0.0795,\n",
       "          -0.9394,  0.0382, -0.0102,  0.0424,  0.0299,  0.0731,  0.1009,\n",
       "           0.0346]]),\n",
       " tensor([[ 0.0378,  0.0333,  0.0089, -0.0121,  3.8579,  0.0384,  0.0295,\n",
       "          -2.5009,  0.0362, -0.1322,  0.0231,  3.6797, -0.0916,  0.1319,\n",
       "           0.0305,  0.0362, -1.6596,  0.0708,  0.0422,  0.0292,  0.3506,\n",
       "           0.0742,  0.0261, -0.0606,  0.0289,  0.1502,  0.0217, -0.0053,\n",
       "           0.0398]]),\n",
       " tensor([[ 0.0060,  0.0872, -0.2741,  0.0894,  0.0448,  0.0348,  0.0590,\n",
       "           0.1882, -0.2036,  0.0216,  0.1006, -0.1150,  0.0636,  1.5668,\n",
       "           0.0357,  0.0568, -1.0975,  0.0322, -0.0020,  0.0784,  4.5168,\n",
       "           0.0691, -2.5852,  0.0346,  0.0720, -0.3425,  0.0300,  0.0570,\n",
       "           0.0391]]),\n",
       " tensor([[-0.2258,  0.0949,  0.0537,  0.0838,  0.8218,  0.0679, -0.6193,\n",
       "           0.0608,  0.0430,  0.0909,  0.1296, -0.3849,  0.0317,  0.0440,\n",
       "           0.0615, -0.7539,  0.0384,  0.0497, -1.4623,  0.0261,  0.0359,\n",
       "           0.0629,  0.0573,  0.1121,  0.0033,  0.0318,  0.0543,  0.0575,\n",
       "           0.0325]]),\n",
       " tensor([[ 0.0552,  0.0373,  0.0304,  0.0574,  0.0259,  0.0816, -2.1442,\n",
       "           0.0158,  0.0271,  0.0494,  0.0364,  0.0395,  0.0134,  0.0306,\n",
       "          -0.0260,  0.0660,  0.0445,  0.0318,  0.0490, -0.3930,  0.0425,\n",
       "           0.0368,  0.0416, -0.0104,  0.0383,  0.0321,  2.5832,  0.0685,\n",
       "           0.0398]]),\n",
       " tensor([[ 0.0460,  0.1638,  0.0708,  0.0449,  0.0337,  0.0285,  0.0350,\n",
       "           0.0930,  0.0686, -0.0043,  1.1631, -0.3432,  4.1253,  0.0600,\n",
       "           0.0533,  0.0247,  0.0077,  0.0349,  0.0668,  0.0314,  0.0987,\n",
       "           0.0402, -0.9979, -0.3046,  0.0513,  0.0450,  0.0351,  0.0840,\n",
       "           0.0651]]),\n",
       " tensor([[ 0.0354,  0.0547,  0.1013,  0.1851,  0.0054,  0.0382,  0.1017,\n",
       "           0.0426, -0.2811,  0.0283,  0.1383, -1.8590,  0.1303,  0.1447,\n",
       "           0.0024,  4.3227,  0.0266,  0.0509,  4.0017,  0.0627,  0.1054,\n",
       "           0.0387,  0.0822,  0.0615, -0.3792,  0.0311,  0.0433,  0.1544,\n",
       "           0.0208]]),\n",
       " tensor([[ 0.5043, -3.0862,  0.0383,  0.0253,  0.0680,  0.0477,  0.0419,\n",
       "           0.0416,  7.1342,  0.0265,  0.0279,  0.0408,  0.0250,  0.0420,\n",
       "           0.0967,  0.0605,  0.0270, -0.0438,  0.0272,  0.0098,  0.0399,\n",
       "           0.0223,  0.0178, -0.6390,  0.8237,  0.0446,  0.0399,  0.0742,\n",
       "           0.0126]]),\n",
       " tensor([[-0.1738,  1.2856,  0.0702,  0.1235, -0.1412,  0.0107,  0.0373,\n",
       "           0.0259,  0.0334,  0.0704,  0.0308,  0.1225,  0.0362,  0.0506,\n",
       "           0.0730,  0.0379,  0.1631,  0.0058, -0.1439,  0.0317,  0.0976,\n",
       "          -0.0871,  0.0338,  0.0702,  0.1198,  0.0480,  0.0925,  0.0369,\n",
       "           0.0482]]),\n",
       " tensor([[ 6.7428e-02, -3.7703e+00,  5.7051e-02, -3.7033e+00,  9.1572e-03,\n",
       "           2.9264e-02,  2.8154e-02, -7.6471e-02,  4.1333e-02, -8.6745e-05,\n",
       "           9.3231e-02, -2.3611e-04, -6.4881e-01,  3.1554e-02, -1.3813e+00,\n",
       "           7.6604e-02,  1.6279e-01,  3.4599e-02,  3.2574e-02,  3.2358e-02,\n",
       "           1.0638e-01, -1.4640e+00,  4.3602e-02,  2.0954e-02,  3.4159e-02,\n",
       "           7.0888e-02,  3.2111e-02, -1.5121e+00, -8.5526e-02]]),\n",
       " tensor([[ 2.8534, -1.9705,  0.1195,  0.0283,  0.0014,  0.1545,  0.0389,\n",
       "           0.0077,  0.0433,  0.6329, -0.0180,  0.0276,  0.0942,  3.2431,\n",
       "           0.0488,  0.0540,  0.0369,  0.0775,  0.0071,  0.0291,  0.0304,\n",
       "          -4.4204,  0.0891,  0.0437,  0.0306, -0.4460, -0.1776,  0.0433,\n",
       "          -0.1567]]),\n",
       " tensor([[ 0.0276,  0.0344,  0.0271,  0.0492,  0.0127, -0.4973, -0.1394,\n",
       "           0.0541,  0.0578,  0.0441, -0.0032,  0.0855,  0.0462,  0.0407,\n",
       "           0.1451, -2.7339,  0.0756,  0.0264,  0.0253,  0.1053,  0.0906,\n",
       "           0.0388,  0.1403,  1.1388,  0.1600,  0.1415,  3.2972, -0.0295,\n",
       "          -0.0737]]),\n",
       " tensor([[ 0.0366,  0.0194,  0.0523,  0.7450,  0.0305,  0.0322,  0.0249,\n",
       "          -0.1002,  0.2133,  0.0488,  0.0690,  0.0321,  6.1828, -0.1045,\n",
       "           7.0263,  0.0721, -3.9619, -0.2913,  0.0439,  1.1748,  0.0147,\n",
       "           0.0316,  0.0405,  0.0149,  0.0413, -2.2144,  0.0553,  0.0456,\n",
       "           0.1504]]),\n",
       " tensor([[ 0.0339,  0.3507,  0.0040,  0.0314,  0.0371,  0.0961,  0.1527,\n",
       "          -0.5196,  0.0123,  0.0448,  0.0100,  0.0225,  0.0190,  0.1033,\n",
       "          -0.0091,  0.0434,  0.0388,  0.0489,  0.0204,  0.1075,  0.0469,\n",
       "           2.5090,  0.0109,  0.1232,  0.0029,  0.0318,  0.0281,  0.0260,\n",
       "          -0.0694]]),\n",
       " tensor([[ 0.0341,  0.1039,  0.1320, -0.9102,  0.0371,  0.0082,  0.0339,\n",
       "           0.0373,  0.0315,  0.0410, -2.7955,  0.0325,  0.0809,  0.0427,\n",
       "           0.0339, -1.1775, -0.6063, -0.0004,  0.0437, -0.0471,  0.0343,\n",
       "          -1.8879, -2.0382,  0.0368,  0.0189,  0.0310,  0.0536,  3.1494,\n",
       "           0.0507]]),\n",
       " tensor([[ 0.0913,  0.0332,  0.0313,  0.0338,  0.0334,  0.0281,  0.0551,\n",
       "           0.0304,  0.0444,  0.0492,  0.0354, -0.1431,  0.0238,  0.0515,\n",
       "           0.0851, -3.1151,  0.0524,  0.0285,  0.0367, -0.1845,  0.0708,\n",
       "           0.0518,  0.0497,  0.0378,  0.0487,  0.0165,  0.0343,  0.0285,\n",
       "           0.1379]]),\n",
       " tensor([[ 0.0378,  0.0344,  0.0765,  0.0715,  0.0076,  0.0065, -0.4699,\n",
       "           0.1457,  0.0925,  0.0391,  0.0102,  0.0423,  0.1212,  0.0349,\n",
       "           0.1240,  0.0471,  0.0537, -0.0265,  0.0310,  0.0425,  0.0790,\n",
       "           0.0335,  2.3335,  0.0088,  0.0189,  0.0388,  0.0596,  0.0686,\n",
       "           0.0388]]),\n",
       " tensor([[ 0.0574,  0.0329,  0.0447,  0.0348,  0.0273,  0.0338,  1.4445,\n",
       "           0.0627,  0.0441,  0.0484, -3.0252,  0.0467,  0.0796,  0.0373,\n",
       "           0.0092,  0.1095,  0.0304, -0.0044, -0.0052,  0.0949,  0.0129,\n",
       "           0.0683,  0.0928, -1.1101,  0.1692,  0.0537,  0.0658,  0.0261,\n",
       "           0.0489]]),\n",
       " tensor([[ 0.0588,  0.0286,  0.1008,  0.0264, -4.1764,  0.0403,  0.0557,\n",
       "           0.0277,  0.0133,  0.0643, -0.0022,  0.0338,  0.0252,  0.0543,\n",
       "           0.0608,  0.0268,  0.0613, -0.0128,  0.0301,  0.9903,  0.0484,\n",
       "           0.0640,  0.0489,  0.0092,  0.0310,  0.0424,  0.0741,  0.0898,\n",
       "           0.0414]]),\n",
       " tensor([[ 0.0305,  0.0427,  0.0417,  0.0358,  0.0368,  0.0494, -0.2331,\n",
       "           0.0534,  0.0165,  0.0460,  0.0165,  0.0791,  0.0670,  0.0841,\n",
       "           0.0192,  0.0348,  0.0432,  0.0180,  0.0721,  0.0757,  0.0377,\n",
       "           0.0724,  0.1022,  0.0507,  0.0286, -0.2000,  0.0347,  0.0315,\n",
       "           0.0483]]),\n",
       " tensor([[ 0.0134,  0.0292, -0.2858,  0.0276,  0.0664,  0.0159,  0.0317,\n",
       "          -0.1268, -0.0051,  0.0587, -0.6722, -0.0109,  0.0807,  0.0459,\n",
       "          -2.2550,  0.1039,  0.0423,  0.0291,  0.0912,  0.0836, -0.0858,\n",
       "           0.0598,  0.0119,  0.0317,  1.4043,  0.0135,  0.0370, -4.7079,\n",
       "           5.5728]]),\n",
       " tensor([[ 0.0059,  0.0827,  0.4302,  0.0288,  0.0419, -1.2734,  0.0062,\n",
       "           0.0288,  0.0423, -0.6205,  0.0588,  0.0269,  0.0290,  0.0334,\n",
       "          -0.0023,  0.7321,  0.0208,  0.0601,  0.0302, -0.2677, -1.2182,\n",
       "          -0.0109,  0.0922,  0.0782,  1.8544,  0.0641,  0.0778, -0.0009,\n",
       "           0.0133]]),\n",
       " tensor([[ 0.0500,  0.0019,  0.0425,  0.0485,  0.0459,  0.0713,  0.0298,\n",
       "           0.0142,  0.0277,  0.1913,  4.6840, -0.2528,  0.0301,  0.0308,\n",
       "           0.0236,  0.0589, -0.0228,  0.0449,  0.0309,  0.0296,  0.0467,\n",
       "           0.0568, -0.0014, -0.0353,  0.0425,  0.0155,  0.0274,  0.0205,\n",
       "           0.0443]]),\n",
       " tensor([[ 0.0624,  0.0993,  0.0858, -0.0003,  0.0633,  0.0177,  0.0550,\n",
       "           0.1410,  6.4742,  0.0242,  0.0367,  0.0492,  0.0336,  0.0292,\n",
       "           0.6426,  0.0129,  0.0464,  0.0371,  0.0549,  0.0350,  0.0485,\n",
       "           0.0295,  0.0327, -0.3570, -1.7123,  0.5955,  0.0317,  0.0291,\n",
       "          -0.0099]]),\n",
       " tensor([[-0.0068,  0.0430, -0.1433,  0.0424, -0.1906,  0.0305,  0.0586,\n",
       "           0.0527,  0.0631,  0.1432,  0.1388, -0.4118, -0.0054,  0.0308,\n",
       "           0.0705,  0.0680,  0.0443,  0.4966,  0.0196,  0.0378,  0.0282,\n",
       "           0.0378,  0.0666, -2.2565,  0.0298,  0.0348,  0.1172, -0.0052,\n",
       "          -0.2921]]),\n",
       " tensor([[ 0.1507,  0.0273,  0.0318,  0.0027,  0.0339,  0.1057,  0.0422,\n",
       "           0.0291,  0.0331,  0.0124, -2.3066,  0.1018,  0.4325,  0.0257,\n",
       "           0.0651,  0.0700,  0.0865,  0.0294,  0.0419, -0.3892,  0.0431,\n",
       "           0.0231,  0.0096,  0.0621,  0.0456, -4.0057,  0.0285,  0.0479,\n",
       "           0.0808]]),\n",
       " tensor([[-0.0216, -1.8497,  0.0259, -0.0077,  0.0332,  0.0264,  0.0305,\n",
       "           0.0375,  0.0421,  0.0302,  0.0610, -1.8847,  0.0087,  0.0403,\n",
       "           0.0265,  0.0308,  0.0687,  0.0274, -0.8973, -0.0069,  0.0327,\n",
       "           0.0421,  0.0283,  4.5964,  0.0445,  0.0492,  0.0592,  0.0451,\n",
       "           0.0365]]),\n",
       " tensor([[ 0.0792,  0.0324, -2.2987,  0.1187, -4.3204,  0.0508,  0.0288,\n",
       "           0.0138,  0.0061, -0.0038,  0.0276,  1.8256,  0.0153, -0.1025,\n",
       "           0.0536,  0.0519,  5.8395,  0.0636,  0.0193,  0.0519,  0.0218,\n",
       "           0.0572,  0.0366,  0.0292,  1.9962,  0.0377,  0.0306, -0.1247,\n",
       "          -0.0097]]),\n",
       " tensor([[ 0.0265,  0.0490, -0.1794,  0.0119,  0.0178,  0.0508,  0.1273,\n",
       "           0.0390,  0.0070,  4.3017,  0.0193,  0.0795,  0.0228,  0.5104,\n",
       "           0.0159,  0.0176,  0.0350,  0.0262,  0.0471,  0.0879,  0.0071,\n",
       "           0.0825,  0.0271,  0.0286, -0.6963,  0.0793,  0.0121,  0.0503,\n",
       "           0.0258]]),\n",
       " tensor([[ 0.0560,  0.0260,  2.7034,  1.6246,  0.0390,  0.0282,  0.0502,\n",
       "           0.0728,  0.0925,  0.0123,  0.0300,  0.0213,  0.0429,  0.0416,\n",
       "           3.3222, -2.7519,  0.0419, -2.0006,  0.0300, -0.0089,  0.0114,\n",
       "           0.0468,  0.0531, -1.3231, -0.1026,  0.0348,  0.0180,  0.0294,\n",
       "           0.0234]]),\n",
       " tensor([[ 0.0811,  0.0467,  0.0392,  0.0273,  0.0517,  0.0340, -0.0130,\n",
       "          -3.7796,  0.0621,  0.0216,  0.0302,  0.1392,  0.0374,  0.0290,\n",
       "          -0.7378,  0.0308,  0.0182,  0.0714,  0.3175,  0.0313, -0.1399,\n",
       "           0.1544, -1.7303,  0.0632,  0.0879,  0.0291, -0.3178,  0.0326,\n",
       "           0.0570]]),\n",
       " tensor([[-0.0262,  0.0583,  0.0178,  0.0086,  0.0264,  0.0267,  0.0775,\n",
       "           0.1427, -4.4165,  0.3478,  0.0464,  0.0534,  0.0299,  0.0237,\n",
       "          -1.1783,  0.5626,  0.4661,  0.0330,  0.0234,  0.0094,  0.0238,\n",
       "           0.0259, -0.0243, -0.1180,  0.0364,  0.1464,  0.1333,  0.0359,\n",
       "           0.0294]]),\n",
       " tensor([[ 0.0245,  0.0458,  0.0285,  0.0964,  0.0175,  0.0054,  0.0148,\n",
       "           0.0261, -0.2838,  0.0116,  0.1084,  0.1227,  0.0753,  0.0426,\n",
       "           0.0314,  0.0318,  0.0193,  0.0576,  0.0321,  0.0756,  0.0194,\n",
       "           0.0320,  0.0478, -0.0184,  0.0107,  0.0280,  0.0296,  0.0319,\n",
       "           0.0955]]),\n",
       " tensor([[-4.5784e+00,  1.3033e-02,  3.2472e-02,  2.6102e-02, -9.9968e-05,\n",
       "           3.7956e-02, -5.8738e-02,  1.0735e-01,  2.4560e-02,  4.7843e-02,\n",
       "           9.0782e-02,  7.2093e-02,  2.1364e-02,  8.6690e-02,  3.8955e-02,\n",
       "          -1.5534e-01,  1.9301e-02,  2.3428e-02,  1.7650e-02,  9.7867e-02,\n",
       "           6.2908e-02,  7.9145e-02,  8.4248e-02,  4.0988e-03,  2.5918e-02,\n",
       "          -1.2211e-02, -7.5645e-01,  2.6271e-02,  1.1644e-01]]),\n",
       " tensor([[ 0.0908,  0.0327,  0.0302,  0.0306,  1.2583, -0.7233,  1.8671,\n",
       "           0.0229, -1.8312,  0.3538,  0.0108,  2.7875,  0.0367,  0.0766,\n",
       "           0.1539,  0.0257,  0.0078,  0.0322, -0.1627,  0.0285,  0.1388,\n",
       "           0.0176,  0.2220,  0.0029,  0.0948,  0.0028,  0.0802,  0.0369,\n",
       "           0.1085]]),\n",
       " tensor([[ 0.0491,  0.0172,  0.0902,  0.0931,  0.0344,  0.0385,  0.0263,\n",
       "           0.0244, -0.3432,  0.0330,  0.0260,  0.1296,  0.0189,  0.0710,\n",
       "           0.0541,  0.0235,  0.0271,  0.0202,  0.1570, -0.2963,  0.1122,\n",
       "          -0.0054,  0.0234,  0.0866,  0.0232,  0.0219,  0.0242,  0.0272,\n",
       "          -0.8892]]),\n",
       " tensor([[-0.0031,  0.0196,  0.2375,  0.1111,  1.0370,  0.0291,  6.4302,\n",
       "          -1.0955,  0.0404,  0.0179,  0.0294,  0.0542,  0.1208, -4.5240,\n",
       "           0.0332,  0.0699, -0.0032,  0.1017, -3.4447,  0.0017,  0.0154,\n",
       "           0.0296,  0.1825,  0.0449,  0.0286,  0.0205,  0.0491,  2.4623,\n",
       "           0.0303]]),\n",
       " tensor([[-0.8601,  0.0292,  0.0133,  0.0503, -0.4361,  0.0198,  0.0159,\n",
       "           0.1353,  0.0004,  0.0332,  0.0515,  0.0473,  0.0275, -0.2668,\n",
       "           0.0378,  2.6913,  0.0100,  1.1083,  0.0237,  0.0687,  0.0858,\n",
       "           0.0285,  0.0208,  0.0335,  0.0743,  0.0049,  1.6090,  0.0203,\n",
       "           0.0181]]),\n",
       " tensor([[ 0.0128,  0.0570, -4.6624,  0.0922,  0.0748,  0.0530,  0.0155,\n",
       "           0.0174, -0.2392, -0.4320,  0.0279,  0.0605, -0.1763,  0.0112,\n",
       "          -1.5347,  0.0226,  0.0166,  0.0052,  0.0677,  0.1497,  0.1179,\n",
       "           0.0307,  0.0802,  0.0052,  0.0192,  0.0218,  0.0220,  0.0210,\n",
       "           0.0344]]),\n",
       " tensor([[ 0.0356,  0.0586,  0.0243,  0.0238,  0.0180,  0.0283,  0.0193,\n",
       "           0.0633,  0.0974,  0.0026,  0.0105,  0.0296,  0.0280,  0.0384,\n",
       "           0.0106,  0.0391,  0.0556,  0.0247,  0.0180,  0.0381,  0.0140,\n",
       "           0.0186, -0.0209,  0.0195,  0.0220,  0.0087, -1.6355, -3.9067,\n",
       "           0.0202]]),\n",
       " tensor([[ 0.0294,  0.0613,  0.0206,  0.0517,  0.0314, -0.0037,  0.0166,\n",
       "           0.0699,  0.3642,  0.1063, -2.1637,  0.0588,  0.0241,  0.0207,\n",
       "           0.0233, -4.7415, -3.6029, -0.0747,  0.0204,  0.0034, -0.8787,\n",
       "           0.0381,  0.0483, -0.0050,  0.0191,  0.0628,  0.1512, -0.0008,\n",
       "           0.0454]]),\n",
       " tensor([[ 0.0684,  0.0165,  0.0179,  0.0053,  0.0185,  0.0186,  0.0171,\n",
       "          -1.9742, -0.0185, -0.0284,  0.0184,  0.0274, -3.2851,  0.1146,\n",
       "           0.0371,  0.0205,  0.0175, -2.3204, -1.2836,  0.1374,  0.0166,\n",
       "           0.0002, -2.5461, -0.0759, -0.5208,  0.0162,  0.0123,  0.0757,\n",
       "          -0.0307]]),\n",
       " tensor([[ 0.0202, -1.2954, -0.0281, -3.0442,  0.1093,  0.0476,  0.0732,\n",
       "          -0.4284,  0.0839,  0.0106,  0.0076, -0.0107,  0.0201,  0.0449,\n",
       "           0.0608,  0.0271,  0.0644,  0.0553,  0.0098, -0.1696,  0.0246,\n",
       "           0.0340,  0.0108,  0.0116, -0.0270,  0.0142,  0.0257,  0.0301,\n",
       "          -0.2373]]),\n",
       " tensor([[-0.0109,  1.5026,  0.0254,  0.0266, -0.0149,  0.0150, -0.0539,\n",
       "           0.0401,  0.0094,  0.0166,  0.0451,  0.0031,  0.0316, -0.1144,\n",
       "           2.9160,  5.8531,  0.0392,  0.0146,  0.1371,  0.0325,  0.0765,\n",
       "           0.0311,  0.0150,  0.0898,  0.0159,  0.3716,  0.0113, -0.0019,\n",
       "           0.0165]]),\n",
       " tensor([[-0.0156,  0.0112,  0.0300, -0.0038,  0.0684,  0.0743,  0.1676,\n",
       "           0.0256, -0.0761,  0.0343, -1.0531,  0.0177,  0.0157, -0.0117,\n",
       "           0.0300,  0.1041,  0.0348, -0.0252,  0.0317,  0.0242, -0.0022,\n",
       "           0.0455,  0.0206,  0.0036,  0.0151,  1.8458,  0.0142,  0.0212,\n",
       "           0.0304]]),\n",
       " tensor([[-0.0019,  0.3474,  0.0168,  0.0124,  1.4798,  0.0189,  0.0330,\n",
       "           0.0366,  0.2572,  0.0334, -0.0189,  0.6943, -0.0444,  0.0253,\n",
       "           0.0388,  0.4587,  0.0013,  0.0298,  0.0341,  0.0410,  0.0274,\n",
       "           0.0318,  0.0217,  0.0322, -1.5724, -4.3728, -1.1832,  0.0815,\n",
       "           0.0131]]),\n",
       " tensor([[ 0.0541, -0.2063,  0.0613,  0.0526,  0.1152,  0.0090,  0.0110,\n",
       "          -0.5014,  0.0152, -0.0375, -0.0067,  0.0127,  0.0106, -0.0056,\n",
       "           0.0053,  0.0288,  0.0109, -0.0021, -0.0002,  0.0117,  0.0134,\n",
       "           0.0624,  0.1031,  0.0709,  0.1056,  0.0235,  0.1086,  0.0630,\n",
       "           0.0246]]),\n",
       " tensor([[-0.0066, -0.0058,  0.0020,  0.0159, -0.0117, -0.0076, -0.0211,\n",
       "           0.1404,  0.0193,  0.0154, -0.0163,  0.0145, -4.8603,  0.1375,\n",
       "           0.0218,  0.0812,  0.0130,  0.0147,  0.1541,  0.1268,  0.0315,\n",
       "           0.0190,  0.0360,  0.4094,  0.0020,  0.0147,  0.0434,  0.0278,\n",
       "          -0.0171]]),\n",
       " tensor([[ 0.0085,  0.1584,  0.0181,  0.0147,  0.0319,  0.0125, -0.3918,\n",
       "          -3.8372,  0.0117,  0.0543,  0.0013,  0.0794, -0.0042,  0.0862,\n",
       "           0.0132,  0.0012,  0.0907, -0.0208,  0.0312,  4.9255,  0.0129,\n",
       "           0.0417,  0.0062, -0.0045,  0.0510,  0.0226,  0.0321,  0.0057,\n",
       "           0.0202]]),\n",
       " tensor([[ 0.0235,  0.0134,  0.0177,  0.0240,  0.0351,  0.0238,  0.0502,\n",
       "           0.0754, -0.0005,  0.0260,  0.0254,  0.0313, -0.2514,  0.0600,\n",
       "           5.0415,  0.0074,  0.0345,  0.0304, -0.4527, -0.0385, -0.6003,\n",
       "           0.1915,  0.0183, -0.2706, -2.4665,  0.0621,  0.0017,  1.6866,\n",
       "          -0.8576]]),\n",
       " tensor([[ 0.0152,  7.1755,  0.0182,  0.0775,  0.0437,  0.0869, -0.5379,\n",
       "           0.0109,  0.0349,  0.0180,  0.0655,  0.0847, -2.1908,  0.0082,\n",
       "          -0.0221,  0.0227,  0.0005,  0.0489,  5.2093, -0.5256,  0.0064,\n",
       "          -1.2229,  0.0335, -0.0162, -0.0035, -0.5264,  0.0055,  0.1056,\n",
       "           0.0150]]),\n",
       " tensor([[ 0.0172,  0.0117,  1.2119,  0.0104,  0.0542,  0.0419,  0.0820,\n",
       "           0.0265,  0.0660,  0.0174,  0.0699, -0.0031, -0.2882,  0.0713,\n",
       "           0.0138,  0.0146,  0.0203,  0.0087,  0.0216, -0.2610,  0.0152,\n",
       "          -0.0012,  0.0196,  0.1686,  2.4073,  0.1536, -0.0007,  0.0816,\n",
       "          -0.0619]])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can train and test model on the generated data\n",
    "synthentic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert array of tensor to tensors\n",
    "temp = torch.Tensor(99)\n",
    "synthentic_data = torch.cat(synthentic_data, out=temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.036590</td>\n",
       "      <td>0.066338</td>\n",
       "      <td>-3.085961</td>\n",
       "      <td>0.114058</td>\n",
       "      <td>0.064432</td>\n",
       "      <td>0.029813</td>\n",
       "      <td>0.093572</td>\n",
       "      <td>0.063873</td>\n",
       "      <td>0.094663</td>\n",
       "      <td>-3.812598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.462587</td>\n",
       "      <td>0.070291</td>\n",
       "      <td>0.080320</td>\n",
       "      <td>-0.106118</td>\n",
       "      <td>0.058923</td>\n",
       "      <td>0.568936</td>\n",
       "      <td>0.074845</td>\n",
       "      <td>0.019187</td>\n",
       "      <td>0.058150</td>\n",
       "      <td>0.043495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.188234</td>\n",
       "      <td>0.072007</td>\n",
       "      <td>0.065378</td>\n",
       "      <td>0.062867</td>\n",
       "      <td>-0.014745</td>\n",
       "      <td>-0.265162</td>\n",
       "      <td>0.046953</td>\n",
       "      <td>-0.000386</td>\n",
       "      <td>0.037717</td>\n",
       "      <td>0.078337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041360</td>\n",
       "      <td>0.086352</td>\n",
       "      <td>0.057410</td>\n",
       "      <td>-0.042508</td>\n",
       "      <td>0.050321</td>\n",
       "      <td>0.054021</td>\n",
       "      <td>0.022572</td>\n",
       "      <td>0.129492</td>\n",
       "      <td>0.066744</td>\n",
       "      <td>0.049808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.575109</td>\n",
       "      <td>-1.063222</td>\n",
       "      <td>0.098396</td>\n",
       "      <td>0.143215</td>\n",
       "      <td>0.062426</td>\n",
       "      <td>0.053933</td>\n",
       "      <td>-0.877024</td>\n",
       "      <td>-0.473791</td>\n",
       "      <td>-0.628724</td>\n",
       "      <td>0.050318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051512</td>\n",
       "      <td>0.054273</td>\n",
       "      <td>-0.372888</td>\n",
       "      <td>0.137870</td>\n",
       "      <td>0.042913</td>\n",
       "      <td>0.040867</td>\n",
       "      <td>0.045393</td>\n",
       "      <td>0.021144</td>\n",
       "      <td>0.049716</td>\n",
       "      <td>4.546776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.759997</td>\n",
       "      <td>0.080162</td>\n",
       "      <td>4.915091</td>\n",
       "      <td>0.068104</td>\n",
       "      <td>0.061577</td>\n",
       "      <td>-1.359456</td>\n",
       "      <td>0.074822</td>\n",
       "      <td>0.117253</td>\n",
       "      <td>0.129618</td>\n",
       "      <td>3.040996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046312</td>\n",
       "      <td>0.059219</td>\n",
       "      <td>0.060280</td>\n",
       "      <td>-4.169616</td>\n",
       "      <td>0.052768</td>\n",
       "      <td>0.059151</td>\n",
       "      <td>0.093343</td>\n",
       "      <td>0.066929</td>\n",
       "      <td>-0.861819</td>\n",
       "      <td>0.080842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.454671</td>\n",
       "      <td>-2.903558</td>\n",
       "      <td>0.024588</td>\n",
       "      <td>0.068801</td>\n",
       "      <td>0.098234</td>\n",
       "      <td>0.059110</td>\n",
       "      <td>0.050709</td>\n",
       "      <td>0.113785</td>\n",
       "      <td>0.148529</td>\n",
       "      <td>0.059449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061445</td>\n",
       "      <td>0.053994</td>\n",
       "      <td>0.049618</td>\n",
       "      <td>0.065974</td>\n",
       "      <td>0.041628</td>\n",
       "      <td>0.153999</td>\n",
       "      <td>0.056930</td>\n",
       "      <td>0.722060</td>\n",
       "      <td>-0.004213</td>\n",
       "      <td>0.244974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.036590  0.066338 -3.085961  0.114058  0.064432  0.029813  0.093572   \n",
       "1  0.188234  0.072007  0.065378  0.062867 -0.014745 -0.265162  0.046953   \n",
       "2 -0.575109 -1.063222  0.098396  0.143215  0.062426  0.053933 -0.877024   \n",
       "3 -0.759997  0.080162  4.915091  0.068104  0.061577 -1.359456  0.074822   \n",
       "4 -1.454671 -2.903558  0.024588  0.068801  0.098234  0.059110  0.050709   \n",
       "\n",
       "         7         8         9     ...           19        20        21  \\\n",
       "0  0.063873  0.094663 -3.812598    ...    -0.462587  0.070291  0.080320   \n",
       "1 -0.000386  0.037717  0.078337    ...     0.041360  0.086352  0.057410   \n",
       "2 -0.473791 -0.628724  0.050318    ...     0.051512  0.054273 -0.372888   \n",
       "3  0.117253  0.129618  3.040996    ...     0.046312  0.059219  0.060280   \n",
       "4  0.113785  0.148529  0.059449    ...     0.061445  0.053994  0.049618   \n",
       "\n",
       "         22        23        24        25        26        27        28  \n",
       "0 -0.106118  0.058923  0.568936  0.074845  0.019187  0.058150  0.043495  \n",
       "1 -0.042508  0.050321  0.054021  0.022572  0.129492  0.066744  0.049808  \n",
       "2  0.137870  0.042913  0.040867  0.045393  0.021144  0.049716  4.546776  \n",
       "3 -4.169616  0.052768  0.059151  0.093343  0.066929 -0.861819  0.080842  \n",
       "4  0.065974  0.041628  0.153999  0.056930  0.722060 -0.004213  0.244974  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tensor to PD data frame\n",
    "synthentic_data_df = pd.DataFrame(data=synthentic_data.data.numpy());\n",
    "synthentic_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns of the synthentic dataset\n",
    "cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "        'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount']\n",
    "synthentic_data_df.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.036590</td>\n",
       "      <td>0.066338</td>\n",
       "      <td>-3.085961</td>\n",
       "      <td>0.114058</td>\n",
       "      <td>0.064432</td>\n",
       "      <td>0.029813</td>\n",
       "      <td>0.093572</td>\n",
       "      <td>0.063873</td>\n",
       "      <td>0.094663</td>\n",
       "      <td>-3.812598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.462587</td>\n",
       "      <td>0.070291</td>\n",
       "      <td>0.080320</td>\n",
       "      <td>-0.106118</td>\n",
       "      <td>0.058923</td>\n",
       "      <td>0.568936</td>\n",
       "      <td>0.074845</td>\n",
       "      <td>0.019187</td>\n",
       "      <td>0.058150</td>\n",
       "      <td>0.043495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.188234</td>\n",
       "      <td>0.072007</td>\n",
       "      <td>0.065378</td>\n",
       "      <td>0.062867</td>\n",
       "      <td>-0.014745</td>\n",
       "      <td>-0.265162</td>\n",
       "      <td>0.046953</td>\n",
       "      <td>-0.000386</td>\n",
       "      <td>0.037717</td>\n",
       "      <td>0.078337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041360</td>\n",
       "      <td>0.086352</td>\n",
       "      <td>0.057410</td>\n",
       "      <td>-0.042508</td>\n",
       "      <td>0.050321</td>\n",
       "      <td>0.054021</td>\n",
       "      <td>0.022572</td>\n",
       "      <td>0.129492</td>\n",
       "      <td>0.066744</td>\n",
       "      <td>0.049808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.575109</td>\n",
       "      <td>-1.063222</td>\n",
       "      <td>0.098396</td>\n",
       "      <td>0.143215</td>\n",
       "      <td>0.062426</td>\n",
       "      <td>0.053933</td>\n",
       "      <td>-0.877024</td>\n",
       "      <td>-0.473791</td>\n",
       "      <td>-0.628724</td>\n",
       "      <td>0.050318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051512</td>\n",
       "      <td>0.054273</td>\n",
       "      <td>-0.372888</td>\n",
       "      <td>0.137870</td>\n",
       "      <td>0.042913</td>\n",
       "      <td>0.040867</td>\n",
       "      <td>0.045393</td>\n",
       "      <td>0.021144</td>\n",
       "      <td>0.049716</td>\n",
       "      <td>4.546776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.759997</td>\n",
       "      <td>0.080162</td>\n",
       "      <td>4.915091</td>\n",
       "      <td>0.068104</td>\n",
       "      <td>0.061577</td>\n",
       "      <td>-1.359456</td>\n",
       "      <td>0.074822</td>\n",
       "      <td>0.117253</td>\n",
       "      <td>0.129618</td>\n",
       "      <td>3.040996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046312</td>\n",
       "      <td>0.059219</td>\n",
       "      <td>0.060280</td>\n",
       "      <td>-4.169616</td>\n",
       "      <td>0.052768</td>\n",
       "      <td>0.059151</td>\n",
       "      <td>0.093343</td>\n",
       "      <td>0.066929</td>\n",
       "      <td>-0.861819</td>\n",
       "      <td>0.080842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.454671</td>\n",
       "      <td>-2.903558</td>\n",
       "      <td>0.024588</td>\n",
       "      <td>0.068801</td>\n",
       "      <td>0.098234</td>\n",
       "      <td>0.059110</td>\n",
       "      <td>0.050709</td>\n",
       "      <td>0.113785</td>\n",
       "      <td>0.148529</td>\n",
       "      <td>0.059449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061445</td>\n",
       "      <td>0.053994</td>\n",
       "      <td>0.049618</td>\n",
       "      <td>0.065974</td>\n",
       "      <td>0.041628</td>\n",
       "      <td>0.153999</td>\n",
       "      <td>0.056930</td>\n",
       "      <td>0.722060</td>\n",
       "      <td>-0.004213</td>\n",
       "      <td>0.244974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  0.036590  0.066338 -3.085961  0.114058  0.064432  0.029813  0.093572   \n",
       "1  0.188234  0.072007  0.065378  0.062867 -0.014745 -0.265162  0.046953   \n",
       "2 -0.575109 -1.063222  0.098396  0.143215  0.062426  0.053933 -0.877024   \n",
       "3 -0.759997  0.080162  4.915091  0.068104  0.061577 -1.359456  0.074822   \n",
       "4 -1.454671 -2.903558  0.024588  0.068801  0.098234  0.059110  0.050709   \n",
       "\n",
       "         V8        V9       V10     ...           V20       V21       V22  \\\n",
       "0  0.063873  0.094663 -3.812598     ...     -0.462587  0.070291  0.080320   \n",
       "1 -0.000386  0.037717  0.078337     ...      0.041360  0.086352  0.057410   \n",
       "2 -0.473791 -0.628724  0.050318     ...      0.051512  0.054273 -0.372888   \n",
       "3  0.117253  0.129618  3.040996     ...      0.046312  0.059219  0.060280   \n",
       "4  0.113785  0.148529  0.059449     ...      0.061445  0.053994  0.049618   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  normAmount  \n",
       "0 -0.106118  0.058923  0.568936  0.074845  0.019187  0.058150    0.043495  \n",
       "1 -0.042508  0.050321  0.054021  0.022572  0.129492  0.066744    0.049808  \n",
       "2  0.137870  0.042913  0.040867  0.045393  0.021144  0.049716    4.546776  \n",
       "3 -4.169616  0.052768  0.059151  0.093343  0.066929 -0.861819    0.080842  \n",
       "4  0.065974  0.041628  0.153999  0.056930  0.722060 -0.004213    0.244974  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tensor to PD data frame\n",
    "#synthentic_data_df = pd.DataFrame(data=synthentic_data.data.numpy());\n",
    "synthentic_data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 1 to Class column since they're all synthetic generated fraud data\n",
    "synthentic_data_df['Class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "data = data.drop(['Time','Amount'],axis=1)\n",
    "\n",
    "# Rearrange columns to the right order\n",
    "cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',\n",
    "        'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'normAmount', 'Class']\n",
    "data = data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add newly generated fraud data to the original data set\n",
    "new_data = pd.concat([data, synthentic_data_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (285053, 29)\n",
      "Shape of y: (285053, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiem/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/kiem/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X = np.array(new_data.ix[:, data.columns != 'Class'])\n",
    "y = np.array(new_data.ix[:, data.columns == 'Class'])\n",
    "print('Shape of X: {}'.format(X.shape))\n",
    "print('Shape of y: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain shape\n",
      "(199537, 29)\n",
      "xtest shape\n",
      "(85516, 29)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print('xtrain shape')\n",
    "print(X_train.shape)\n",
    "print('xtest shape')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[85297,     9],\n",
       "       [   70,   140]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
